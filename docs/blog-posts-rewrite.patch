diff --git a/content/posts/JWT's/index.mdx b/content/posts/JWT's/index.mdx
index 90a8881..1c2657e 100644
--- a/content/posts/JWT's/index.mdx
+++ b/content/posts/JWT's/index.mdx
@@ -1,127 +1,199 @@
 ---
-title: "JWT CTF Lab: A Full Debug Walkthrough"
-date: "2025-10-08"
+title: "JWT CTF Lab: Build, Decode, and Verify Tokens"
+date: "2025-08-10"
 slug: "/jwt-ctf-lab"
-tags: ["security", "ctf", "jwt", "debugging"]
-description: "A complete lab tutorial for decoding, checking, and fixing JWTs without claiming production experience."
+tags: ["security", "jwt", "ctf"]
+description: "A full JWT lab guide with exact steps, commands, and verification checks."
 theme: "Security"
 ---
 
-JWT bugs are common because people skip basic checks.
+JWTs are simple until you trust them without verifying.
 
-This post is a full lab walkthrough, not a production story.
+This lab walks through creating, decoding, and validating a JWT.
 
-You will decode a token, validate claims, and fix a broken flow.
+You will build a tiny token workflow and verify each step.
 
-If you are new to JWTs, this is for you.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
 
-## What this is
-
-High level, a JWT is a signed string used to pass claims between a client and server.
-
-Low level, it is three base64url parts: header, payload, signature.
-
-Key terms:
-
-- **Header**: metadata like the algorithm used to sign the token.
-- **Payload**: the claims, such as subject, issuer, and expiry.
-- **Signature**: the cryptographic proof the token was not tampered with.
-
-## What you need
-
-- A JWT string from your lab.
-- Node.js 18 or later.
-- A text editor and a terminal.
-
-Optional but helpful:
-
-- A JWT debugger like https://jwt.io
-
-## Start to finish
+## The promise
 
-1) Copy the token from the lab.
+By the end, you will:
 
-Why: You need the raw string for every check.
+- generate a JWT
+- decode it safely
+- verify its signature
 
-2) Split the token into its three parts.
+> "If you do not verify the signature, the token is just a string."
 
-Why: You need to inspect header and payload separately.
-
-```bash
-node -e "const t=process.argv[1]; console.log(t.split('.'))" "YOUR.JWT.HERE"
-```
-
-3) Decode the payload.
-
-Why: You need to read claims like exp and aud.
-
-```bash
-node -e "const p=process.argv[1].split('.')[1]; console.log(Buffer.from(p,'base64url').toString('utf8'))" "YOUR.JWT.HERE"
-```
-
-4) Check core claims.
-
-Why: Most lab failures are missing or wrong claims.
-
-Check at least:
+## What this is
 
-- iss (issuer)
-- aud (audience)
-- sub (subject)
-- exp (expiry)
+**High level:** JWT is a signed token format used for auth and claims.
 
-5) Fix the claim or regenerate the token.
+**Low level:** You sign a JSON payload with a secret, then verify that signature on decode.
 
-Why: A corrected claim is only valid if it matches the lab rules.
+**Key terms:**
 
-If you are using a lab tool, follow its exact signing steps.
+- **JWT:** JSON Web Token.
+- **Header:** The token metadata.
+- **Payload:** The claims inside the token.
+- **Signature:** The cryptographic proof the token was not modified.
 
-6) Retry the lab request.
+## What you need
 
-Why: You need to confirm the fix, not just the theory.
+- Node 18+
+- A terminal
+- A new folder for the lab
+
+## Start to Finish
+
+### Step 1: Create the lab project
+Goal:
+Set up a minimal Node project for the JWT lab.
+
+Actions:
+- Commands:
+  ```bash
+  mkdir jwt-lab && cd jwt-lab
+  npm init -y
+  npm install jsonwebtoken
+  ```
+- File path: `index.js`
+
+Why:
+A clean folder keeps the lab isolated. `jsonwebtoken` gives you signing and verification in a few lines. This keeps the example focused. It also makes it easy to delete later.
+
+Verify:
+- Run:
+  ```bash
+  npm list jsonwebtoken
+  ```
+- Expected: `jsonwebtoken` appears.
+- This confirms the dependency is installed.
+
+If it fails:
+- Symptom: `jsonwebtoken` missing.
+- Fix: run `npm install jsonwebtoken` again.
+
+### Step 2: Sign a token
+Goal:
+Generate a JWT with a known payload.
+
+Actions:
+- File path: `index.js`
+- Add:
+  ```js
+  const jwt = require("jsonwebtoken");
+
+  const secret = "demo-secret";
+  const payload = { userId: "123", role: "user" };
+
+  const token = jwt.sign(payload, secret, { expiresIn: "1h" });
+  console.log("token:", token);
+  ```
+
+Why:
+You need a real token to test. A short payload is easy to inspect. A fixed secret keeps the lab deterministic. Expiration helps demonstrate validation later.
+
+Verify:
+- Run:
+  ```bash
+  node index.js
+  ```
+- Expected: a long token string printed to the console.
+- This confirms the token is generated.
+
+If it fails:
+- Symptom: `jwt.sign is not a function`.
+- Fix: check the import and package install.
+
+### Step 3: Decode the token
+Goal:
+Inspect the token without verifying it yet.
+
+Actions:
+- File path: `index.js`
+- Add after signing:
+  ```js
+  const decoded = jwt.decode(token, { complete: true });
+  console.log("decoded:", decoded);
+  ```
+
+Why:
+Decoding shows the header and payload. This proves what is inside the token. It also shows why decoding alone is not enough for trust. This step is the baseline for the next verification step.
+
+Verify:
+- Run `node index.js`.
+- Expected: `header` and `payload` printed.
+- This confirms the decode works.
+
+If it fails:
+- Symptom: `decoded` is null.
+- Fix: ensure the token string is passed correctly.
+
+### Step 4: Verify the signature
+Goal:
+Verify the token with the correct secret.
+
+Actions:
+- File path: `index.js`
+- Add:
+  ```js
+  try {
+    const verified = jwt.verify(token, secret);
+    console.log("verified:", verified);
+  } catch (err) {
+    console.error("verify failed:", err.message);
+  }
+  ```
+
+Why:
+Verification is the security step. It proves the token was signed with the secret. Without this, any attacker can forge a token. This is the part that turns the JWT into a trustworthy claim.
+
+Verify:
+- Run `node index.js`.
+- Expected: `verified` object printed with `userId` and `role`.
+- This confirms the signature is valid.
+
+If it fails:
+- Symptom: `invalid signature`.
+- Fix: ensure the secret used to verify matches the secret used to sign.
 
 ## Verify it worked
 
-- The lab should accept the token.
-- The server should stop returning auth errors.
-
-If you see **401 Unauthorized**, it usually means a missing or invalid claim.
-
-If you see **403 Forbidden**, it often means the claim is valid but not allowed.
+- A token prints.
+- The token decodes into header and payload.
+- Verification returns the payload without errors.
 
 ## Common mistakes
 
-- **Symptom**: Token decodes but still fails.
-  **Cause**: You only decoded, you did not verify claims.
-  **Fix**: Check iss, aud, sub, exp one by one.
+- **Symptom:** Verification fails with `invalid signature`.  
+  **Cause:** Secret mismatch.  
+  **Fix:** use the same secret for signing and verifying.
 
-- **Symptom**: Expired token.
-  **Cause**: Wrong system time or wrong exp value.
-  **Fix**: Regenerate with a fresh exp.
+- **Symptom:** Token expires immediately.  
+  **Cause:** Bad `expiresIn` value.  
+  **Fix:** use `1h` or a numeric value in seconds.
 
-- **Symptom**: Algorithm mismatch.
-  **Cause**: Header alg does not match the lab rules.
-  **Fix**: Use the correct signing algorithm.
+- **Symptom:** You trust decoded tokens without verifying.  
+  **Cause:** Misunderstanding of JWT.  
+  **Fix:** always call `verify`.
 
 ## Cheat sheet
 
-- JWT is header.payload.signature.
-- Decode does not equal verify.
-- Check iss, aud, sub, exp every time.
-- Fix one claim at a time.
-- Re-run the lab after each change.
+- Sign with a secret.
+- Decode for visibility.
+- Verify for trust.
 
 ## Next steps
 
-- Try the same checklist on a new lab token.
-- Write a short debug note after each fix.
-- Read the OWASP JWT cheat sheet.
+- Add a refresh token flow.
+- Store the token securely in memory only for demos.
 
-Related links:
+## Related links
 
-- https://jwt.io/introduction
-- https://owasp.org/www-project-cheat-sheets/cheatsheets/JSON_Web_Token_Cheat_Sheet_for_Java.html
+- https://jwt.io/
+- https://github.com/auth0/node-jsonwebtoken
 
-Final CTA:
+## Final CTA
 
-If you want a short JWT checklist you can print, ask and I will share it.
+If you do not verify the token, you do not have security.
diff --git a/content/posts/ai-end-to-end-projects/index.mdx b/content/posts/ai-end-to-end-projects/index.mdx
index e0ccad6..ecdd8a3 100644
--- a/content/posts/ai-end-to-end-projects/index.mdx
+++ b/content/posts/ai-end-to-end-projects/index.mdx
@@ -1,138 +1,200 @@
 ---
-title: "End to End Projects With AI: A Complete, Honest Workflow"
-date: "2025-10-01"
+title: "End to End Projects With AI: A Strict, Verifiable Workflow"
+date: "2025-05-03"
 slug: "/ai-end-to-end-projects"
-tags: ["ai", "process", "projects", "web-development"]
-description: "A full start to finish workflow for using AI as a helper without overstating results."
+tags: ["ai", "workflow", "productivity"]
+description: "A practical AI assisted build workflow with exact steps, files, and checks."
 theme: "Process"
 ---
 
-AI can speed up the boring parts, not replace the work.
+AI can speed you up, but it can also lie.
 
-This post is a full workflow from idea to demo with guardrails.
+This post shows the exact workflow I use to keep AI output honest.
 
-You will learn how I scope, build, verify, and document projects.
+You will set up a prompt log, verify code, and document gaps.
 
-If you want a clean process without fluff, this is for you.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
 
-## What this is
-
-High level, this is a project workflow that uses AI for drafts and refactors.
-
-Low level, it is a checklist with prompts, code steps, and verification checks.
-
-Key terms:
-
-- **Scope**: the smallest version of the idea you can ship.
-- **Verification**: proof that the demo works and matches the claims.
-- **Notes**: short documentation that explains what is missing.
-
-## What you need
+## The promise
 
-- A simple project idea.
-- A local dev setup for the stack you plan to use.
-- A text editor and terminal.
+By the end, you will:
 
-Optional tools:
+- structure an AI assisted build workflow
+- keep a proof trail in your repo
+- avoid claims your code cannot back up
 
-- ChatGPT or Copilot for drafting.
-- A README template.
+> "AI helps me draft. I still verify everything."
 
-## Start to finish
-
-1) Write a one sentence scope.
+## What this is
 
-Why: This prevents the project from bloating.
+**High level:** This is a project workflow that uses AI for drafts and refactors, then verifies every output by running code.
 
-Example:
+**Low level:** You keep a prompt log, implement changes in files, run tests, and write a gap note.
 
-```text
-Build a static demo that lists 151 Pokemon with search and details.
-```
+**Key terms:**
 
-2) Ask AI for a rough plan.
+- **Prompt log:** A file where you track AI requests and edits.
+- **Verification:** Running code or checks to confirm output.
+- **Gap note:** A short list of what is not done yet.
 
-Why: You want an outline, not a full solution.
+## What you need
 
-Prompt:
+- A repo with a working build command
+- A text editor
+- A place to store notes in the repo
 
-```text
-Give me a 6 step plan for this demo. Keep it small and direct.
-```
+## Start to Finish
 
-3) Build the first working path.
+### Step 1: Add a prompt log
+Goal:
+Create a file to record AI inputs and outputs.
 
-Why: The first working path proves the idea.
+Actions:
+- File path: `docs/prompt-log.md`
+- Add:
+  ```text
+  Prompt log
+  Date:
+  Prompt:
+  Output summary:
+  Verified by:
+  ```
 
-Example steps:
+Why:
+A prompt log makes the work traceable. It shows what came from the model and what you changed. This keeps your repo honest. It also helps you debug when something breaks later.
 
-- Create the project folder.
-- Build one screen.
-- Show one data item.
+Verify:
+- Run:
+  ```bash
+  cat docs/prompt-log.md
+  ```
+- Expected: the template appears.
+- This confirms the file exists.
+
+If it fails:
+- Symptom: file not found.
+- Fix: create the `docs/` folder and add the file again.
+
+### Step 2: Draft a change with AI
+Goal:
+Use AI to draft a change you can verify.
+
+Actions:
+- Example prompt in your log:
+  ```text
+  Prompt: Add a button component with props for label and onClick.
+  Output summary: Drafted Button.tsx and usage example.
+  ```
+- File path: `src/components/Button.tsx`
+- Add:
+  ```tsx
+  type ButtonProps = { label: string; onClick?: () => void };
+
+  export function Button({ label, onClick }: ButtonProps) {
+    return <button onClick={onClick}>{label}</button>;
+  }
+  ```
+
+Why:
+A small component is easy to verify. It keeps the AI output scoped. This step also forces you to review the code before you ship it. A tiny change is easier to debug than a big refactor.
 
-4) Add a README with limits.
+Verify:
+- Import and render the button in a page.
+- Run your dev server.
+- Expected: a button appears and clicks.
+- This confirms the code runs.
+
+If it fails:
+- Symptom: build error in TypeScript.
+- Fix: add the correct props type or adjust the import.
+
+### Step 3: Run verification checks
+Goal:
+Confirm the change does not break the project.
+
+Actions:
+- Commands:
+  ```bash
+  npm run build
+  npm run lint
+  ```
+- Add a line to the prompt log:
+  ```text
+  Verified by: build + lint
+  ```
+
+Why:
+AI output can be wrong. Running your build and lint is the fastest proof you have. Logging the checks keeps you honest. This step is what turns a draft into a real change.
 
-Why: You need to document gaps honestly.
+Verify:
+- Expected: both commands exit with code 0.
+- This proves the change is safe to keep.
 
-Example section:
+If it fails:
+- Symptom: build error.
+- Fix: revert the change or correct the failing file.
 
-```md
-## Reality snapshot
-- What works
-- What is missing
-- What is planned
-```
+### Step 4: Write a gap note
+Goal:
+Record what is still missing.
 
-5) Verify and publish.
+Actions:
+- File path: `docs/gaps.md`
+- Add:
+  ```text
+  Gaps
+  - Button has no hover styles
+  - Button has no disabled state
+  ```
 
-Why: A demo without a working link is not useful.
+Why:
+Gap notes stop you from overstating progress. They also make future work clear. This is the most honest part of the workflow. It prevents fake polish.
 
 Verify:
+- Open `docs/gaps.md` and confirm the list is there.
+- This confirms the gap note exists.
 
-- The demo runs locally.
-- The live link works.
-- The README matches the code.
+If it fails:
+- Symptom: the list is empty.
+- Fix: add at least one real gap.
 
 ## Verify it worked
 
-- You can load the demo without errors.
-- The main flow works in a private browser window.
-- The README does not claim features you do not have.
-
-If you see build errors, fix them before publishing.
+- The prompt log exists and has entries.
+- The change runs locally.
+- Build and lint pass.
+- A gap note exists.
 
 ## Common mistakes
 
-- **Symptom**: The demo works locally but not online.
-  **Cause**: Missing base path or missing assets.
-  **Fix**: Update config and rebuild.
+- **Symptom:** AI output is merged without review.  
+  **Cause:** No verification step.  
+  **Fix:** Run build and lint before merge.
 
-- **Symptom**: README claims features not in code.
-  **Cause**: Copying an outline without edits.
-  **Fix**: Rewrite the README after the demo works.
+- **Symptom:** Claims do not match code.  
+  **Cause:** No prompt log or gap note.  
+  **Fix:** add both files and keep them updated.
 
-- **Symptom**: AI output is wrong.
-  **Cause**: You accepted it without verification.
-  **Fix**: Check docs and test the code.
+- **Symptom:** Changes are too big.  
+  **Cause:** One giant prompt.  
+  **Fix:** split the work into small tasks.
 
 ## Cheat sheet
 
-- Write a one sentence scope.
-- Build the smallest working path.
-- Add a reality snapshot.
-- Verify the live link.
+- Write the prompt log.
+- Keep AI changes small.
+- Verify with build and lint.
+- Write a gap note.
 
 ## Next steps
 
-- Ship one small demo this week.
-- Remove one claim you cannot prove.
-- Add one proof link.
+- Add a short CHANGELOG entry per AI assisted change.
+- Add tests for the new component.
 
-Related links:
+## Related links
 
-- https://bradleymatera.dev/projects/
-- https://github.com/BradleyMatera/AnimalSounds
+- https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks
 
-Final CTA:
+## Final CTA
 
-If you want my README template, ask and I will share it.
+Use AI for drafts, then prove every change with a real check.
diff --git a/content/posts/amazon-internship-troubleshooting/index.mdx b/content/posts/amazon-internship-troubleshooting/index.mdx
index 14cca86..3fb1fc4 100644
--- a/content/posts/amazon-internship-troubleshooting/index.mdx
+++ b/content/posts/amazon-internship-troubleshooting/index.mdx
@@ -1,117 +1,211 @@
 ---
-title: "AWS Cloud Support Internship: Lab Notes and Capstone Walkthrough"
-date: "2025-08-01"
+title: "AWS Cloud Support Internship: What I Actually Practiced"
+date: "2025-08-18"
 slug: "/aws-cloud-support-internship-mastering-troubleshooting-and-architecture"
-tags: ["aws", "cloud-support", "internship", "devops"]
-description: "A lab only walkthrough of the AWS support internship work, including the capstone outline."
-theme: "Cloud & DevOps"
+tags: ["aws", "troubleshooting", "cloud"]
+description: "A clear, bounded summary of training labs and a capstone build."
+theme: "Cloud"
 ---
 
-This post covers lab work, not production support.
+This is an internship post about guided training and a capstone.
 
-I worked in training environments with mock tickets and labs.
+It does not describe production ownership or customer tickets.
 
-You will learn the structure, the capstone flow, and how I documented it.
+You will see the exact lab style steps I practiced and how I verified them.
 
-If you want a realistic view of what the internship covered, this is it.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
 
-## What this is
-
-High level, this is a summary of lab exercises and a capstone build.
-
-Low level, it is a set of steps and checks that match what I practiced.
-
-Key terms:
-
-- **Lab**: a safe environment with guided tasks and no customer data.
-- **Capstone**: a small project that ties together multiple services.
-- **Mock ticket**: a simulated issue used to practice troubleshooting.
-
-## What you need
-
-- An AWS account in a sandbox or lab.
-- AWS CLI and basic IAM access.
-- A text editor for notes.
-
-Docs used:
-
-- https://docs.aws.amazon.com/lambda/
-- https://docs.aws.amazon.com/dynamodb/
-- https://docs.aws.amazon.com/s3/
-
-## Start to finish
-
-1) Set up the lab account.
-
-Why: You need a safe space to practice without real data.
-
-2) Create an S3 bucket for uploads.
-
-Why: The capstone ingests files from S3.
+## The promise
 
-3) Create a Lambda function that reads metadata.
+By the end, you will:
 
-Why: Lambda does the extraction work.
+- understand the scope of the training work
+- see a concrete capstone outline
+- know what I did and did not do
 
-4) Store results in DynamoDB.
+> "Guided labs build skill, but they are not production ownership."
 
-Why: The metadata needs a simple queryable store.
+## What this is
 
-5) Build a basic Amplify UI.
+**High level:** This is a summary of guided lab rotations and a capstone build.
 
-Why: The UI is for internal review, not a public app.
+**Low level:** The capstone used S3, Lambda, DynamoDB, and an Amplify frontend plus a cost model.
 
-6) Document costs.
+**Key terms:**
 
-Why: The capstone includes a basic cost model.
+- **Guided lab:** A training environment with sample data and instructions.
+- **Capstone:** A final project that demonstrates skills in a controlled setting.
 
-### Example resource sketch
+## What you need
 
-```text
-S3 upload -> Lambda -> DynamoDB -> Amplify UI
-```
+- AWS account
+- Access to S3, Lambda, DynamoDB, and Amplify
+- A small test dataset
+
+## Start to Finish
+
+### Step 1: Create the S3 input bucket
+Goal:
+Create a bucket for uploads that Lambda can read.
+
+Actions:
+- AWS Console → S3 → Create bucket
+- Name: `metadata-input-demo`
+- Region: your default region
+
+Why:
+The bucket is the entry point for uploads. It provides a stable source for the workflow. This keeps the capstone simple. You can restrict permissions later if needed.
+
+Verify:
+- Expected: bucket appears in the S3 list.
+- This confirms the input bucket exists.
+
+If it fails:
+- Symptom: bucket name already taken.  
+  **Fix:** choose a unique name.
+
+### Step 2: Add a Lambda function
+Goal:
+Parse object metadata when a file is uploaded.
+
+Actions:
+- AWS Console → Lambda → Create function
+- Runtime: Node.js 18
+- Trigger: S3 upload from `metadata-input-demo`
+- Code snippet:
+  ```js
+  exports.handler = async (event) => {
+    const record = event.Records[0];
+    const key = record.s3.object.key;
+    console.log("uploaded:", key);
+    return { ok: true, key };
+  };
+  ```
+
+Why:
+A simple Lambda function proves the event flow. You can extend it later to extract metadata. This keeps the capstone easy to verify. The console log is the first proof step.
+
+Verify:
+- Upload a file to the bucket.
+- Check CloudWatch logs for the key.
+- This confirms the trigger works.
+
+If it fails:
+- Symptom: no logs.  
+  **Fix:** check the S3 trigger permissions.
+
+### Step 3: Store records in DynamoDB
+Goal:
+Persist extracted metadata.
+
+Actions:
+- AWS Console → DynamoDB → Create table
+- Table name: `metadata-items`
+- Partition key: `id` (string)
+- Update Lambda code to write:
+  ```js
+  // pseudocode
+  // put item with id and metadata
+  ```
+
+Why:
+A database makes the result queryable. DynamoDB keeps it simple with no servers. This step turns logs into data. It also lets the UI fetch real records.
+
+Verify:
+- After uploading, check the DynamoDB table.
+- Expected: a new item appears.
+- This confirms persistence works.
+
+If it fails:
+- Symptom: no new items.  
+  **Fix:** confirm the Lambda IAM role allows DynamoDB writes.
+
+### Step 4: Add a simple frontend in Amplify
+Goal:
+Display the stored metadata in a web UI.
+
+Actions:
+- Create a small frontend that calls an API.
+- Deploy via AWS Amplify.
+- Set the API URL in environment variables.
+
+Why:
+A UI makes the capstone visible. Amplify provides a fast deploy path. Environment variables keep the API configurable. This is enough for a demo view.
+
+Verify:
+- Open the Amplify URL.
+- Expected: a page loads and shows data.
+- This confirms the end to end flow works.
+
+If it fails:
+- Symptom: API errors.  
+  **Fix:** check the API URL and CORS settings.
+
+### Step 5: Add a cost model note
+Goal:
+Document a basic cost estimate.
+
+Actions:
+- File path: `docs/cost-model.md`
+- Add inputs:
+  ```text
+  Inputs:
+  - Requests per month
+  - GB stored
+  - Lambda GB seconds
+  - DynamoDB read and write units
+  ```
+
+Why:
+A cost model shows you thought about pricing. It keeps assumptions visible. It also makes the capstone reproducible. A small note is enough.
+
+Verify:
+- Open the file and confirm all inputs are listed.
+- This confirms the model is documented.
+
+If it fails:
+- Symptom: assumptions missing.  
+  **Fix:** add the missing inputs.
 
 ## Verify it worked
 
-- Upload a test file to S3.
-- Confirm a DynamoDB item is created.
-- Load the Amplify UI and read the metadata.
-
-If you see no DynamoDB items, check the Lambda logs in CloudWatch.
+- Upload triggers Lambda logs.
+- DynamoDB contains metadata records.
+- UI loads and shows data.
+- Cost model inputs are documented.
 
 ## Common mistakes
 
-- **Symptom**: Lambda runs but writes nothing.
-  **Cause**: Missing DynamoDB permissions.
-  **Fix**: Add an IAM policy that allows PutItem.
+- **Symptom:** Lambda never triggers.  
+  **Cause:** Missing S3 trigger.  
+  **Fix:** add the trigger and permissions.
 
-- **Symptom**: UI shows empty data.
-  **Cause**: Wrong table name or region.
-  **Fix**: Check environment variables in the UI config.
+- **Symptom:** DynamoDB writes fail.  
+  **Cause:** IAM role missing permission.  
+  **Fix:** attach the DynamoDB write policy.
 
-- **Symptom**: Costs are unclear.
-  **Cause**: No inputs recorded.
-  **Fix**: Log request counts and storage size before estimating.
+- **Symptom:** UI shows no data.  
+  **Cause:** API URL or CORS mismatch.  
+  **Fix:** update the URL and CORS config.
 
 ## Cheat sheet
 
-- Use a sandbox account.
-- Keep the capstone small.
-- Write a short note for each lab.
-- Verify each service step.
+- S3 upload triggers Lambda.
+- Lambda writes to DynamoDB.
+- UI reads from API.
+- Cost model is a simple note.
 
 ## Next steps
 
-- Add a second test file type.
-- Add simple error handling.
-- Expand the cost model inputs.
+- Add retries and error logs.
+- Add a small test script for the API.
 
-Related links:
+## Related links
 
-- https://docs.aws.amazon.com/lambda/latest/dg/welcome.html
-- https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html
-- https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html
+- https://docs.aws.amazon.com/lambda/
+- https://docs.aws.amazon.com/s3/
+- https://docs.aws.amazon.com/amazondynamodb/
 
-Final CTA:
+## Final CTA
 
-If you want the capstone outline in plain text, ask and I will share it.
+Keep the capstone small and prove each step. That is the honest path.
diff --git a/content/posts/aws-free-tier-honest-guide/index.mdx b/content/posts/aws-free-tier-honest-guide/index.mdx
index f86236a..3a22132 100644
--- a/content/posts/aws-free-tier-honest-guide/index.mdx
+++ b/content/posts/aws-free-tier-honest-guide/index.mdx
@@ -1,117 +1,146 @@
 ---
-title: "AWS Free Tier: A Step by Step Lab Budget Guide"
-date: "2025-09-12"
+title: "AWS Free Tier: What Actually Costs Money"
+date: "2025-07-05"
 slug: "/aws-free-tier-honest-guide"
-tags: ["aws", "cost-management", "learning"]
-description: "A full lab budget workflow that keeps AWS costs visible and small."
-theme: "Cloud & DevOps"
+tags: ["aws", "cloud", "costs"]
+description: "A clear free tier checklist with exact console steps and verification checks."
+theme: "Cloud"
 ---
 
-AWS free tier is useful, but it is not a shield.
+The free tier is only free if you keep tracking it.
 
-This post shows the exact steps I use to keep lab costs under control.
+This post shows a short list of checks that prevent surprise bills.
 
-You will set alerts, track usage, and clean up safely.
+You will set up a budget alert and audit active resources.
 
-If you run labs, this is for you.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
+
+## The promise
+
+By the end, you will:
+
+- check what is running in your account
+- set a cost alert
+- verify the alert works
+
+> "If you cannot see the cost, it will surprise you."
 
 ## What this is
 
-High level, this is a cost control workflow for labs.
+**High level:** The free tier is a set of limited monthly allowances.
 
-Low level, it is a checklist with alerts, usage notes, and cleanup steps.
+**Low level:** You track running resources and set a budget alert in AWS Billing.
 
-Key terms:
+**Key terms:**
 
-- **Free tier**: limited AWS usage that does not cost money within specific limits.
-- **Budget alert**: a notification when spend crosses a set amount.
-- **Cleanup**: deleting resources so they do not keep billing.
+- **Free tier:** AWS usage limits that are free for a period.
+- **Budget alert:** A notification triggered by cost thresholds.
 
 ## What you need
 
-- An AWS account.
-- Access to Billing and Cost Management.
-- A small monthly lab budget.
-
-Docs used:
+- AWS account
+- Access to Billing
+- Email address for alerts
 
-- https://aws.amazon.com/free/
-- https://docs.aws.amazon.com/cost-management/
+## Start to Finish
 
-## Start to finish
+### Step 1: Audit active resources
+Goal:
+Find what is currently running.
 
-1) Set a budget alert.
+Actions:
+- AWS Console → EC2 → Instances
+- Stop anything you do not need.
+- AWS Console → RDS → Databases
+- Delete unused databases.
 
-Why: You need an early warning before spend grows.
+Why:
+Most surprise bills come from resources left running. EC2 and RDS are the usual suspects. This step reduces risk quickly. It also tells you what you are paying for.
 
-2) Pick one service for your lab.
+Verify:
+- Expected: only the resources you intend to keep are running.
+- This confirms your account is clean.
 
-Why: Fewer services means easier tracking.
+If it fails:
+- Symptom: resources reappear.
+- Fix: check for auto scaling or scheduled instances.
 
-3) Log your expected usage.
+### Step 2: Create a budget alert
+Goal:
+Get notified before costs spike.
 
-Why: You need a baseline for cost math.
+Actions:
+- AWS Console → Billing → Budgets → Create budget
+- Budget type: Cost
+- Amount: `$5`
+- Alerts: Email
 
-4) Run the lab.
+Why:
+A small budget alert is cheap insurance. It tells you early when something is wrong. It also makes the free tier safer. This is the easiest guardrail you can add.
 
-Why: You need real usage data.
+Verify:
+- Expected: the budget appears in the list.
+- This confirms the alert is set.
 
-5) Record actual usage.
+If it fails:
+- Symptom: Budgets unavailable.  
+  **Fix:** ensure Billing access is enabled in your account.
 
-Why: You need to compare expected vs actual.
+### Step 3: Check the cost explorer
+Goal:
+See where costs are coming from.
 
-6) Clean up the resources.
+Actions:
+- AWS Console → Cost Explorer
+- Time range: last 7 days
+- Group by: Service
 
-Why: Idle resources keep billing.
+Why:
+Cost Explorer is the fastest way to see cost sources. Grouping by service tells you what to shut down first. This keeps your account honest. It also confirms the alert is not your only signal.
 
-### Example cost math
+Verify:
+- Expected: a chart listing services and costs.
+- This confirms you have visibility.
 
-```text
-Monthly storage cost = GB-month * price_per_gb
-Request cost = requests * price_per_request
-Total = storage + request cost
-```
+If it fails:
+- Symptom: Cost Explorer not enabled.
+- Fix: enable it in Billing settings.
 
 ## Verify it worked
 
-- The budget alert exists.
-- Usage numbers are in your notes.
-- The resources are deleted.
-
-If you still see charges, check for hidden resources like snapshots.
+- Only intended resources are running.
+- Budget alert exists.
+- Cost Explorer shows data.
 
 ## Common mistakes
 
-- **Symptom**: Unexpected charges.
-  **Cause**: Resources left running.
-  **Fix**: Delete and confirm in the console.
+- **Symptom:** Costs appear without warning.  
+  **Cause:** No budget alert.  
+  **Fix:** create a small budget now.
 
-- **Symptom**: No alert emails.
-  **Cause**: Budget alert not configured.
-  **Fix**: Create the alert and test it.
+- **Symptom:** Costs keep growing.  
+  **Cause:** A resource is still running.  
+  **Fix:** stop or delete unused resources.
 
-- **Symptom**: Costs are unclear.
-  **Cause**: No usage notes.
-  **Fix**: Log usage right after the lab.
+- **Symptom:** You cannot access Billing.  
+  **Cause:** Insufficient permissions.  
+  **Fix:** enable billing access for your user.
 
 ## Cheat sheet
 
-- Set a budget alert first.
-- Keep labs small.
-- Record usage and costs.
-- Clean up the same day.
+- Stop unused resources.
+- Add a small budget alert.
+- Check Cost Explorer weekly.
 
 ## Next steps
 
-- Add a short cleanup checklist to your notes.
-- Try a second lab with a new service.
-- Keep the budget number visible.
+- Add a second alert at a higher threshold.
+- Tag resources so you can group costs by project.
 
-Related links:
+## Related links
 
-- https://aws.amazon.com/free/
-- https://docs.aws.amazon.com/cost-management/
+- https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/monitoring-costs.html
 
-Final CTA:
+## Final CTA
 
-If you want my budget note template, ask and I will share it.
+If you use the free tier, set a budget alert today.
diff --git a/content/posts/balancing-school-work-projects/index.mdx b/content/posts/balancing-school-work-projects/index.mdx
index 3183507..c982815 100644
--- a/content/posts/balancing-school-work-projects/index.mdx
+++ b/content/posts/balancing-school-work-projects/index.mdx
@@ -1,108 +1,163 @@
 ---
-title: "Balancing School, Work, and Projects: A Complete Weekly System"
-date: "2025-09-20"
+title: "Balancing Work, School, and Projects: A Simple Schedule"
+date: "2025-04-12"
 slug: "/balancing-school-work-projects"
-tags: ["time-management", "learning", "career"]
-description: "A complete weekly system with steps, checks, and common mistakes."
-theme: "Productivity"
+tags: ["process", "time-management", "career"]
+description: "A concrete weekly plan that balances classes, work, and projects."
+theme: "Process"
 ---
 
-This is a simple weekly system that kept me shipping.
+Balance is not a vibe. It is a schedule you can repeat.
 
-It is not a hustle routine. It is a checklist you can follow.
+This post shows a simple weekly plan and how to track it.
 
-You will learn the schedule, the rules, and the debug steps when it fails.
+You will create a schedule file, time box tasks, and review what worked.
 
-If you are juggling multiple priorities, this is for you.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
 
-## What this is
+## The promise
 
-High level, this is a weekly planning system for small projects.
+By the end, you will:
 
-Low level, it is a set of daily tasks, a weekly review, and a cleanup routine.
+- set weekly time blocks
+- keep one project moving without burnout
+- measure what actually got done
 
-Key terms:
+> "If it is not on the schedule, it will not happen."
 
-- **Weekly review**: a short check to decide what stays and what drops.
-- **Shipping block**: a focused session to push one feature or fix.
-- **Retro note**: a short note about what worked and what did not.
+## What this is
 
-## What you need
+**High level:** This is a weekly planning loop that protects focus time.
 
-- A calendar.
-- A single project you care about.
-- A place to store short notes.
+**Low level:** You write a schedule file, use time blocks, and update one review note each week.
 
-## Start to finish
+**Key terms:**
 
-1) Pick one project for the week.
+- **Time block:** A fixed block of time for one type of work.
+- **Review note:** A short weekly reflection on what worked.
 
-Why: One focus avoids split attention.
+## What you need
 
-2) Write one task per day.
+- A calendar
+- A notes file
+- One weekly review slot
 
-Why: Small tasks keep momentum.
+## Start to Finish
 
-3) Schedule one shipping block.
+### Step 1: Create a weekly schedule file
+Goal:
+Write a schedule you can repeat every week.
+
+Actions:
+- File path: `planning/weekly-schedule.md`
+- Add:
+  ```text
+  Weekly schedule
+  - Mon: class + 60 min project
+  - Tue: work + 60 min project
+  - Wed: class + review
+  - Thu: work + 60 min project
+  - Fri: project block
+  ```
+- Command:
+  ```bash
+  mkdir -p planning
+  $EDITOR planning/weekly-schedule.md
+  ```
+
+Why:
+A written schedule is harder to ignore. It gives you a baseline you can adjust. It also makes time limits visible. This keeps you from overcommitting.
+
+Verify:
+- Open the file and confirm each day has a block.
+- This confirms the schedule exists.
+
+If it fails:
+- Symptom: the file is empty.
+- Fix: add just one line per day and refine later.
+
+### Step 2: Add a project time box
+Goal:
+Protect one focused project block each week.
 
-Why: You need uninterrupted time to finish.
+Actions:
+- Add to `planning/weekly-schedule.md`:
+  ```text
+  - Fri: 2 hour project block
+  ```
+- Add a calendar event for that block.
 
-4) Do a weekly review.
+Why:
+One larger block is where deep work happens. Short daily blocks are good, but a bigger block helps you finish features. This keeps the project moving. The calendar event makes it real.
 
-Why: You need to drop tasks that are not moving the project.
+Verify:
+- Expected: a calendar event exists.
+- This confirms the block is scheduled.
 
-5) Write a retro note.
+If it fails:
+- Symptom: no event.
+- Fix: add it now and set a reminder.
 
-Why: You need to learn from the week.
+### Step 3: Add a weekly review note
+Goal:
+Record what worked and what did not.
 
-### Example weekly note
+Actions:
+- File path: `planning/weekly-review.md`
+- Add:
+  ```text
+  Week of:
+  - What shipped:
+  - What slipped:
+  - One change next week:
+  ```
 
-```text
-Goal:
-What shipped:
-What broke:
-Next small step:
-```
+Why:
+The review keeps the schedule honest. It shows what actually happened. It also helps you adjust the next week. One short note is enough.
 
-## Verify it worked
+Verify:
+- Fill it in once.
+- This confirms the review loop is active.
 
-- You shipped at least one small change.
-- You have a written retro note.
-- The next task is clear.
+If it fails:
+- Symptom: notes stay blank.
+- Fix: write one line per section and move on.
+
+## Verify it worked
 
-If you did not ship, reduce scope next week.
+- A weekly schedule file exists.
+- One project block is on your calendar.
+- A review note is filled out.
 
 ## Common mistakes
 
-- **Symptom**: Tasks keep rolling over.
-  **Cause**: Tasks are too big.
-  **Fix**: Split into smaller pieces.
+- **Symptom:** Project time disappears.  
+  **Cause:** No calendar event.  
+  **Fix:** schedule it like a meeting.
 
-- **Symptom**: No shipping block.
-  **Cause**: No calendar hold.
-  **Fix**: Put it on the calendar first.
+- **Symptom:** You keep overbooking.  
+  **Cause:** No review loop.  
+  **Fix:** adjust the schedule weekly.
 
-- **Symptom**: No retro notes.
-  **Cause**: Skipping reflection.
-  **Fix**: Write three lines only.
+- **Symptom:** Schedule feels unrealistic.  
+  **Cause:** Too many blocks.  
+  **Fix:** cut one block and reassess.
 
 ## Cheat sheet
 
-- One project per week.
-- One task per day.
-- One shipping block.
-- One retro note.
+- Write the schedule.
+- Time box one project block.
+- Review weekly.
 
 ## Next steps
 
-- Try this for two weeks.
-- Drop one task if you miss twice.
-- Keep the notes short.
+- Add a short "done" list for each week.
+- Track time spent once per week.
 
-Related links:
+## Related links
 
-- https://bradleymatera.dev/
+- https://www.calnewport.com/blog/2009/01/12/time-blocking/
 
-Final CTA:
+## Final CTA
 
-If you want my weekly template, ask and I will share it.
+Keep the schedule small enough to follow. That is the only version that works.
diff --git a/content/posts/certifications-continuous-learning/index.mdx b/content/posts/certifications-continuous-learning/index.mdx
index 3286954..b951531 100644
--- a/content/posts/certifications-continuous-learning/index.mdx
+++ b/content/posts/certifications-continuous-learning/index.mdx
@@ -1,112 +1,156 @@
 ---
-title: "AWS Certification Study Plan: A Full Start to Finish Checklist"
-date: "2025-05-12"
+title: "Certifications and Continuous Learning: A Simple Track"
+date: "2025-04-05"
 slug: "/certifications-continuous-learning"
-tags: ["learning", "certifications", "career"]
-description: "A complete study workflow that ties each badge to a small demo."
-theme: "Cloud & DevOps"
+tags: ["certifications", "learning", "career"]
+description: "A concrete way to track cert prep without pretending it is production experience."
+theme: "Career"
 ---
 
-Certifications show learning, not production ownership.
+Certs matter only if you can show what you learned.
 
-This post is a full study system with steps, checks, and honest limits.
+This post shows a simple way to track certification study and proof links.
 
-You will get the workflow and the exact way I pair a badge with a demo.
+You will build a study plan file and a proof list you can update.
 
-If you want a clear study plan, this is for you.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
 
-## What this is
-
-High level, this is a study workflow plus a proof demo for each badge.
-
-Low level, it is a set of steps, note templates, and review checks.
-
-Key terms:
-
-- **Badge**: a certification you earn after passing an exam.
-- **Lab**: a guided practice task.
-- **Proof demo**: a small project that shows the concept in code.
-
-## What you need
-
-- The exam guide.
-- A note system.
-- A small project idea.
+## The promise
 
-Docs used:
+By the end, you will:
 
-- https://aws.amazon.com/certification/
+- track certification study in one file
+- link each cert to proof
+- keep the claims honest
 
-## Start to finish
+> "If a cert is listed, the proof link should be one click away."
 
-1) Read the exam outline.
-
-Why: You need to map what you do and do not know.
-
-2) Mark weak domains.
-
-Why: This tells you what to practice first.
-
-3) Run labs for weak areas.
-
-Why: Labs convert theory into steps you can repeat.
+## What this is
 
-4) Build one small demo.
+**High level:** This is a documentation method for certification progress and proof.
 
-Why: The demo is the proof for the badge.
+**Low level:** You keep a study plan, log the exam date, and link to the public badge.
 
-5) Write a short retro note.
+**Key terms:**
 
-Why: You need to capture what worked.
+- **Badge:** A public URL that verifies the certification.
+- **Study plan:** A list of topics with dates and notes.
 
-### Example study note
+## What you need
 
-```text
-Domain:
-What I missed:
-One lab:
-One demo idea:
-```
+- A notes folder
+- Your badge URLs
+
+## Start to Finish
+
+### Step 1: Create a certification tracker
+Goal:
+List your certs and public proof.
+
+Actions:
+- File path: `docs/certifications.md`
+- Add:
+  ```text
+  Certifications
+  - AWS Certified AI Practitioner: https://www.credly.com/your-badge
+  - AWS SAA: https://www.credly.com/your-badge
+  ```
+
+Why:
+This keeps the proof close to the claim. It also makes audits easy. If you do not have a badge URL, you should not list the cert. This is the easiest way to stay honest.
+
+Verify:
+- Open each link and confirm the badge loads.
+- This confirms the proof is real.
+
+If it fails:
+- Symptom: badge link is private.
+- Fix: make it public or remove it.
+
+### Step 2: Add a study plan
+Goal:
+Write a short plan that you can actually follow.
+
+Actions:
+- File path: `docs/cert-study-plan.md`
+- Add:
+  ```text
+  Study plan
+  - Week 1: core services summary
+  - Week 2: practice questions
+  - Week 3: review gaps
+  ```
+
+Why:
+A short plan keeps you moving. It prevents endless study with no test date. It also shows what you did, not just the result. That makes the learning visible.
+
+Verify:
+- Confirm each week has one clear focus.
+- This confirms the plan is usable.
+
+If it fails:
+- Symptom: plan is too big.  
+  Fix: cut it down to three or four weeks.
+
+### Step 3: Add a proof section to your resume
+Goal:
+Link certs in one place so they can be verified.
+
+Actions:
+- File path: `resume.md`
+- Add a section:
+  ```text
+  Certifications
+  - AWS Certified AI Practitioner (badge link)
+  - AWS Certified Solutions Architect Associate (badge link)
+  ```
+
+Why:
+If you list certs, you should link them. This prevents confusion and removes doubts. It also saves recruiters time. This keeps your story consistent across the site and resume.
+
+Verify:
+- Open the resume file and click the links.
+- This confirms the proof is visible.
+
+If it fails:
+- Symptom: links are missing.  
+  Fix: add the URLs or remove the line.
 
 ## Verify it worked
 
-- You can explain the weak domain in plain terms.
-- You can run the demo without extra setup.
-- Your notes explain the limits clearly.
-
-If you cannot explain it, it is not learned yet.
+- Badge links exist and are public.
+- The study plan is short and dated.
+- The resume matches the tracker.
 
 ## Common mistakes
 
-- **Symptom**: Studying without labs.
-  **Cause**: Trying to memorize only.
-  **Fix**: Run a small lab and write notes.
+- **Symptom:** Certs listed with no proof.  
+  **Cause:** Badge links missing.  
+  **Fix:** add the badge URL or remove the cert.
 
-- **Symptom**: No proof demo.
-  **Cause**: Stopping after the exam.
-  **Fix**: Build a small demo that matches a domain.
+- **Symptom:** Study plan keeps growing.  
+  **Cause:** No deadline.  
+  **Fix:** set a date and keep it short.
 
-- **Symptom**: Notes too long.
-  **Cause**: Overwriting instead of summarizing.
-  **Fix**: Keep notes to a few lines.
+- **Symptom:** Resume and site disagree.  
+  **Cause:** Two sources of truth.  
+  **Fix:** update them together.
 
 ## Cheat sheet
 
-- Read the outline first.
-- Run labs for weak areas.
-- Build one demo per badge.
-- Write a short retro note.
+- Keep a badge link per cert.
+- Keep a short study plan.
+- Keep resume and site aligned.
 
 ## Next steps
 
-- Pick one domain and run a lab this week.
-- Build a tiny demo to prove it.
-- Update your notes with one lesson.
+- Add one study note per week.
+- Add one small project that uses a service from the cert.
 
-Related links:
+## Related links
 
-- https://aws.amazon.com/certification/
+- https://www.credly.com/
 
-Final CTA:
+## Final CTA
 
-If you want my note template, ask and I will share it.
+List only the certs you can prove in one click.
diff --git a/content/posts/cloud-ready-web-experiences/index.mdx b/content/posts/cloud-ready-web-experiences/index.mdx
index bd3aa2b..fbe34a5 100644
--- a/content/posts/cloud-ready-web-experiences/index.mdx
+++ b/content/posts/cloud-ready-web-experiences/index.mdx
@@ -1,108 +1,178 @@
 ---
-title: "Cloud Ready Demos: A Full Pre-Launch Guide"
-date: "2025-06-22"
+title: "Cloud Ready Web Checks: A Small, Honest Prelaunch Checklist"
+date: "2025-06-05"
 slug: "/cloud-ready-web-experiences"
-tags: ["web-development", "accessibility", "process"]
-description: "A full pre-launch guide for small public demos."
-theme: "Front-End & Full-Stack"
+tags: ["web", "reliability", "checklist"]
+description: "A concrete prelaunch checklist with exact files, commands, and verification steps."
+theme: "Process"
 ---
 
-Public demos can look polished and still be honest.
+A launch checklist is only useful if you can run every line.
 
-This post shows a full pre-launch workflow with setup, checks, and clean notes.
+This post shows a small prelaunch checklist you can wire into any web project.
 
-You will get the full sequence from setup to verification.
+You will add scripts, a checklist file, and a few clear checks you can verify.
 
-If you ship demos, this is for you.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
 
-## What this is
-
-High level, this is a pre-launch checklist for small demo sites.
-
-Low level, it is a set of steps with verification and cleanup.
-
-Key terms:
-
-- **Demo**: a public project that proves a feature without full production scope.
-- **Checklist**: a short list of checks you run before publishing.
-- **Reality snapshot**: a short section that lists limits and gaps.
-
-## What you need
+## The promise
 
-- A deployed demo link.
-- A README you can edit.
-- A browser for checks.
+By the end, you will:
 
-## Start to finish
+- add a real checklist file to your repo
+- run three concrete checks before deploy
+- document what you did and what you skipped
 
-1) Review the main flow.
+> "A short checklist you actually run beats a perfect checklist you ignore."
 
-Why: You need the core experience to work before polish.
-
-2) Check accessibility.
-
-Why: A demo should still be usable.
-
-3) Check links and errors.
-
-Why: Broken links kill trust fast.
+## What this is
 
-4) Add a reality snapshot.
+**High level:** A prelaunch checklist is a short list of checks you run before publishing.
 
-Why: You must list limits clearly.
+**Low level:** You add scripts, run commands, capture the output, and record any gaps.
 
-5) Re-test after edits.
+**Key terms:**
 
-Why: Changes can break working flows.
+- **Checklist:** A written list of checks you can repeat.
+- **Build:** The process that turns source code into deployable output.
+- **Smoke check:** A short manual pass of core flows.
 
-### Example reality snapshot
+## What you need
 
-```md
-## Reality snapshot
-- What works
-- What is missing
-- What is planned
-```
+- A web repo with `package.json`
+- A build command that works locally
+- A place to store notes, like `docs/`
+
+## Start to Finish
+
+### Step 1: Create a checklist file
+Goal:
+Create a single source of truth for prelaunch checks.
+
+Actions:
+- File path: `docs/prelaunch-checklist.md`
+- Add:
+  ```text
+  Prelaunch checklist
+  - Build passes locally
+  - Lint passes locally
+  - Core user flow smoke check
+  - 404 page loads
+  - Contact link works
+  ```
+
+Why:
+A checklist is only real if it is written down. Keeping it in `docs/` makes it part of the repo. A short list is easier to keep up to date. This also gives you a place to track gaps later.
+
+Verify:
+- Run:
+  ```bash
+  cat docs/prelaunch-checklist.md
+  ```
+- Expected: the five lines above.
+- This confirms the checklist exists.
+
+If it fails:
+- Symptom: file not found.
+- Fix: create the `docs/` folder and add the file again.
+
+### Step 2: Add scripts for build and lint
+Goal:
+Ensure build and lint commands exist and are consistent.
+
+Actions:
+- File path: `package.json`
+- Add or confirm:
+  ```json
+  {
+    "scripts": {
+      "build": "your-build-command",
+      "lint": "your-lint-command"
+    }
+  }
+  ```
+- Replace the placeholders with your real commands.
+
+Why:
+Scripts standardize how you run checks. This avoids “works on my machine” issues. It also keeps the checklist repeatable. A single entry point is the cleanest path to verification.
+
+Verify:
+- Run:
+  ```bash
+  npm run build
+  npm run lint
+  ```
+- Expected: both commands exit with code 0.
+- This confirms the scripts are real.
+
+If it fails:
+- Symptom: script not found.
+- Fix: update `package.json` with the correct command.
+
+### Step 3: Add a smoke check note
+Goal:
+Document what you manually verify before publish.
+
+Actions:
+- File path: `docs/smoke-check.md`
+- Add:
+  ```text
+  Smoke check
+  - Home page loads
+  - Primary CTA works
+  - One form submits
+  ```
+
+Why:
+Manual checks are valid when written down. They help you catch UI regressions that automation does not cover. This also keeps your claims honest. If you did not check it, do not claim it.
+
+Verify:
+- Run the app and follow the list.
+- Expected: each step works once.
+- This confirms the smoke check is real.
+
+If it fails:
+- Symptom: you cannot reproduce the check.
+- Fix: update the list to match what you actually tested.
 
 ## Verify it worked
 
-- The demo loads in a private window.
-- Keyboard navigation works on key buttons.
-- The reality snapshot matches the demo.
-
-If a link fails, fix it before sharing.
+- `docs/prelaunch-checklist.md` exists and is readable.
+- `npm run build` and `npm run lint` complete without errors.
+- Smoke check steps are listed and verified manually.
 
 ## Common mistakes
 
-- **Symptom**: Users report broken links.
-  **Cause**: Links not checked after deploy.
-  **Fix**: Click every link before publishing.
+- **Symptom:** Lint is skipped before deploy.  
+  **Cause:** No lint script exists.  
+  **Fix:** Add the script and run it once.
 
-- **Symptom**: Accessibility regressions.
-  **Cause**: New styles without contrast checks.
-  **Fix**: Run a quick contrast review.
+- **Symptom:** Checklist grows and stops being used.  
+  **Cause:** Too many items.  
+  **Fix:** Keep it under 10 items and cut anything not run.
 
-- **Symptom**: Confusing scope.
-  **Cause**: No limits listed.
-  **Fix**: Add a reality snapshot.
+- **Symptom:** Smoke checks are vague.  
+  **Cause:** No specific steps.  
+  **Fix:** Write three exact actions and confirm them.
 
 ## Cheat sheet
 
-- Test the main flow.
-- Check accessibility.
-- Fix links and errors.
-- Write a reality snapshot.
+- Write the checklist.
+- Add build and lint scripts.
+- Run both commands.
+- Do one smoke check and note it.
 
 ## Next steps
 
-- Add a small checklist to each demo README.
-- Re-run the checklist before every share.
+- Add a small release note file per deploy.
+- Add a simple 404 test page check.
+- Add one accessibility scan if your tools support it.
 
-Related links:
+## Related links
 
-- https://developer.mozilla.org/docs/Web/Accessibility
-- https://www.w3.org/WAI/standards-guidelines/wcag/
+- https://web.dev/reliable/
+- https://web.dev/learn/console/
 
-Final CTA:
+## Final CTA
 
-If you want my checklist, ask and I will share it.
+Make the checklist short enough that you can finish it before every deploy.
diff --git a/content/posts/containerization-orchestration/index.mdx b/content/posts/containerization-orchestration/index.mdx
index d97fa94..854ae24 100644
--- a/content/posts/containerization-orchestration/index.mdx
+++ b/content/posts/containerization-orchestration/index.mdx
@@ -1,151 +1,195 @@
 ---
-title: "Containerization and Orchestration Labs: Full Walkthrough"
-date: "2025-07-15"
+title: "Containerization and Orchestration: A Small Lab You Can Run"
+date: "2025-06-22"
 slug: "/containerization-and-orchestration"
-tags: ["docker", "containerization", "devops"]
-description: "A full lab guide for container basics with clear steps and verification."
-theme: "Cloud & DevOps"
+tags: ["docker", "kubernetes", "devops"]
+description: "A minimal container lab plus a small orchestration step with exact commands."
+theme: "DevOps"
 ---
 
-This is a lab guide, not a production guide.
+Containerization is easier when you keep the stack small.
 
-I use containers to learn reproducible builds and small service wiring.
+This post shows one container and one orchestration step you can verify.
 
-You will build, run, verify, and clean up a tiny stack.
+You will build a container and run it under Compose.
 
-If you are new to Docker, this is for you.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
 
-## What this is
-
-High level, containers package an app with its dependencies.
-
-Low level, a Dockerfile builds an image and Docker Compose runs services together.
-
-Key terms:
-
-- **Image**: a snapshot of an app and its dependencies.
-- **Container**: a running instance of an image.
-- **Compose**: a YAML file that runs multiple containers.
-
-## What you need
-
-- Docker Desktop installed.
-- A simple app folder.
-- A terminal.
-
-Docs used:
-
-- https://docs.docker.com/get-started/
-- https://docs.docker.com/compose/
-
-## Start to finish
+## The promise
 
-1) Create a Dockerfile.
+By the end, you will:
 
-Why: The Dockerfile tells Docker how to build your image.
+- build one container image
+- run it with Compose
+- verify the service is healthy
 
-Example:
+> "Orchestration is just a repeatable run command."
 
-```Dockerfile
-FROM node:20-alpine
-WORKDIR /app
-COPY package.json package-lock.json ./
-RUN npm ci
-COPY . .
-CMD ["npm", "run", "start"]
-```
-
-2) Build the image.
-
-Why: You need a built image before you can run it.
-
-```bash
-docker build -t demo-app .
-```
-
-3) Run the container.
-
-Why: This proves the image works.
-
-```bash
-docker run -p 3000:3000 demo-app
-```
-
-4) Add Compose if you need a database.
-
-Why: Compose makes multi-service setups simple.
+## What this is
 
-```yaml
-services:
-  app:
-    build: .
-    ports:
-      - "3000:3000"
-  db:
-    image: postgres:15
-    environment:
-      POSTGRES_PASSWORD: example
-```
+**High level:** Containerization packages your app, orchestration runs it consistently.
 
-5) Run Compose.
+**Low level:** You write a Dockerfile, then use Compose to run it with ports.
 
-```bash
-docker compose up
-```
+**Key terms:**
 
-6) Clean up.
+- **Dockerfile:** Build instructions for an image.
+- **Compose:** A YAML file that runs containers together.
+- **Port mapping:** The link between host and container ports.
 
-Why: Stopping containers avoids accidental costs and clutter.
+## What you need
 
-```bash
-docker compose down
-```
+- Docker Desktop
+- A small web server app
+
+## Start to Finish
+
+### Step 1: Create the app
+Goal:
+Create a tiny web server that returns a health response.
+
+Actions:
+- File path: `app/index.js`
+- Add:
+  ```js
+  const express = require("express");
+  const app = express();
+  app.get("/health", (_req, res) => res.json({ ok: true }));
+  app.listen(3000, () => console.log("server on 3000"));
+  ```
+- File path: `app/package.json`
+- Add:
+  ```json
+  {
+    "name": "app",
+    "version": "1.0.0",
+    "main": "index.js",
+    "scripts": { "start": "node index.js" },
+    "dependencies": { "express": "^4.19.2" }
+  }
+  ```
+- Command:
+  ```bash
+  cd app && npm install
+  ```
+
+Why:
+A health route is a clean verification step. Express makes the app tiny. This keeps the lab focused on containers, not frameworks. A small app is easier to debug.
+
+Verify:
+- Run:
+  ```bash
+  npm start
+  ```
+- Expected: `server on 3000`.
+- This confirms the app works.
+
+If it fails:
+- Symptom: module not found.
+- Fix: run `npm install`.
+
+### Step 2: Add a Dockerfile
+Goal:
+Package the app into a container image.
+
+Actions:
+- File path: `app/Dockerfile`
+- Add:
+  ```dockerfile
+  FROM node:18-alpine
+  WORKDIR /app
+  COPY package.json package-lock.json ./
+  RUN npm install --production
+  COPY . .
+  EXPOSE 3000
+  CMD ["npm", "start"]
+  ```
+
+Why:
+The Dockerfile defines how the image is built. Using a small base image keeps builds fast. Exposing the port documents the service. This is the minimum to containerize a Node app.
+
+Verify:
+- Run:
+  ```bash
+  docker build -t lab-app app
+  ```
+- Expected: build completes without errors.
+- This confirms the Dockerfile works.
+
+If it fails:
+- Symptom: missing `package-lock.json`.
+- Fix: run `npm install` to generate it.
+
+### Step 3: Orchestrate with Compose
+Goal:
+Run the container with a repeatable command.
+
+Actions:
+- File path: `docker-compose.yml`
+- Add:
+  ```yaml
+  services:
+    app:
+      build: ./app
+      ports:
+        - "3000:3000"
+  ```
+- Command:
+  ```bash
+  docker compose up --build
+  ```
+
+Why:
+Compose turns the run command into config. This makes it repeatable. It also makes it easy to add more services later. For a small lab, this is enough orchestration.
+
+Verify:
+- Run:
+  ```bash
+  curl http://localhost:3000/health
+  ```
+- Expected: `{ "ok": true }`.
+- This confirms the container is running.
+
+If it fails:
+- Symptom: connection refused.  
+  **Fix:** check port mapping and ensure the container is running.
 
 ## Verify it worked
 
-- You can reach the app at http://localhost:3000
-- Containers show as running in Docker Desktop
-- Logs show the app started without errors
-
-If the container exits, check the logs:
-
-```bash
-docker logs <container-id>
-```
+- `docker compose up` runs.
+- `/health` returns JSON.
+- Logs show the server started.
 
 ## Common mistakes
 
-- **Symptom**: Build fails on npm install.
-  **Cause**: Missing package-lock.json.
-  **Fix**: Copy it into the image or use npm install.
+- **Symptom:** Container exits immediately.  
+  **Cause:** The command is wrong.  
+  **Fix:** use `npm start` and ensure it exists.
 
-- **Symptom**: Port not reachable.
-  **Cause**: Port mapping missing.
-  **Fix**: Use -p 3000:3000 in docker run.
+- **Symptom:** Port not reachable.  
+  **Cause:** No port mapping.  
+  **Fix:** add `3000:3000` in Compose.
 
-- **Symptom**: Compose fails.
-  **Cause**: YAML indentation errors.
-  **Fix**: Re-check spaces and alignment.
+- **Symptom:** Build fails.  
+  **Cause:** Missing lockfile.  
+  **Fix:** run `npm install` and rebuild.
 
 ## Cheat sheet
 
-- Dockerfile builds the image.
-- docker build creates the image.
-- docker run starts one container.
-- docker compose runs multiple services.
-- docker compose down stops them.
+- Build a tiny app.
+- Write a Dockerfile.
+- Run with Compose.
+- Verify with curl.
 
 ## Next steps
 
-- Add a health check route.
-- Add a small .env file and load it.
-- Write short notes for the lab.
+- Add a second service.
+- Add a health check in Compose.
 
-Related links:
+## Related links
 
-- https://docs.docker.com/get-started/
 - https://docs.docker.com/compose/
 
-Final CTA:
+## Final CTA
 
-If you want a starter Dockerfile, ask and I will share it.
+Keep the lab small and verify every step.
diff --git a/content/posts/designing-systems-that-actually-ship/index.mdx b/content/posts/designing-systems-that-actually-ship/index.mdx
index 53d23d2..d3962d4 100644
--- a/content/posts/designing-systems-that-actually-ship/index.mdx
+++ b/content/posts/designing-systems-that-actually-ship/index.mdx
@@ -1,114 +1,161 @@
 ---
-title: "Designing Systems That Ship: A Full Start to Finish Checklist"
-date: "2025-06-01"
+title: "Designing Systems That Actually Ship: A Small Template"
+date: "2025-05-19"
 slug: "/designing-systems-that-actually-ship"
-tags: ["process", "projects", "web-development"]
-description: "A complete project shipping workflow with setup, steps, and verification."
-theme: "Process"
+tags: ["systems", "documentation", "process"]
+description: "A simple system design template with exact files and checks."
+theme: "Systems"
 ---
 
-Shipping is a system, not a mood.
+Systems only ship when the design is short and testable.
 
-This post is a full workflow I use to move a demo from idea to link.
+This post shows a lightweight design template you can finish in one sitting.
 
-You will get the setup, the steps, and the checks.
+You will write a one page design, then verify it against code.
 
-If you want a clear process, this is for you.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
 
-## What this is
-
-High level, this is a repeatable way to ship small software projects.
-
-Low level, it is a set of steps, files, and verification checks.
-
-Key terms:
-
-- **Scope**: the smallest feature set you will ship.
-- **Proof link**: a repo or demo URL that shows the work.
-- **Reality snapshot**: a short list of limits and gaps.
-
-## What you need
+## The promise
 
-- A project idea.
-- A repo you can push to.
-- A place to host a demo.
+By the end, you will:
 
-## Start to finish
+- write a one page system design
+- define scope and boundaries
+- verify the design against a small repo
 
-1) Write a one sentence scope.
+> "If the design cannot fit on one page, it probably will not ship."
 
-Why: Scope controls time and quality.
-
-2) Create the repo and file map.
-
-Why: You need a clean structure before you build.
-
-```text
-/README.md
-/src
-  /components
-  /pages
-/notes
-```
-
-3) Build the main flow.
+## What this is
 
-Why: A working flow proves the idea.
+**High level:** A system design is a short plan for how parts connect.
 
-4) Add a README with limits.
+**Low level:** You document inputs, outputs, and the minimal data flow.
 
-Why: You must write what is missing.
+**Key terms:**
 
-```md
-## Reality snapshot
-- What works
-- What is missing
-- What is planned
-```
+- **Boundary:** The edge between two parts of the system.
+- **Data flow:** How data moves between parts.
+- **Scope:** What the system does and does not do.
 
-5) Deploy and verify.
+## What you need
 
-Why: A demo without a working link is not done.
+- A repo for the system
+- A place to store docs
+
+## Start to Finish
+
+### Step 1: Create a one page design file
+Goal:
+Write a short system design you can actually use.
+
+Actions:
+- File path: `docs/system-design.md`
+- Add:
+  ```text
+  System design
+  Goal:
+  Inputs:
+  Outputs:
+  Boundaries:
+  Data flow:
+  ```
+
+Why:
+A one page template keeps the design tight. It forces you to define scope. It also makes review easier. This prevents vague architecture that never ships.
+
+Verify:
+- Open the file and fill one line per section.
+- This confirms the design is not empty.
+
+If it fails:
+- Symptom: the file grows beyond one page.
+- Fix: cut features or move them to a "next" section.
+
+### Step 2: Map the design to files
+Goal:
+Connect the design to real code locations.
+
+Actions:
+- File path: `docs/system-design.md`
+- Add a mapping section:
+  ```text
+  File map:
+  - API routes: src/api/
+  - UI: src/components/
+  - Data: src/data/
+  ```
+
+Why:
+If the design cannot map to files, it is not real. This step keeps the doc tied to code. It also makes reviews faster. You can confirm the system is implemented where you expect.
+
+Verify:
+- Check that each path exists in the repo.
+- This confirms the mapping is valid.
+
+If it fails:
+- Symptom: paths do not exist.
+- Fix: update the map to match real folders.
+
+### Step 3: Add a "done" definition
+Goal:
+Define the minimum ship criteria.
+
+Actions:
+- File path: `docs/system-design.md`
+- Add:
+  ```text
+  Done when:
+  - API returns 200 on /health
+  - UI renders the core screen
+  - One demo flow is verified
+  ```
+
+Why:
+A clear "done" prevents scope creep. It also gives you a verification target. This makes shipping possible. Without a done definition, work drifts.
+
+Verify:
+- Confirm each bullet can be tested.
+- This proves the criteria are measurable.
+
+If it fails:
+- Symptom: "done" is vague.  
+  **Fix:** rewrite bullets as concrete checks.
 
 ## Verify it worked
 
-- The demo loads in a private window.
-- The main flow works end to end.
-- The README matches the code.
-
-If you cannot verify it, you cannot claim it.
+- The design file is one page.
+- The file map matches the repo.
+- The done criteria are testable.
 
 ## Common mistakes
 
-- **Symptom**: The demo is live but broken.
-  **Cause**: No verification step.
-  **Fix**: Add a short verification checklist.
+- **Symptom:** The design is longer than the code.  
+  **Cause:** Too much scope.  
+  **Fix:** cut features until it fits one page.
 
-- **Symptom**: Claims do not match code.
-  **Cause**: Notes are stale.
-  **Fix**: Update notes after each change.
+- **Symptom:** Design does not match files.  
+  **Cause:** No file map.  
+  **Fix:** add a file map section.
 
-- **Symptom**: Scope explodes.
-  **Cause**: No one sentence goal.
-  **Fix**: Rewrite the scope and cut features.
+- **Symptom:** "Done" is not testable.  
+  **Cause:** Vague wording.  
+  **Fix:** write checks you can run.
 
 ## Cheat sheet
 
-- Write the one sentence scope.
-- Build the main flow first.
-- Add a reality snapshot.
-- Verify the demo link.
+- One page design.
+- File map.
+- Done criteria.
 
 ## Next steps
 
-- Apply this to one demo this week.
-- Remove one unverified claim.
-- Add one proof link.
+- Add one diagram if the system grows.
+- Link the design from the README.
 
-Related links:
+## Related links
 
-- https://bradleymatera.dev/projects/
+- https://12factor.net/
 
-Final CTA:
+## Final CTA
 
-If you want my project template, ask and I will share it.
+Keep the design small enough to ship this week.
diff --git a/content/posts/docker-multilang-proj/index.mdx b/content/posts/docker-multilang-proj/index.mdx
index fa30cab..c688db0 100644
--- a/content/posts/docker-multilang-proj/index.mdx
+++ b/content/posts/docker-multilang-proj/index.mdx
@@ -1,153 +1,314 @@
 ---
-title: "Docker for Multi-Language Projects: Full Setup From Zero"
-date: "2025-06-10"
-description: "A complete lab guide for running two languages in one Docker setup with clear steps and verification."
+title: "Docker Multilang: One Compose File, Three Runtimes"
+date: "2025-02-10"
 slug: "/containerizing-multi-language-projects-with-docker-a-practical-guide"
-tags: ["docker", "containerization", "web-development"]
-theme: "Cloud & DevOps"
+tags: ["docker", "containers", "devops"]
+description: "A concrete guide to containerizing a small multi-language project with exact files and commands."
+theme: "DevOps"
 ---
 
-Running two languages in one repo can get messy fast.
+Containers are simple when you keep the surface area small.
 
-This post is a full lab guide to keep the setup simple and repeatable.
+This post shows a minimal multi-language setup using Docker and Compose.
 
-You will build a two service setup with Docker and Compose.
+You will build a Node API, a Python worker, and a static frontend in one stack.
 
-If you are new to multi-service repos, this is for you.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
 
-## What this is
-
-High level, Docker lets each service run in its own container.
-
-Low level, you write a Dockerfile per service and a Compose file to run them together.
-
-Key terms:
-
-- **Dockerfile**: the build recipe for an image.
-- **Image**: the built artifact that includes your app and dependencies.
-- **Compose**: a YAML file that defines how services run together.
-
-## What you need
-
-- Docker Desktop installed.
-- A repo with two folders, such as `api` and `web`.
-- A terminal.
-
-Docs used:
-
-- https://docs.docker.com/build/
-- https://docs.docker.com/compose/
+## The promise
 
-## Start to finish
+By the end, you will:
 
-1) Create a Dockerfile for each service.
+- run three services with one `docker compose up`
+- understand how ports and networks line up
+- verify each service with a concrete check
 
-Why: Each service should build in isolation.
+> "If each container has one job, Compose becomes easy to reason about."
 
-`api/Dockerfile` example:
-
-```Dockerfile
-FROM node:20-alpine
-WORKDIR /app
-COPY package.json package-lock.json ./
-RUN npm ci
-COPY . .
-CMD ["npm", "run", "start"]
-```
-
-`web/Dockerfile` example:
-
-```Dockerfile
-FROM node:20-alpine
-WORKDIR /app
-COPY package.json package-lock.json ./
-RUN npm ci
-COPY . .
-CMD ["npm", "run", "dev"]
-```
-
-2) Create `docker-compose.yml` at the repo root.
-
-Why: Compose starts both services together.
-
-```yaml
-services:
-  api:
-    build: ./api
-    ports:
-      - "4000:4000"
-  web:
-    build: ./web
-    ports:
-      - "3000:3000"
-    depends_on:
-      - api
-```
-
-3) Build and run the stack.
+## What this is
 
-Why: This confirms both services start.
+**High level:** Docker packages each service with its runtime so they run consistently.
 
-```bash
-docker compose up --build
-```
+**Low level:** You write a Dockerfile per service and a `docker-compose.yml` that wires ports and dependencies.
 
-4) Add a small note in README.
+**Key terms:**
 
-Why: You need steps anyone can repeat.
+- **Image:** A packaged filesystem and runtime.
+- **Container:** A running instance of an image.
+- **Compose:** A config file that runs multiple containers together.
 
-Example:
+## What you need
 
-```md
-## Run locally
-- docker compose up --build
-- open http://localhost:3000
-```
+- Docker Desktop installed
+- Node 18
+- Python 3.11
+
+## Start to Finish
+
+### Step 1: Create the project layout
+Goal:
+Create a folder structure that separates services.
+
+Actions:
+- Commands:
+  ```bash
+  mkdir multilang && cd multilang
+  mkdir api worker web
+  ```
+- File tree:
+  ```text
+  multilang/
+    api/
+    worker/
+    web/
+  ```
+
+Why:
+Clear folders keep Dockerfiles simple and predictable. Each folder becomes a build context. This avoids accidental file leaks into images. It also makes it easy to reason about which service owns which code.
+
+Verify:
+- Run:
+  ```bash
+  ls
+  ```
+- Expected: `api`, `worker`, and `web` directories.
+- This confirms the structure is in place.
+
+If it fails:
+- Symptom: missing folder.
+- Fix: re run the `mkdir` commands.
+
+### Step 2: Add the Node API
+Goal:
+Create a minimal Node API that responds on `/health`.
+
+Actions:
+- File path: `api/index.js`
+- Add:
+  ```js
+  const express = require("express");
+  const app = express();
+  app.get("/health", (_req, res) => res.json({ ok: true }));
+  app.listen(3001, () => console.log("api on 3001"));
+  ```
+- File path: `api/package.json`
+- Add:
+  ```json
+  {
+    "name": "api",
+    "version": "1.0.0",
+    "main": "index.js",
+    "scripts": {
+      "start": "node index.js"
+    },
+    "dependencies": {
+      "express": "^4.19.2"
+    }
+  }
+  ```
+- Command:
+  ```bash
+  cd api && npm install
+  ```
+
+Why:
+A health route is the simplest verification check. Express keeps the example short. Having a fixed port is important for Compose mapping. This step establishes one running service in the stack.
+
+Verify:
+- Run locally:
+  ```bash
+  npm start
+  ```
+- Expected output: `api on 3001`.
+- This confirms the API starts.
+
+If it fails:
+- Symptom: `Cannot find module 'express'`.
+- Fix: run `npm install` in `api/`.
+
+### Step 3: Add the Python worker
+Goal:
+Create a Python script that prints a heartbeat.
+
+Actions:
+- File path: `worker/main.py`
+- Add:
+  ```py
+  import time
+
+  while True:
+      print("worker alive")
+      time.sleep(5)
+  ```
+- File path: `worker/requirements.txt`
+- Add:
+  ```text
+  ```
+
+Why:
+The worker does not need a server. A loop proves it is running. This keeps the example focused on multi runtime containers. You can replace the worker with real jobs later.
+
+Verify:
+- Run:
+  ```bash
+  python3 worker/main.py
+  ```
+- Expected output: `worker alive` every 5 seconds.
+- This confirms the worker runs.
+
+If it fails:
+- Symptom: `python3: command not found`.
+- Fix: install Python 3.11.
+
+### Step 4: Add a static web page
+Goal:
+Create a tiny static site to serve.
+
+Actions:
+- File path: `web/index.html`
+- Add:
+  ```html
+  <h1>Multilang Demo</h1>
+  <p>If you see this, the web container works.</p>
+  ```
+
+Why:
+A static page verifies your web container without extra tooling. It keeps the stack small. You can replace it with a real frontend later. This makes the web service easy to test.
+
+Verify:
+- Open the file in a browser.
+- Expected: the heading and paragraph show.
+- This confirms the file renders.
+
+If it fails:
+- Symptom: blank page.
+- Fix: check the file path and content.
+
+### Step 5: Add Dockerfiles
+Goal:
+Create one Dockerfile per service.
+
+Actions:
+- File path: `api/Dockerfile`
+  ```dockerfile
+  FROM node:18-alpine
+  WORKDIR /app
+  COPY package.json package-lock.json ./
+  RUN npm install --production
+  COPY . .
+  EXPOSE 3001
+  CMD ["npm", "start"]
+  ```
+- File path: `worker/Dockerfile`
+  ```dockerfile
+  FROM python:3.11-slim
+  WORKDIR /app
+  COPY requirements.txt ./
+  RUN pip install -r requirements.txt
+  COPY . .
+  CMD ["python", "main.py"]
+  ```
+- File path: `web/Dockerfile`
+  ```dockerfile
+  FROM nginx:alpine
+  COPY index.html /usr/share/nginx/html/index.html
+  ```
+
+Why:
+Each Dockerfile describes one runtime. Small base images keep builds fast. Copying only required files reduces noise. Exposing the API port makes the mapping explicit.
+
+Verify:
+- Build one image:
+  ```bash
+  docker build -t multilang-api api
+  ```
+- Expected: build completes without errors.
+- This confirms the Dockerfile is valid.
+
+If it fails:
+- Symptom: missing `package-lock.json`.
+- Fix: run `npm install` in `api/` or update the COPY line.
+
+### Step 6: Add Compose
+Goal:
+Run all services with one command.
+
+Actions:
+- File path: `docker-compose.yml`
+- Add:
+  ```yaml
+  services:
+    api:
+      build: ./api
+      ports:
+        - "3001:3001"
+    worker:
+      build: ./worker
+    web:
+      build: ./web
+      ports:
+        - "8080:80"
+  ```
+- Command:
+  ```bash
+  docker compose up --build
+  ```
+
+Why:
+Compose maps your services to ports and builds them together. This is the simplest way to run three containers in one stack. Keeping the file short avoids hidden behavior. You can add networks and env later.
+
+Verify:
+- Open http://localhost:8080
+- Expected: "Multilang Demo" page.
+- Run:
+  ```bash
+  curl http://localhost:3001/health
+  ```
+- Expected: `{ "ok": true }`
+- This confirms the web and API containers run.
+
+If it fails:
+- Symptom: port already in use.
+- Fix: change the host ports in `docker-compose.yml`.
 
 ## Verify it worked
 
-- The web app opens at http://localhost:3000
-- The API responds at http://localhost:4000
-- `docker compose ps` shows both services running
-
-If the API does not respond, check the container logs:
-
-```bash
-docker logs <container-id>
-```
+- Web page loads on port 8080.
+- API health returns JSON on port 3001.
+- Worker logs show "worker alive" every 5 seconds.
 
 ## Common mistakes
 
-- **Symptom**: Build fails on npm install.
-  **Cause**: Missing lockfile in the image.
-  **Fix**: Copy `package-lock.json` before running `npm ci`.
+- **Symptom:** API container exits instantly.  
+  **Cause:** `npm install` did not run.  
+  **Fix:** rebuild with `docker compose up --build`.
 
-- **Symptom**: Web app cannot reach API.
-  **Cause**: Wrong host or port.
-  **Fix**: Use the API service name from Compose.
+- **Symptom:** Web page 404.  
+  **Cause:** Nginx did not receive the `index.html`.  
+  **Fix:** confirm the COPY path in the web Dockerfile.
 
-- **Symptom**: Compose fails to start.
-  **Cause**: YAML indentation errors.
-  **Fix**: Re-check spaces and alignment.
+- **Symptom:** Worker spams errors.  
+  **Cause:** Python file name mismatch.  
+  **Fix:** ensure `main.py` exists in `worker/`.
 
 ## Cheat sheet
 
 - One Dockerfile per service.
-- One Compose file at the root.
-- Build and run with `docker compose up --build`.
-- Verify both ports respond.
+- One port per public service.
+- Compose runs everything.
+- Verify with curl and a browser.
 
 ## Next steps
 
-- Add a health check route.
-- Add a `.env` example file.
-- Add a short troubleshooting section.
+- Add environment variables for config.
+- Add a shared network and service names.
+- Add health checks in Compose.
 
-Related links:
+## Related links
 
 - https://docs.docker.com/compose/
 - https://docs.docker.com/build/
 
-Final CTA:
+## Final CTA
 
-If you want my example repo structure, ask and I will share it.
+Build one service at a time, then compose them together.
diff --git a/content/posts/exploring-zig-efficient-parsing/index.mdx b/content/posts/exploring-zig-efficient-parsing/index.mdx
index 50524d8..4ee62ee 100644
--- a/content/posts/exploring-zig-efficient-parsing/index.mdx
+++ b/content/posts/exploring-zig-efficient-parsing/index.mdx
@@ -1,128 +1,235 @@
 ---
-title: "Zig Parsing Lab: Build a Small OBJ Parser"
-date: "2025-03-10"
+title: "Zig OBJ Parsing: A Small, Fast Parser You Can Verify"
+date: "2025-06-18"
 slug: "/exploring-zig-efficient-parsing"
 tags: ["zig", "parsing", "performance"]
-description: "A complete lab tutorial for parsing OBJ files in Zig with clear steps and checks."
+description: "A complete, minimal OBJ parser in Zig with exact steps, checks, and debug tips."
 theme: "Systems"
 ---
 
-Parsing gets clearer when you build something tiny.
+Parsing is not magic. It is careful input handling and boring loops.
 
-This post is a full lab tutorial for a small OBJ parser in Zig.
+This post shows a minimal OBJ parser in Zig and how to verify it works.
 
-You will parse one file type, log errors, and verify the output.
+You will build one file parser, run it, and confirm the output.
 
-If you are new to Zig, this is a good start.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
 
-## What this is
-
-High level, you read a text file and translate lines into structured data.
-
-Low level, you parse line prefixes like `v` and store numbers in arrays.
+## The promise
 
-Key terms:
+By the end, you will:
 
-- **OBJ**: a simple 3D model file format.
-- **Token**: a piece of a line, such as a float or an index.
-- **Parser**: code that turns text into data.
-
-## What you need
+- understand what an OBJ parser does
+- build a tiny Zig parser that reads vertices
+- verify output with exact expected lines
 
-- Zig 0.13.x installed.
-- A small OBJ file.
-- A terminal.
+> "If the output is predictable, the parser is correct."
 
-Docs used:
+## What this is
 
-- https://ziglang.org/documentation/0.13.0/
-- https://en.wikipedia.org/wiki/Wavefront_.obj_file
+**High level:** An OBJ parser reads a text file and turns lines like `v 0.0 1.0 0.0` into numbers you can use.
 
-## Start to finish
+**Low level:** You open a file, read it line by line, split on spaces, and store floats in a list.
 
-1) Create a Zig project.
+**Key terms:**
 
-Why: You need a clean place to test parsing.
+- **OBJ:** A plain text 3D file format with lines like `v`, `vn`, and `f`.
+- **Parser:** Code that reads text and converts it into structured data.
+- **Allocator:** Zig’s memory manager for dynamic arrays.
+- **Slice:** A view into a contiguous block of memory.
+- **Token:** A split piece of a string, usually from a delimiter.
+- **Struct:** A Zig type that groups fields together.
 
-```bash
-mkdir zig-obj && cd zig-obj
-zig init
-```
+## What you need
 
-2) Load the file line by line.
+- Zig installed (0.12.x or later)
+- A terminal
+- A text editor
 
-Why: OBJ is line-based.
+Optional test file:
 
-```zig
-const line = try reader.readUntilDelimiterOrEofAlloc(allocator, '\n', 2048);
+```text
+v 0.0 1.0 0.0
+v -1.0 0.0 0.0
+v 1.0 0.0 0.0
 ```
 
-3) Match line prefixes.
-
-Why: `v` lines are vertices, `f` lines are faces.
+## Start to Finish
+
+### Step 1: Create the project
+Goal:
+Create a clean Zig project with one executable.
+
+Actions:
+- Command:
+  ```bash
+  mkdir zig-obj && cd zig-obj
+  zig init-exe
+  ```
+- File path: `src/main.zig`
+- Keep the generated `main.zig` for now.
+
+Why:
+This gives you a minimal build target and a predictable folder layout. You need `src/main.zig` so Zig knows where to start. Using `zig init-exe` avoids missing build files. It also makes it easy to run `zig build run` later. This keeps the setup aligned with Zig defaults.
+
+Verify:
+- Run:
+  ```bash
+  zig build
+  ```
+- Expected output: no errors.
+- This confirms the project compiles before you add code.
+
+If it fails:
+- Symptom: `zig: command not found`
+- Fix: install Zig and ensure it is on your PATH.
+
+### Step 2: Add a small OBJ file
+Goal:
+Create a known input file with three vertices.
+
+Actions:
+- File path: `data/sample.obj`
+- Create the file with:
+  ```text
+  v 0.0 1.0 0.0
+  v -1.0 0.0 0.0
+  v 1.0 0.0 0.0
+  ```
+- Command:
+  ```bash
+  mkdir -p data
+  $EDITOR data/sample.obj
+  ```
+
+Why:
+A parser is only as testable as its input. Using three fixed vertices gives you known output. This also avoids huge files while you are still debugging. Keeping the file under `data/` makes it easy to reference and share. You will use this file for verification.
+
+Verify:
+- Run:
+  ```bash
+  cat data/sample.obj
+  ```
+- Expected output: the three lines shown above.
+- This confirms the input is correct.
+
+If it fails:
+- Symptom: `No such file or directory`
+- Fix: re run `mkdir -p data` and create the file again.
+
+### Step 3: Implement the parser
+Goal:
+Read the OBJ file and collect vertices into a list.
+
+Actions:
+- File path: `src/main.zig`
+- Replace the file contents with:
 
 ```zig
-if (std.mem.startsWith(u8, line, "v ")) {
-    // parse vertex
+const std = @import("std");
+
+const Vertex = struct {
+    x: f32,
+    y: f32,
+    z: f32,
+};
+
+pub fn main() !void {
+    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
+    defer _ = gpa.deinit();
+    const allocator = gpa.allocator();
+
+    var file = try std.fs.cwd().openFile("data/sample.obj", .{});
+    defer file.close();
+
+    const reader = file.reader();
+    var buf_reader = std.io.bufferedReader(reader);
+    var in_stream = buf_reader.reader();
+
+    var vertices = std.ArrayList(Vertex).init(allocator);
+    defer vertices.deinit();
+
+    var line_buf: [256]u8 = undefined;
+    while (try in_stream.readUntilDelimiterOrEof(&line_buf, '\n')) |line| {
+        if (line.len == 0) continue;
+        if (line[0] != 'v') continue;
+
+        var it = std.mem.tokenize(u8, line, " ");
+        _ = it.next(); // skip "v"
+        const x = try std.fmt.parseFloat(f32, it.next().?);
+        const y = try std.fmt.parseFloat(f32, it.next().?);
+        const z = try std.fmt.parseFloat(f32, it.next().?);
+        try vertices.append(.{ .x = x, .y = y, .z = z });
+    }
+
+    const out = std.io.getStdOut().writer();
+    for (vertices.items) |v| {
+        try out.print("v {d:.1} {d:.1} {d:.1}\n", .{ v.x, v.y, v.z });
+    }
 }
 ```
 
-4) Parse numbers.
-
-Why: You need floats and indices in numeric form.
-
-Use `std.fmt.parseFloat` for floats.
-
-5) Store parsed values.
-
-Why: You need a data structure to verify later.
+Why:
+This parser is minimal and predictable. It only reads lines that start with `v`, so it avoids faces and normals for now. Tokenizing by space keeps the logic straightforward. `ArrayList` gives you a dynamic list without guessing the vertex count. Printing the parsed vertices gives you a direct verification target.
+
+Verify:
+- Run:
+  ```bash
+  zig build run
+  ```
+- Expected output:
+  ```text
+  v 0.0 1.0 0.0
+  v -1.0 0.0 0.0
+  v 1.0 0.0 0.0
+  ```
+- This confirms the parser reads the file and extracts floats correctly.
+
+If it fails:
+- Symptom: `parseFloat` error or null token
+- Fix: ensure each `v` line has three numeric values and no extra spaces.
 
 ## Verify it worked
 
-- Print the number of vertices and faces.
-- Compare with the input file.
-
-Example output:
-
-```text
-vertices: 8
-faces: 12
-```
+- You see three vertices printed exactly as the input.
+- There are no panics or parse errors.
+- Running twice gives the same output.
 
-If the numbers are wrong, log the line that failed.
+This proves the parser is deterministic for simple inputs.
 
 ## Common mistakes
 
-- **Symptom**: Parser crashes on a line.
-  **Cause**: Line length too small.
-  **Fix**: Increase the buffer size.
+- **Symptom:** Only one vertex prints.  
+  **Cause:** The line buffer is too small.  
+  **Fix:** Increase `line_buf` size to 512.
 
-- **Symptom**: Wrong counts.
-  **Cause**: Ignoring blank or comment lines.
-  **Fix**: Skip lines that start with `#`.
+- **Symptom:** No output at all.  
+  **Cause:** The file path is wrong.  
+  **Fix:** Check that `data/sample.obj` exists and matches the path in code.
 
-- **Symptom**: Float parse errors.
-  **Cause**: Extra spaces.
-  **Fix**: Split by whitespace and trim.
+- **Symptom:** Crashes on lines with tabs.  
+  **Cause:** The tokenizer only splits on spaces.  
+  **Fix:** Replace tabs with spaces or add a tab check.
 
 ## Cheat sheet
 
-- Read line by line.
-- Check line prefixes.
-- Parse tokens into numbers.
-- Log failures.
+- Use `zig init-exe` to scaffold.
+- Put test inputs in `data/`.
+- Read with `readUntilDelimiterOrEof`.
+- Split with `tokenize` and parse floats.
+- Print what you parsed to verify.
 
 ## Next steps
 
-- Add face parsing.
-- Add a simple benchmark.
-- Add unit tests for bad input.
+- Add support for `vn` and `f` lines.
+- Support comments and blank lines.
+- Store vertices in a file format your renderer consumes.
 
-Related links:
+## Related links
 
-- https://ziglang.org/documentation/0.13.0/
+- https://ziglang.org/learn/
 - https://en.wikipedia.org/wiki/Wavefront_.obj_file
 
-Final CTA:
+## Final CTA
 
-If you want the sample OBJ files I used, ask and I will share them.
+Build the parser, run it, and keep the output small enough to reason about.
diff --git a/content/posts/from-medic-to-engineer/index.mdx b/content/posts/from-medic-to-engineer/index.mdx
index 75aa091..8243a61 100644
--- a/content/posts/from-medic-to-engineer/index.mdx
+++ b/content/posts/from-medic-to-engineer/index.mdx
@@ -1,103 +1,151 @@
 ---
-title: "Combat Medic to Software Engineer: A Practical Transfer Guide"
-date: "2025-04-20"
+title: "Combat Medic to Software Engineer: Mapping Skills Without Hype"
+date: "2025-01-11"
 slug: "/from-medic-to-engineer"
-tags: ["career", "storytelling", "learning"]
-description: "A complete guide to what transfers from medicine to software and how to apply it."
+tags: ["career", "communication", "process"]
+description: "A practical method to translate real experience into engineering proof."
 theme: "Career"
 ---
 
-This is not a hype post. It is a practical transfer guide.
+Transferable skills only matter if you can show them.
 
-Some skills carry over. Many do not. I list both.
+This post shows how I map real experience to concrete engineering behavior.
 
-You will get concrete examples and a checklist you can use.
+You will build a simple mapping doc and link it to real work.
 
-If you are changing careers, this is for you.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
+
+## The promise
+
+By the end, you will:
+
+- map past experience to specific engineering actions
+- avoid vague claims and keep it provable
+- build a short proof list you can show
+
+> "If it cannot be shown, it should not be claimed."
 
 ## What this is
 
-High level, this is a mapping of real-world habits to software work.
+**High level:** This is a method to translate past roles into engineering behaviors without exaggeration.
 
-Low level, it is a set of examples, notes, and steps for practice.
+**Low level:** You write a table that links a real behavior to a real artifact like a repo or note.
 
-Key terms:
+**Key terms:**
 
-- **Transferable skill**: a habit that applies in a new context.
-- **Debug log**: a short note that captures a problem and fix.
-- **Practice loop**: repeatable steps to build a habit.
+- **Behavior:** A concrete action you performed.
+- **Artifact:** A file or link that proves the behavior.
 
 ## What you need
 
-- One small project you can change.
-- A place to write short notes.
-- A peer who can read your notes.
+- A notes file
+- A list of projects or posts you can link to
 
-## Start to finish
+## Start to Finish
 
-1) Pick one habit to transfer.
+### Step 1: Create a skills map file
+Goal:
+Make a table that links experience to proof.
 
-Why: You cannot fix everything at once.
+Actions:
+- File path: `docs/skills-map.md`
+- Add:
+  ```text
+  Experience | Behavior | Proof
+  Medic | Calm triage under pressure | Link to a debug post
+  Operations | Process cleanup | Link to a checklist
+  ```
 
-2) Translate it into a software action.
+Why:
+A table forces clarity. It avoids vague phrases like "leadership" without proof. It also makes it easy to review and edit. This is the backbone of honest storytelling.
 
-Why: Habits need a concrete action.
+Verify:
+- Open the file and confirm each row has a link you can click.
+- This confirms the proof exists.
 
-3) Practice it on one small bug.
+If it fails:
+- Symptom: you cannot find proof.
+- Fix: remove the row or replace it with a real artifact.
 
-Why: Small scope keeps it repeatable.
+### Step 2: Add one real example
+Goal:
+Link one skill to a real engineering artifact.
 
-4) Write a short debug log.
+Actions:
+- Example row:
+  ```text
+  Medic | Structured communication | https://bradleymatera.dev/service-worker-held-site-hostage/
+  ```
 
-Why: Notes make the habit visible.
+Why:
+One real example is better than a list of vague traits. It shows you can connect experience to output. This keeps your story grounded. It also gives readers a place to verify.
 
-### Example debug log
+Verify:
+- Click the link.
+- Expected: the post loads.
+- This confirms the proof is accessible.
 
-```text
-Problem:
-What I saw:
-What I tried:
-What fixed it:
-```
+If it fails:
+- Symptom: 404.
+- Fix: update the URL or remove the row.
 
-## Verify it worked
+### Step 3: Add a short summary block
+Goal:
+Explain the mapping in one paragraph.
 
-- You can explain the bug in one paragraph.
-- You can reproduce the fix.
-- You can hand the note to someone else.
+Actions:
+- File path: `docs/skills-map.md`
+- Add:
+  ```text
+  Summary: I map past experience to specific, verifiable behaviors. If I cannot link it, I do not claim it.
+  ```
+
+Why:
+The summary makes the intent explicit. It prevents overreach. It also helps readers understand the method quickly. This keeps the story consistent across pages.
+
+Verify:
+- The summary is one or two sentences.
+- This confirms it is concise.
+
+If it fails:
+- Symptom: summary is long and vague.
+- Fix: cut it to one sentence.
+
+## Verify it worked
 
-If you cannot reproduce it, the note is not done.
+- The table exists.
+- Each row has a working link.
+- The summary is concise.
 
 ## Common mistakes
 
-- **Symptom**: Notes are too long.
-  **Cause**: Trying to capture everything.
-  **Fix**: Keep it to four lines.
+- **Symptom:** Skills sound like buzzwords.  
+  **Cause:** No proof link.  
+  **Fix:** add proof or delete the line.
 
-- **Symptom**: Habits do not stick.
-  **Cause**: No repeat loop.
-  **Fix**: Use the same steps for three weeks.
+- **Symptom:** Claims imply production ownership.  
+  **Cause:** Overreach in wording.  
+  **Fix:** use neutral, verifiable wording.
 
-- **Symptom**: Claims sound bigger than they are.
-  **Cause**: Vague language.
-  **Fix**: Describe the exact action you took.
+- **Symptom:** Too many rows.  
+  **Cause:** Trying to list everything.  
+  **Fix:** keep only what you can prove.
 
 ## Cheat sheet
 
-- Pick one habit.
-- Translate it to one action.
-- Practice on one bug.
-- Write a short log.
+- Map experience to behavior.
+- Link behavior to proof.
+- Cut anything unverified.
 
 ## Next steps
 
-- Pick a second habit next month.
-- Ask a peer to review your notes.
+- Add two more proof links.
+- Use the same table for your resume bullets.
 
-Related links:
+## Related links
 
-- https://bradleymatera.dev/
+- https://bradleymatera.dev/blog/
 
-Final CTA:
+## Final CTA
 
-If you want my note template, ask and I will share it.
+If you cannot prove it, do not claim it. That rule keeps the story strong.
diff --git a/content/posts/full-sail-university/index.mdx b/content/posts/full-sail-university/index.mdx
index 5489e9a..a1f3e81 100644
--- a/content/posts/full-sail-university/index.mdx
+++ b/content/posts/full-sail-university/index.mdx
@@ -1,102 +1,158 @@
 ---
-title: "Full Sail Web Development: A Complete, Honest Breakdown"
-date: "2025-09-18"
+title: "What Formal Training Actually Gave Me"
+date: "2025-01-15"
 slug: "/full-sail-university-web-development"
-tags: ["education", "learning", "career"]
-description: "A full breakdown of what the program covered, what it did not, and how I filled the gaps."
+tags: ["education", "process", "career"]
+description: "A clear breakdown of what I kept from formal training and how I use it now."
 theme: "Career"
 ---
 
-This is a straight breakdown of what the program did and did not teach.
+Formal training only matters if you can reuse it in real projects.
 
-It is not a promo post. It is a clear list of reps and gaps.
+This post shows what I kept from school, how I organize it, and how I verify it.
 
-You will get the structure, the gaps, and the practical steps I used to close them.
+You will build a small structure to preserve course work and keep it honest.
 
-If you are choosing a program, this is for you.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
 
-## What this is
-
-High level, this is a summary of training plus a gap plan.
-
-Low level, it is a list of skills, missing areas, and the steps I used to practice them.
-
-Key terms:
-
-- **Reps**: repeated practice that builds skill.
-- **Gap**: a topic the program did not cover deeply.
-- **Retro note**: a short summary of what worked and what did not.
-
-## What you need
-
-- A way to track notes.
-- One small project outside class.
-- A weekly review habit.
-
-## Start to finish
+## The promise
 
-1) Track weekly reps.
+By the end, you will:
 
-Why: You need a record of what you actually practiced.
+- organize course projects with proof links
+- capture one repeatable process you can use now
+- avoid overstating what school covered
 
-2) List gaps every month.
+> "Keep the parts that still run and delete the rest."
 
-Why: The gaps pile up fast if you do not name them.
+## What this is
 
-3) Build one demo to close one gap.
+**High level:** This is a method for extracting useful habits from formal training.
 
-Why: The demo is the proof that you learned it.
+**Low level:** You archive projects, add proof links, and write short notes about what the project does today.
 
-4) Write a short retro note.
+**Key terms:**
 
-Why: It helps you retain the lesson.
+- **Proof link:** A repo or live URL that shows the project works.
+- **Archive:** A folder for old projects you no longer update.
 
-### Example retro note
+## What you need
 
-```text
-What I learned:
-What I missed:
-What I will practice next:
-```
+- A folder for archived projects
+- A short README template
+- One example project you can still run
+
+## Start to Finish
+
+### Step 1: Create an archive folder
+Goal:
+Store school projects in one place.
+
+Actions:
+- Command:
+  ```bash
+  mkdir -p school-projects/archive
+  ```
+
+Why:
+A clean archive stops old projects from polluting your current work. It also makes the story clear. This is the first step to honest documentation. You can always delete what no longer works.
+
+Verify:
+- Run:
+  ```bash
+  ls school-projects
+  ```
+- Expected: `archive` folder.
+- This confirms the structure is ready.
+
+If it fails:
+- Symptom: folder not found.
+- Fix: re-run the `mkdir` command.
+
+### Step 2: Add a short project README
+Goal:
+Document one project with a clear proof link.
+
+Actions:
+- File path: `school-projects/archive/project-name/README.md`
+- Add:
+  ```text
+  Project: Example Project
+  Status: archived
+  Proof link: https://github.com/yourname/project
+  What still works: builds locally
+  What does not: no live demo
+  ```
+
+Why:
+This keeps the project honest. It shows what still runs and what does not. It also prevents vague claims. A short README is enough to be clear.
+
+Verify:
+- Open the README and confirm the proof link works.
+- This confirms the project is documented.
+
+If it fails:
+- Symptom: no proof link.
+- Fix: add the repo URL or remove the project.
+
+### Step 3: Capture the reusable habit
+Goal:
+Write one habit you still use from training.
+
+Actions:
+- File path: `school-projects/habits.md`
+- Add:
+  ```text
+  Habit: small weekly deliverables
+  How I use it now: weekly demo goals
+  ```
+
+Why:
+The habit is the long term value. This turns education into a repeatable process. It also keeps the post grounded in what you actually do now. The habit should be specific and testable.
+
+Verify:
+- Open the file and confirm the habit is stated in one sentence.
+- This confirms it is usable.
+
+If it fails:
+- Symptom: the habit is vague.
+- Fix: rewrite it with a concrete example.
 
 ## Verify it worked
 
-- You can explain the concept without notes.
-- The demo runs without hidden setup.
-- The retro note is short and clear.
-
-If the demo does not run, the gap is still open.
+- Archive folder exists.
+- One project has a README and proof link.
+- One habit is documented.
 
 ## Common mistakes
 
-- **Symptom**: Too many gaps at once.
-  **Cause**: Trying to cover everything.
-  **Fix**: Pick one gap per month.
+- **Symptom:** Old projects are listed as current.  
+  **Cause:** No archive or status note.  
+  **Fix:** add a status line in README.
 
-- **Symptom**: Demos never ship.
-  **Cause**: No weekly review.
-  **Fix**: Add a short review block.
+- **Symptom:** Claims do not match reality.  
+  **Cause:** No proof link.  
+  **Fix:** add a link or remove the claim.
 
-- **Symptom**: Notes disappear.
-  **Cause**: No consistent storage.
-  **Fix**: Keep notes in one place.
+- **Symptom:** The habit is abstract.  
+  **Cause:** No concrete example.  
+  **Fix:** add one real example.
 
 ## Cheat sheet
 
-- Track weekly reps.
-- List gaps monthly.
-- Ship one demo per gap.
-- Write a short retro note.
+- Archive old projects.
+- Add proof links.
+- Keep one real habit.
 
 ## Next steps
 
-- Choose one gap and build a demo this month.
-- Add a short retro note to the demo README.
+- Move broken projects to archive.
+- Keep only the ones you can still run.
 
-Related links:
+## Related links
 
-- https://bradleymatera.dev/
+- https://guides.github.com/features/wikis/
 
-Final CTA:
+## Final CTA
 
-If you want my gap tracking template, ask and I will share it.
+Keep the useful parts and cut the rest. That is the honest way to do this.
diff --git a/content/posts/gatsby-nav-bar-haunted/index.mdx b/content/posts/gatsby-nav-bar-haunted/index.mdx
index 996b109..bfe647f 100644
--- a/content/posts/gatsby-nav-bar-haunted/index.mdx
+++ b/content/posts/gatsby-nav-bar-haunted/index.mdx
@@ -1,116 +1,153 @@
 ---
-title: "Gatsby Navbar Bug: Full Debug Walkthrough"
-date: "2025-04-04"
+title: "Gatsby Nav Bar Haunted: Fixing a Stuck Dropdown"
+date: "2025-02-02"
 slug: "/gatsby-nav-bar-haunted"
-tags: ["gatsby", "debugging", "web-development"]
-description: "A complete walkthrough of a Gatsby nav bug with steps, verification, and fixes."
-theme: "Front-End & Full-Stack"
+tags: ["gatsby", "react", "debugging"]
+description: "A precise fix for a dropdown that stays open in a Gatsby navigation menu."
+theme: "Debugging"
 ---
 
-My navbar broke after deploy even though it worked locally.
+My nav dropdown stayed open even when it should have closed.
 
-This post shows the full debug path and the fix.
+This post shows the exact fix: a click outside handler and a visibility reset.
 
-You will learn how to reproduce, verify, and prevent it.
+You will add two small effects and verify the menu closes correctly.
 
-If your Gatsby nav breaks only in production, this is for you.
+These steps are verifiable in this repo.
 
-## What this is
-
-High level, this is a routing mismatch caused by build output and path config.
-
-Low level, it is a check of link paths, build output, and trailing slash settings.
-
-Key terms:
-
-- **Base path**: the prefix your site uses when hosted in a subfolder.
-- **Trailing slash**: whether URLs end in `/` or not.
-- **Build output**: the HTML files Gatsby generates.
-
-## What you need
-
-- A Gatsby site.
-- Access to the build output folder.
-- A deploy where the bug happens.
-
-Docs used:
+## The promise
 
-- https://www.gatsbyjs.com/docs/reference/routing/
+By the end, you will:
 
-## Start to finish
+- add a click outside handler for the menu
+- close the dropdown when the nav hides
+- verify the fix in the browser
 
-1) Rebuild the site locally.
+> "If the menu stays open after you click outside, it is your state logic."
 
-Why: You need the output that matches the deploy.
-
-```bash
-gatsby build
-```
-
-2) Inspect the `public` folder.
-
-Why: You need to know the real paths.
-
-```bash
-ls public
-```
-
-3) Compare link paths with output paths.
-
-Why: If they do not match, nav fails.
+## What this is
 
-Example:
+**High level:** A dropdown should close when the user clicks elsewhere or when the nav hides.
 
-```jsx
-<Link to="/about/">About</Link>
-```
+**Low level:** You track open state in React and reset it in event handlers.
 
-4) Check trailing slash settings.
+**Key terms:**
 
-Why: Gatsby can generate different paths based on this.
+- **Dropdown state:** A boolean that says the menu is open or closed.
+- **Click outside handler:** A document listener that closes the menu when you click away.
 
-5) Deploy and test in a private window.
+## What you need
 
-Why: It removes cache effects.
+- This repo
+- A running dev server
+- The navigation component open in your editor
+
+## Start to Finish
+
+### Step 1: Add a click outside handler
+Goal:
+Close the dropdown when a user clicks outside of it.
+
+Actions:
+- File path: `src/@lekoarts/gatsby-theme-minimal-blog/components/navigation.tsx`
+- Add or confirm this effect exists:
+  ```tsx
+  const dropdownRef = React.useRef<HTMLLIElement>(null);
+
+  React.useEffect(() => {
+    const handleClickOutside = (event: MouseEvent) => {
+      if (dropdownRef.current && !dropdownRef.current.contains(event.target as Node)) {
+        setDropdownOpen(false);
+      }
+    };
+    document.addEventListener("mousedown", handleClickOutside);
+    return () => document.removeEventListener("mousedown", handleClickOutside);
+  }, []);
+  ```
+- Make sure the dropdown container uses the ref:
+  ```tsx
+  <li ref={dropdownRef}>
+  ```
+
+Why:
+React state does not update itself when users click away. A document listener lets you detect clicks outside the dropdown. This is the standard fix for stuck menus. Without it, the open state stays true and the menu never closes.
+
+Verify:
+- Run:
+  ```bash
+  bun run dev
+  ```
+- Open the dropdown and click anywhere else.
+- Expected: the dropdown closes.
+- This confirms the click outside handler works.
+
+If it fails:
+- Symptom: dropdown never closes.
+- Fix: ensure the ref is attached to the correct element.
+
+### Step 2: Reset when the nav hides
+Goal:
+Close the dropdown if the nav is hidden or collapsed.
+
+Actions:
+- File path: `src/@lekoarts/gatsby-theme-minimal-blog/components/navigation.tsx`
+- Add or confirm this effect exists:
+  ```tsx
+  React.useEffect(() => {
+    if (!isVisible) {
+      setDropdownOpen(false);
+    }
+  }, [isVisible]);
+  ```
+
+Why:
+A hidden nav should not keep its dropdown state. If the nav is collapsed, the dropdown should be closed. Resetting state when visibility changes prevents ghost menus. This also prevents focus traps in hidden menus.
+
+Verify:
+- Hide the nav (for example, collapse the menu on mobile).
+- Expected: dropdown state resets and stays closed.
+- This confirms the visibility reset works.
+
+If it fails:
+- Symptom: menu stays open when collapsed.
+- Fix: ensure `isVisible` changes are passed correctly to the component.
 
 ## Verify it worked
 
-- Nav links load the correct pages.
-- No 404s after deploy.
-- The same links work in a private window.
-
-If it still fails, check the deploy base path.
+- Dropdown closes on outside click.
+- Dropdown closes when nav is hidden.
+- No console errors appear.
 
 ## Common mistakes
 
-- **Symptom**: Works locally, fails in deploy.
-  **Cause**: Base path mismatch.
-  **Fix**: Align base path and links.
+- **Symptom:** Outside click does nothing.  
+  **Cause:** The ref is attached to the wrong element.  
+  **Fix:** Attach the ref to the dropdown container element.
 
-- **Symptom**: Random 404s.
-  **Cause**: Trailing slash mismatch.
-  **Fix**: Set trailingSlash and update links.
+- **Symptom:** Dropdown closes immediately after opening.  
+  **Cause:** The click handler captures the opening click.  
+  **Fix:** Use `mousedown` and ensure the button is inside the ref container.
 
-- **Symptom**: Old links still load.
-  **Cause**: Cache.
-  **Fix**: Use a private window and clear cache.
+- **Symptom:** Dropdown state gets out of sync.  
+  **Cause:** Multiple state sources.  
+  **Fix:** Keep one boolean state for open and close.
 
 ## Cheat sheet
 
-- Build locally.
-- Inspect `public` paths.
-- Match links to output.
-- Test in a private window.
+- Track dropdown state in React.
+- Close it on outside click.
+- Reset state when nav hides.
+- Verify in the browser.
 
 ## Next steps
 
-- Add a short routing note in your README.
-- Run a build before sharing links.
+- Add keyboard support for Escape to close.
+- Add focus management for accessibility.
 
-Related links:
+## Related links
 
-- https://www.gatsbyjs.com/docs/reference/routing/
+- https://react.dev/reference/react/useEffect
 
-Final CTA:
+## Final CTA
 
-If you hit the same bug, send me your steps and I will compare notes.
+Fix the close behavior first. Fancy styling can wait.
diff --git a/content/posts/github-actions-aws/index.mdx b/content/posts/github-actions-aws/index.mdx
index 89e266f..2aa0aac 100644
--- a/content/posts/github-actions-aws/index.mdx
+++ b/content/posts/github-actions-aws/index.mdx
@@ -1,118 +1,146 @@
 ---
-title: "GitHub Actions for Demos: Full Setup and Verification"
-date: "2025-07-07"
-slug: "/github-actions-aws-automation"
-tags: ["ci-cd", "github-actions", "aws", "automation"]
-description: "A full start to finish CI setup for demos, with clear notes on what is planned."
-theme: "Cloud & DevOps"
+title: "GitHub Actions Basics: One CI Check You Can Trust"
+date: "2025-02-25"
+slug: "/github-actions-aws"
+tags: ["github", "ci", "devops"]
+description: "A minimal GitHub Actions workflow with exact file paths and checks."
+theme: "DevOps"
 ---
 
-This is a demo-scale CI setup, not enterprise automation.
+CI does not need to be complicated to be useful.
 
-I use a small workflow to lint, test, and build demos. Anything beyond that is labeled as planned.
+This post shows a single workflow that runs build and lint.
 
-You will get the workflow, the checks, and the mistakes I avoid.
+You will add one workflow file and verify it in GitHub.
 
-If you are setting up CI for a small project, this is for you.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
 
-## What this is
-
-High level, CI runs checks on every push and pull request.
-
-Low level, it is a YAML workflow in `.github/workflows/`.
-
-Key terms:
-
-- **Workflow**: the YAML file GitHub Actions runs.
-- **Job**: a set of steps in a workflow.
-- **Runner**: the machine that executes the steps.
-
-## What you need
+## The promise
 
-- A GitHub repo.
-- A `package.json` with scripts.
-- GitHub Actions enabled.
+By the end, you will:
 
-Docs used:
+- add a workflow file to your repo
+- run build and lint on every push
+- verify the check appears in GitHub
 
-- https://docs.github.com/actions
+> "If CI runs one real check, it is already worth it."
 
-## Start to finish
-
-1) Create a workflow file.
-
-Why: GitHub Actions only runs files in `.github/workflows/`.
-
-Create `.github/workflows/ci.yml`:
+## What this is
 
-```yaml
-name: ci
-on:
-  push:
-    branches: [main]
-  pull_request:
-    branches: [main]
+**High level:** GitHub Actions runs scripts for you in the cloud when you push code.
 
-jobs:
-  build:
-    runs-on: ubuntu-latest
-    steps:
-      - uses: actions/checkout@v4
-      - uses: actions/setup-node@v4
-        with:
-          node-version: 20
-      - run: npm ci
-      - run: npm run lint
-      - run: npm run test --if-present
-      - run: npm run build
-```
+**Low level:** You add a YAML file under `.github/workflows` and list the commands to run.
 
-2) Commit and push.
+**Key terms:**
 
-Why: The workflow will not run until it is in the repo.
+- **Workflow:** A YAML file that defines CI tasks.
+- **Job:** A set of steps that run on a GitHub runner.
+- **Step:** A command or action inside a job.
 
-3) Check the Actions tab.
+## What you need
 
-Why: You need to confirm the workflow runs.
+- A GitHub repo
+- A build and lint command in `package.json`
+
+## Start to Finish
+
+### Step 1: Add a workflow file
+Goal:
+Create a CI workflow that runs build and lint.
+
+Actions:
+- File path: `.github/workflows/ci.yml`
+- Add:
+  ```yaml
+  name: CI
+  on:
+    push:
+      branches: ["main"]
+    pull_request:
+      branches: ["main"]
+
+  jobs:
+    build:
+      runs-on: ubuntu-latest
+      steps:
+        - uses: actions/checkout@v4
+        - uses: actions/setup-node@v4
+          with:
+            node-version: 18
+        - run: npm install
+        - run: npm run lint
+        - run: npm run build
+  ```
+
+Why:
+This workflow is the smallest useful CI setup. It installs dependencies and runs your real checks. It does not claim to do anything else. This keeps it honest and easy to maintain. You can expand later once this is stable.
+
+Verify:
+- Commit and push the file.
+- In GitHub, open the Actions tab.
+- Expected: a CI run appears and completes.
+- This confirms the workflow is active.
+
+If it fails:
+- Symptom: workflow does not trigger.
+- Fix: check the branch name and YAML indentation.
+
+### Step 2: Verify build output locally
+Goal:
+Ensure the workflow commands pass on your machine.
+
+Actions:
+- Commands:
+  ```bash
+  npm run lint
+  npm run build
+  ```
+
+Why:
+CI should match local behavior. If you cannot run it locally, CI will fail. This step saves time by catching issues before push. It also confirms the scripts are correct.
+
+Verify:
+- Expected: both commands exit with code 0.
+- This confirms CI should pass.
+
+If it fails:
+- Symptom: command not found.
+- Fix: update the script names in the workflow.
 
 ## Verify it worked
 
-- The workflow shows as green in Actions.
-- Lint, test, and build steps complete.
-- Failed steps show a readable error log.
-
-If it fails, fix the script locally and re-run.
+- The Actions tab shows a green check.
+- The workflow logs show both commands ran.
 
 ## Common mistakes
 
-- **Symptom**: Workflow does not run.
-  **Cause**: File not in `.github/workflows/`.
-  **Fix**: Move it and push.
+- **Symptom:** CI fails on install.  
+  **Cause:** Missing lockfile or incompatible Node version.  
+  **Fix:** commit `package-lock.json` and match Node version.
 
-- **Symptom**: npm ci fails.
-  **Cause**: Missing lockfile.
-  **Fix**: Commit `package-lock.json` or use `npm install`.
+- **Symptom:** CI passes but build is wrong.  
+  **Cause:** You are not running the same build locally.  
+  **Fix:** run the exact commands before push.
 
-- **Symptom**: Tests not found.
-  **Cause**: No test script.
-  **Fix**: Use `--if-present` or add a script.
+- **Symptom:** Workflow never triggers.  
+  **Cause:** wrong branch name.  
+  **Fix:** change `main` to your real default branch.
 
 ## Cheat sheet
 
-- Put workflows in `.github/workflows/`.
-- Use `npm ci` for clean installs.
-- Check Actions after every push.
+- Put workflows in `.github/workflows`.
+- Run `lint` and `build` only.
+- Verify in the Actions tab.
 
 ## Next steps
 
-- Add a deploy step if you need it.
-- Add a smoke check script.
-- Keep planned items listed in README.
+- Add tests if you have them.
+- Add a build artifact upload if needed.
 
-Related links:
+## Related links
 
-- https://docs.github.com/actions
+- https://docs.github.com/en/actions
 
-Final CTA:
+## Final CTA
 
-If you want my baseline workflow, ask and I will share it.
+One clean workflow is enough to start. Make it real, then expand it later.
diff --git a/content/posts/github-pages-deployments-confusing/index.mdx b/content/posts/github-pages-deployments-confusing/index.mdx
index aac6f44..8ed829e 100644
--- a/content/posts/github-pages-deployments-confusing/index.mdx
+++ b/content/posts/github-pages-deployments-confusing/index.mdx
@@ -1,111 +1,172 @@
 ---
-title: "GitHub Pages Deployments: Full Fix for Base Path Bugs"
-date: "2025-05-05"
+title: "GitHub Pages Deployments That Stop Being Confusing"
+date: "2025-03-01"
 slug: "/github-pages-deployments-confusing"
-tags: ["github", "deployment", "web-development"]
-description: "A complete fix guide for the most common GitHub Pages base path mistake."
-theme: "Front-End & Full-Stack"
+tags: ["github", "deployment", "web"]
+description: "Exact steps to deploy a static site to GitHub Pages and verify it works."
+theme: "Deployment"
 ---
 
-GitHub Pages fails fast when the base path is wrong.
+GitHub Pages is simple once you wire the base path correctly.
 
-This post shows a full fix with setup, verification, and cleanup.
+This post shows a minimal deployment flow with exact files and checks.
 
-You will get the exact steps and a checklist you can reuse.
+You will publish a static build and verify the live URL.
 
-If your Pages site breaks after deploy, this is for you.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
 
-## What this is
-
-High level, Pages hosts static files under a repo path.
-
-Low level, your app needs a base path that matches the repo name.
-
-Key terms:
-
-- **Base path**: the URL prefix for a repo site.
-- **Project site**: a Pages site served from a repo, not a custom domain.
-- **Deploy**: pushing a build output to Pages.
-
-## What you need
-
-- A GitHub repo.
-- A built static site.
-- GitHub Pages enabled.
+## The promise
 
-Docs used:
+By the end, you will:
 
-- https://docs.github.com/pages
+- set the correct base path
+- publish to GitHub Pages
+- verify the live URL works
 
-## Start to finish
+> "If the base path is wrong, every asset breaks."
 
-1) Confirm the repo name.
-
-Why: The base path must match it.
-
-2) Set the base path in your config.
-
-Why: Static assets must resolve to the repo path.
-
-Example `next.config.js`:
-
-```js
-const isProd = process.env.NODE_ENV === "production";
+## What this is
 
-module.exports = {
-  basePath: isProd ? "/MyRepoName" : "",
-};
-```
+**High level:** GitHub Pages hosts static files from a branch or a folder.
 
-3) Build the site.
+**Low level:** You build the site into a folder and push that folder to `gh-pages`.
 
-Why: You need the correct output for Pages.
+**Key terms:**
 
-```bash
-npm run build
-```
+- **Base path:** The URL prefix where your site lives.
+- **Static build:** The output folder with HTML, CSS, and JS files.
+- **Pages branch:** The branch GitHub Pages serves from.
 
-4) Deploy to Pages.
+## What you need
 
-Why: The build output must be published.
+- A GitHub repo
+- A build command that outputs static files
+- A `gh-pages` deploy script or manual branch push
+
+## Start to Finish
+
+### Step 1: Set the base path
+Goal:
+Make sure the app uses the repo name as the base path.
+
+Actions:
+- File path: `package.json`
+- Add or confirm:
+  ```json
+  {
+    "homepage": "https://yourname.github.io/your-repo"
+  }
+  ```
+
+Why:
+GitHub Pages hosts your site under a repo path. Without the base path, your assets point to `/` and break. This one line fixes 90 percent of broken deployments. It also makes local and production builds consistent.
+
+Verify:
+- Run:
+  ```bash
+  npm run build
+  ```
+- Open the generated `index.html` and confirm asset URLs include `/your-repo/`.
+- This confirms the base path is applied.
+
+If it fails:
+- Symptom: assets still point to `/`.
+- Fix: ensure the `homepage` value is exact and rebuild.
+
+### Step 2: Add a deploy script
+Goal:
+Publish the build folder to GitHub Pages.
+
+Actions:
+- Install the deploy tool:
+  ```bash
+  npm install -D gh-pages
+  ```
+- File path: `package.json`
+- Add:
+  ```json
+  {
+    "scripts": {
+      "deploy": "gh-pages -d dist"
+    }
+  }
+  ```
+- Replace `dist` with your build output folder.
+
+Why:
+`gh-pages` pushes your build output to the Pages branch. The script keeps the command consistent. This makes your deploy repeatable. It also reduces mistakes when you publish updates.
+
+Verify:
+- Run:
+  ```bash
+  npm run deploy
+  ```
+- Expected: a new `gh-pages` branch is created.
+- This confirms the deploy script worked.
+
+If it fails:
+- Symptom: `ENOENT` for the build folder.
+- Fix: check your build output folder name.
+
+### Step 3: Turn on GitHub Pages
+Goal:
+Enable GitHub Pages for the repo.
+
+Actions:
+- GitHub repo settings → Pages
+- Source: Deploy from branch
+- Branch: `gh-pages`
+- Folder: `/`
+
+Why:
+GitHub Pages does not serve anything until it is configured. Picking the correct branch is the most common mistake. This step makes the site live. It is a one time setup.
+
+Verify:
+- Open the Pages URL provided by GitHub.
+- Expected: your site loads with CSS and JS.
+- This confirms the site is live.
+
+If it fails:
+- Symptom: 404 or blank page.
+- Fix: wait for the build to finish, then hard reload.
 
 ## Verify it worked
 
-- The home page loads.
-- Assets load without 404s.
-- Links work in a private window.
-
-If assets 404, the base path is still wrong.
+- The live URL loads.
+- CSS and JS assets load without 404s.
+- Refreshing does not break routes.
 
 ## Common mistakes
 
-- **Symptom**: CSS and JS 404.
-  **Cause**: Missing base path.
-  **Fix**: Set base path and rebuild.
+- **Symptom:** Blank page with console errors.  
+  **Cause:** Wrong base path.  
+  **Fix:** set `homepage` and rebuild.
 
-- **Symptom**: Works locally, fails on Pages.
-  **Cause**: Using local root paths.
-  **Fix**: Use the repo path in config.
+- **Symptom:** 404 on the root URL.  
+  **Cause:** Pages not enabled or wrong branch.  
+  **Fix:** enable Pages on the `gh-pages` branch.
 
-- **Symptom**: Stale assets.
-  **Cause**: Cached service worker.
-  **Fix**: Unregister the service worker.
+- **Symptom:** Old version keeps showing.  
+  **Cause:** Browser cache.  
+  **Fix:** hard reload and clear cache.
 
 ## Cheat sheet
 
-- Base path must equal repo name.
-- Build before deploy.
-- Test in a private window.
+- Set `homepage` to the repo URL.
+- Build the site.
+- Deploy to `gh-pages`.
+- Enable Pages in settings.
 
 ## Next steps
 
-- Add a short deploy note to README.
-- Keep a small checklist in the repo.
+- Add a custom domain if you have one.
+- Add a README section with deploy steps.
 
-Related links:
+## Related links
 
-- https://docs.github.com/pages
+- https://pages.github.com/
+- https://www.npmjs.com/package/gh-pages
 
-Final CTA:
+## Final CTA
 
-If you want a Pages checklist, ask and I will share it.
+Make the base path correct first. Everything else is secondary.
diff --git a/content/posts/how-i-learn-by-doing/index.mdx b/content/posts/how-i-learn-by-doing/index.mdx
index 89e7292..3a68f32 100644
--- a/content/posts/how-i-learn-by-doing/index.mdx
+++ b/content/posts/how-i-learn-by-doing/index.mdx
@@ -1,108 +1,181 @@
 ---
-title: "How I Learn by Building: A Full Start to Finish Method"
-date: "2025-03-18"
+title: "How I Learn by Doing: A Repeatable Mini Project Loop"
+date: "2025-03-14"
 slug: "/how-i-learn-by-doing"
-tags: ["learning", "projects", "web-development"]
-description: "A complete method for learning by shipping small demos."
+tags: ["learning", "process", "projects"]
+description: "A concrete loop for building small demos with proof and short lessons."
 theme: "Process"
 ---
 
-Tutorials are fine. Building is faster.
+I learn faster when I build something small and ship it.
 
-This post is a full method for learning by shipping small demos.
+This post shows the loop I use: pick a goal, build, verify, and document the lesson.
 
-You will get the steps, the checks, and the mistakes to avoid.
+You will get exact steps and a simple template you can reuse.
 
-If you learn best by doing, this is for you.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
+
+## The promise
+
+By the end, you will:
+
+- plan a tiny project you can finish
+- ship it with a live link
+- document what you learned in a repeatable format
+
+> "Small demos with proof teach faster than big plans."
 
 ## What this is
 
-High level, this is a learning loop built around small projects.
+**High level:** A learning loop is a short cycle of build, verify, and document.
 
-Low level, it is a set of steps, a note template, and a verification checklist.
+**Low level:** You write a README, add a live link, and capture one lesson in a notes file.
 
-Key terms:
+**Key terms:**
 
-- **Demo**: a tiny project that proves one concept.
-- **Learning loop**: repeatable steps that turn effort into skill.
-- **Proof link**: a repo or demo URL that shows the work.
+- **Demo:** A small project with one clear feature.
+- **Proof link:** A URL that shows the demo works.
+- **Lesson:** A short note about what you learned and what to change next.
 
 ## What you need
 
-- One small idea.
-- A repo to publish.
-- A short note template.
+- A GitHub repo
+- A live deploy target (GitHub Pages is enough)
+- A notes file in the repo
 
-## Start to finish
+## Start to Finish
 
-1) Pick one concept.
-
-Why: A single focus is easier to complete.
+### Step 1: Define the smallest goal
+Goal:
+Pick a single feature that you can finish in a weekend.
 
-2) Write a one sentence goal.
+Actions:
+- File path: `README.md`
+- Add a goal line:
+  ```text
+  Goal: Play one sound when a user clicks a card.
+  ```
 
-Why: It keeps the demo small.
+Why:
+A single goal keeps the scope small. It reduces unfinished work. It also makes verification simple. You cannot learn from a demo that never ships.
 
-3) Build the smallest version.
+Verify:
+- Expected: the goal is one sentence and measurable.
+- This confirms the scope is small.
 
-Why: You need a working result fast.
+If it fails:
+- Symptom: the goal reads like a full product.
+- Fix: cut features until it fits in one sentence.
 
-4) Write a short note.
+### Step 2: Build the smallest feature
+Goal:
+Implement the core feature only.
+
+Actions:
+- File path: `src/index.js` (or your entry file)
+- Add a button or handler for the one feature.
+- Example:
+  ```js
+  button.addEventListener("click", () => audio.play());
+  ```
+
+Why:
+The smallest feature proves the idea. This keeps the demo real and testable. It also avoids spending time on polish before the feature works. You can add polish later if the core works.
+
+Verify:
+- Run the app locally.
+- Click the button.
+- Expected: the audio plays.
+- This confirms the core feature works.
+
+If it fails:
+- Symptom: nothing happens on click.
+- Fix: check the element ID and the event listener binding.
+
+### Step 3: Add a proof link
+Goal:
+Make the demo public so others can verify it.
 
-Why: The note is the real learning artifact.
+Actions:
+- Deploy to GitHub Pages or another static host.
+- Add the URL to `README.md`:
+  ```text
+  Live demo: https://yourname.github.io/your-repo/
+  ```
 
-5) Publish the link.
+Why:
+A public link is the proof. It also forces you to finish the build step. Without a live link, the demo is just a claim. This is how you build trust.
 
-Why: A demo without a link is not verifiable.
+Verify:
+- Open the URL in a browser.
+- Expected: the demo loads and works.
+- This confirms the demo is real.
 
-### Example note template
+If it fails:
+- Symptom: 404 or blank page.
+- Fix: check the base path and rebuild.
 
-```text
+### Step 4: Write the lesson
 Goal:
-What worked:
-What failed:
-What I learned:
-```
+Record what you learned in a short, repeatable format.
+
+Actions:
+- File path: `docs/lessons.md`
+- Add:
+  ```text
+  Lesson
+  - What worked:
+  - What broke:
+  - What I would change next:
+  ```
+
+Why:
+The lesson is the real output of the project. It keeps your learning visible. It also makes it easier to explain the project later. A short template keeps it from turning into a novel.
+
+Verify:
+- Open `docs/lessons.md` and fill the three lines.
+- This confirms the learning is captured.
+
+If it fails:
+- Symptom: the file stays empty.
+- Fix: write one sentence per line and move on.
 
 ## Verify it worked
 
 - The demo runs locally.
-- The demo runs from the live link.
-- The note explains what is missing.
-
-If you cannot reproduce it, rewrite the note.
+- The demo is live.
+- The lesson template is filled out.
 
 ## Common mistakes
 
-- **Symptom**: Demo never ships.
-  **Cause**: Scope too large.
-  **Fix**: Cut it in half.
+- **Symptom:** The demo never ships.  
+  **Cause:** The goal was too big.  
+  **Fix:** cut it down to one feature.
 
-- **Symptom**: Notes are empty.
-  **Cause**: Skipped reflection.
-  **Fix**: Write three lines only.
+- **Symptom:** The demo works but has no proof link.  
+  **Cause:** You skipped deploy.  
+  **Fix:** deploy before you call it done.
 
-- **Symptom**: Claims are vague.
-  **Cause**: No proof link.
-  **Fix**: Add the repo or demo link.
+- **Symptom:** The lesson is vague.  
+  **Cause:** No template.  
+  **Fix:** use the three line format.
 
 ## Cheat sheet
 
-- One concept.
-- One sentence goal.
-- One small demo.
-- One short note.
+- One feature.
+- One proof link.
+- One lesson template.
+- Repeat.
 
 ## Next steps
 
-- Ship one demo this week.
-- Add a reality snapshot to the README.
-- Share the link with one person.
+- Build a second demo in a different tool.
+- Share one lesson with a peer.
 
-Related links:
+## Related links
 
-- https://bradleymatera.dev/projects/
+- https://pages.github.com/
 
-Final CTA:
+## Final CTA
 
-If you want my note template, ask and I will share it.
+Ship the small demo. Then write the lesson while it is fresh.
diff --git a/content/posts/interactive-portfolios/index.mdx b/content/posts/interactive-portfolios/index.mdx
index 2e7e1a3..176e7a1 100644
--- a/content/posts/interactive-portfolios/index.mdx
+++ b/content/posts/interactive-portfolios/index.mdx
@@ -1,105 +1,206 @@
 ---
-title: "Interactive Portfolios: A Full Guide to Clarity and Proof"
-date: "2025-03-30"
+title: "Interactive Portfolio Basics: Keep It Fun and Still Clear"
+date: "2025-03-29"
 slug: "/interactive-portfolios-recruiter-expectations"
-tags: ["portfolio", "ux", "web-development"]
-description: "A complete guide to building an interactive portfolio that stays clear and honest."
-theme: "Front-End & Full-Stack"
+tags: ["portfolio", "ux", "web"]
+description: "A practical guide to building an interactive portfolio that still scans fast."
+theme: "Portfolio"
 ---
 
-Interactive portfolios can help, but only if they stay readable.
+Portfolios should be readable first, flashy second.
 
-This post is a full guide to balancing motion with clarity and proof.
+This post shows how to build a small interactive portfolio without breaking clarity.
 
-You will get the structure, the steps, and the mistakes to avoid.
+You will set up a basic structure, add one interactive section, and verify accessibility.
 
-If you are building a portfolio site, this is for you.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
 
-## What this is
-
-High level, this is a content and UX guide for portfolio sites.
-
-Low level, it is a layout checklist and proof link process.
-
-Key terms:
-
-- **Proof link**: a repo or demo URL that backs a claim.
-- **CTA**: a clear action like “View demo” or “Read case study.”
-- **Reality snapshot**: a short list of limits and gaps.
-
-## What you need
-
-- A list of projects.
-- A demo link for each project.
-- A short summary for each project.
-
-## Start to finish
+## The promise
 
-1) Write a one sentence summary for each project.
+By the end, you will:
 
-Why: Readers scan fast.
+- build a simple portfolio structure
+- add one interactive section without hiding content
+- verify that a recruiter can scan it fast
 
-2) Add proof links.
+> "If the content is not readable without motion, it is not ready."
 
-Why: Claims need evidence.
+## What this is
 
-3) Keep motion limited.
+**High level:** An interactive portfolio is a normal portfolio with small enhancements like animation or toggles.
 
-Why: Motion should support, not block.
+**Low level:** You keep the core HTML readable, then add a small JavaScript layer that does not hide content.
 
-4) Add a reality snapshot where needed.
+**Key terms:**
 
-Why: Honest limits build trust.
+- **Skip link:** A link that jumps to the main content for keyboard users.
+- **CTA:** The main action you want a reader to take.
+- **Progressive enhancement:** Adding JS only after the core page works.
 
-### Example project block
+## What you need
 
-```md
-Problem: one sentence
-Solution: one sentence
-Proof: repo and demo
-Limits: one sentence
-```
+- A folder for your site
+- A simple web server (optional)
+- A text editor
+
+## Start to Finish
+
+### Step 1: Create the base HTML
+Goal:
+Create a clean, readable structure with headings and sections.
+
+Actions:
+- File path: `index.html`
+- Add:
+  ```html
+  <!doctype html>
+  <html lang="en">
+    <head>
+      <meta charset="utf-8" />
+      <meta name="viewport" content="width=device-width, initial-scale=1" />
+      <title>Bradley Matera - Portfolio</title>
+    </head>
+    <body>
+      <a class="skip-link" href="#main">Skip to content</a>
+      <header>
+        <h1>Bradley Matera</h1>
+        <p>Systems minded software engineer.</p>
+      </header>
+      <main id="main">
+        <section>
+          <h2>Projects</h2>
+          <ul>
+            <li>AnimalSounds</li>
+            <li>WebGPU Triangle Demo</li>
+          </ul>
+        </section>
+        <section>
+          <h2>Contact</h2>
+          <p>Email: bradmatera@gmail.com</p>
+        </section>
+      </main>
+    </body>
+  </html>
+  ```
+
+Why:
+This is the readable core. Headings give structure for scanning. A skip link is a basic accessibility requirement. The portfolio should be usable even with no CSS. This also makes the interactive layer optional.
+
+Verify:
+- Open `index.html` in a browser.
+- Expected: readable text with a working skip link.
+- This confirms the base page works.
+
+If it fails:
+- Symptom: skip link does nothing.
+- Fix: ensure the `main` element has `id="main"`.
+
+### Step 2: Add a small interactive section
+Goal:
+Add one interactive panel without hiding essential content.
+
+Actions:
+- File path: `index.html`
+- Add after Projects:
+  ```html
+  <section>
+    <h2>Focus</h2>
+    <button id="toggle">Show focus areas</button>
+    <ul id="focus" hidden>
+      <li>Clear documentation</li>
+      <li>Repeatable deploys</li>
+      <li>Honest demos</li>
+    </ul>
+  </section>
+  <script>
+    const button = document.getElementById("toggle");
+    const list = document.getElementById("focus");
+    button.addEventListener("click", () => {
+      list.hidden = !list.hidden;
+      button.textContent = list.hidden ? "Show focus areas" : "Hide focus areas";
+    });
+  </script>
+  ```
+
+Why:
+A small toggle is enough to make the page feel interactive. Keeping the content in the DOM means it can still be read. This avoids hiding critical information from search and screen readers. The toggle is a safe, low risk interaction.
+
+Verify:
+- Click the button.
+- Expected: the list shows and hides.
+- This confirms the interaction works.
+
+If it fails:
+- Symptom: button does nothing.
+- Fix: ensure the element IDs match the script.
+
+### Step 3: Add basic styling
+Goal:
+Make the layout readable without overpowering the content.
+
+Actions:
+- File path: `styles.css`
+- Add:
+  ```css
+  body { font-family: system-ui, sans-serif; max-width: 60ch; margin: 2rem auto; line-height: 1.6; }
+  .skip-link { position: absolute; left: -999px; }
+  .skip-link:focus { left: 1rem; background: #000; color: #fff; padding: 0.5rem; }
+  ```
+- Link it in `index.html`:
+  ```html
+  <link rel="stylesheet" href="styles.css" />
+  ```
+
+Why:
+Readable line length matters more than effects. The skip link styles make keyboard navigation visible. This keeps the page accessible while still being clean. The styles stay minimal.
+
+Verify:
+- Reload the page.
+- Expected: text is centered and readable.
+- This confirms CSS is applied.
+
+If it fails:
+- Symptom: no styling applied.
+- Fix: ensure the CSS file name matches the link.
 
 ## Verify it worked
 
-- The homepage explains what you do in one sentence.
-- Each project has a repo or demo link.
-- No section hides the main content.
-
-If a project has no proof link, remove it.
+- The page reads fine without JavaScript.
+- The toggle works when JavaScript is enabled.
+- The skip link is visible on focus.
 
 ## Common mistakes
 
-- **Symptom**: Visitors get lost.
-  **Cause**: Over-designed nav.
-  **Fix**: Keep the nav simple and visible.
+- **Symptom:** Interactive section hides important info.  
+  **Cause:** You moved core content into a toggle.  
+  **Fix:** Keep core content visible and use toggles for extras.
 
-- **Symptom**: Claims feel inflated.
-  **Cause**: No proof links.
-  **Fix**: Add the repo or demo.
+- **Symptom:** Page feels cluttered.  
+  **Cause:** Too many animations.  
+  **Fix:** Keep one or two small interactions.
 
-- **Symptom**: Too much motion.
-  **Cause**: Multiple animated sections.
-  **Fix**: Keep motion to one or two areas.
+- **Symptom:** Skip link does not work.  
+  **Cause:** Missing target ID.  
+  **Fix:** Add `id="main"` to the main element.
 
 ## Cheat sheet
 
-- Clear summary first.
-- Proof link for every claim.
-- Motion stays minimal.
-- Limits are visible.
+- Build a readable HTML base first.
+- Add one small interaction.
+- Keep core content visible.
+- Verify keyboard navigation.
 
 ## Next steps
 
-- Audit your site in 10 minutes.
-- Remove one unverified claim.
-- Add one missing proof link.
+- Add a projects page with case studies.
+- Add a single featured project section.
+- Add a contact form if needed.
 
-Related links:
+## Related links
 
-- https://developer.mozilla.org/docs/Web/Accessibility
-- https://bradleymatera.dev/
+- https://webaim.org/techniques/keyboard/
+- https://web.dev/learn/accessibility/
 
-Final CTA:
+## Final CTA
 
-If you want a fast portfolio review, ask and I will give you notes.
+Ship the simple version first, then add motion only if the content still scans fast.
diff --git a/content/posts/job-hunting-is-poopie/index.mdx b/content/posts/job-hunting-is-poopie/index.mdx
index 95e3176..754b776 100644
--- a/content/posts/job-hunting-is-poopie/index.mdx
+++ b/content/posts/job-hunting-is-poopie/index.mdx
@@ -1,103 +1,154 @@
 ---
-title: "Job Hunting Is Rough: A Full Reality Guide"
-date: "2025-10-20"
+title: "Job Hunting Is Rough: A Practical Reset"
+date: "2025-01-27"
 slug: "/job-hunting-is-poopie"
-tags: ["career", "job-search", "early-career"]
-description: "A complete, blunt guide to what helped and what did not in my search."
+tags: ["career", "process", "mindset"]
+description: "A blunt reset for job hunting with exact steps and honest checks."
 theme: "Career"
 ---
 
-Job hunting is rough. That is the honest starting point.
+Job hunting can mess with your head.
 
-This post is a full breakdown of what helped, what failed, and what I changed.
+This post is a reset. It turns the chaos into a short, repeatable loop.
 
-You will get steps, checks, and a checklist you can copy.
+You will set boundaries, track progress, and avoid burnout.
 
-If you are in the middle of the search, this is for you.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
 
-## What this is
-
-High level, this is a reality check with concrete habits.
-
-Low level, it is a weekly routine plus a proof link system.
-
-Key terms:
-
-- **Proof link**: a repo or demo that backs a skill claim.
-- **Signal**: a response, referral, or interview request.
-- **Noise**: applications that go nowhere.
-
-## What you need
-
-- A simple tracker.
-- A short list of strong projects.
-- One sentence per project that matches the repo.
-
-## Start to finish
+## The promise
 
-1) Write a short skills list.
+By the end, you will:
 
-Why: You need clarity before applying.
+- limit daily job search time
+- track applications without spiraling
+- keep one honest note about what is working
 
-2) Attach proof links to each skill.
+> "A shorter, focused search beats a ten hour panic day."
 
-Why: Proof is your credibility.
+## What this is
 
-3) Apply to a small, focused list.
+**High level:** This is a job search routine that protects energy and focuses on consistency.
 
-Why: Focus beats volume.
+**Low level:** You set a time box, log applications, and write one weekly lesson.
 
-4) Do a weekly review.
+**Key terms:**
 
-Why: You need to cut noise.
+- **Time box:** A fixed block of time for a task.
+- **Recovery day:** A day with no applications to avoid burnout.
 
-### Example weekly note
+## What you need
 
-```text
-What I sent:
-What replied:
-What I will change:
-```
+- A calendar
+- A tracker file
+- A notes file
+
+## Start to Finish
+
+### Step 1: Set a daily time box
+Goal:
+Limit job search time to a fixed window.
+
+Actions:
+- Set a calendar event for a 90 minute block.
+- Write it down in `job-search/notes.md`:
+  ```text
+  Daily time box: 90 minutes
+  ```
+
+Why:
+Time boxes prevent burnout. They also force you to focus on the most important actions. Consistency beats intensity. This keeps job hunting from swallowing the whole day.
+
+Verify:
+- Expected: a calendar event exists for the week.
+- This confirms the time box is real.
+
+If it fails:
+- Symptom: you keep extending the time box.
+- Fix: stop after the alarm and pick up tomorrow.
+
+### Step 2: Track applications
+Goal:
+Log each application once and move on.
+
+Actions:
+- File path: `job-search/tracker.csv`
+- Add headers:
+  ```text
+  company,role,date_applied,status,link,notes
+  ```
+- Add one row per application.
+
+Why:
+A tracker is a memory system. It stops you from reapplying to the same role. It also shows patterns over time. You need data to adjust your approach.
+
+Verify:
+- Expected: at least one row is filled.
+- This confirms tracking is active.
+
+If it fails:
+- Symptom: the tracker stays empty.
+- Fix: add just one row right after each application.
+
+### Step 3: Add a weekly reset note
+Goal:
+Write one honest note about what is working.
+
+Actions:
+- File path: `job-search/weekly-reset.md`
+- Add:
+  ```text
+  Week of:
+  - What worked:
+  - What did not:
+  - One change next week:
+  ```
+
+Why:
+This makes the process less personal and more measurable. It helps you change strategy without spiraling. One small change per week is enough. The note also becomes your proof of progress.
+
+Verify:
+- Expected: the template is filled once per week.
+- This confirms the reset loop is active.
+
+If it fails:
+- Symptom: no weekly notes.
+- Fix: set a recurring reminder and keep it to three lines.
 
 ## Verify it worked
 
-- You can open your tracker and see the next action.
-- Each skill has a proof link.
-- You removed at least one weak target.
-
-If you cannot verify proof links, fix them before applying again.
+- You kept the time box.
+- Applications are logged.
+- Weekly notes exist.
 
 ## Common mistakes
 
-- **Symptom**: Many applications, no interviews.
-  **Cause**: Weak proof links.
-  **Fix**: Strengthen one demo and resubmit.
+- **Symptom:** You apply for hours and feel worse.  
+  **Cause:** No time box.  
+  **Fix:** set one and stop when it ends.
 
-- **Symptom**: Burnout.
-  **Cause**: No weekly cutoff.
-  **Fix**: Set a limit and stop.
+- **Symptom:** You forget where you applied.  
+  **Cause:** No tracker.  
+  **Fix:** log every application the same day.
 
-- **Symptom**: Vague resume bullets.
-  **Cause**: No link to code.
-  **Fix**: Add a repo or demo URL.
+- **Symptom:** You do the same thing every week.  
+  **Cause:** No review note.  
+  **Fix:** write one change each week.
 
 ## Cheat sheet
 
-- Small list of roles.
-- One proof link per claim.
-- Weekly review.
-- Cut noise.
+- Time box daily.
+- Track every application.
+- Reset weekly.
 
 ## Next steps
 
-- Update one project summary today.
-- Remove one claim you cannot prove.
-- Send one focused application.
+- Reduce low quality applications.
+- Focus on better fit roles.
 
-Related links:
+## Related links
 
-- https://bradleymatera.dev/projects/
+- https://www.indeed.com/career-advice/
 
-Final CTA:
+## Final CTA
 
-If you want blunt feedback on your proof links, ask me.
+Keep the process simple and steady. That is what gets results.
diff --git a/content/posts/job-hunting/index.mdx b/content/posts/job-hunting/index.mdx
index b90561a..7e64327 100644
--- a/content/posts/job-hunting/index.mdx
+++ b/content/posts/job-hunting/index.mdx
@@ -1,105 +1,162 @@
 ---
-title: "Job Hunting Workflow: A Complete, Honest System"
-date: "2025-09-01"
+title: "Job Search Notes That Actually Help"
+date: "2025-02-20"
 slug: "/job-hunting"
-tags: ["career", "job-search"]
-description: "A full job search system with steps, checks, and proof link habits."
+tags: ["career", "process", "writing"]
+description: "A practical job search workflow with exact files, steps, and checks."
 theme: "Career"
 ---
 
-Job hunting gets chaotic fast if you do not track proof.
+Job hunting is chaotic unless you make it a repeatable process.
 
-This post is a complete workflow I use to keep the process honest and focused.
+This post shows the exact workflow I use to track applications and improve the resume.
 
-You will get the steps, the checks, and the common mistakes.
+You will create a tracker, a resume folder, and a short feedback loop.
 
-If you are applying right now, this is for you.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
 
-## What this is
-
-High level, this is a job search system that prioritizes proof and clarity.
-
-Low level, it is a tracker, a weekly review, and a proof link checklist.
-
-Key terms:
-
-- **Proof link**: a repo or demo URL that backs a claim.
-- **Tracker**: a simple list of applications and follow-ups.
-- **Weekly review**: a short check to drop weak leads.
-
-## What you need
+## The promise
 
-- A basic spreadsheet or note.
-- A list of target roles.
-- One proof link per skill.
+By the end, you will:
 
-## Start to finish
+- create a simple application tracker
+- organize resume versions in a clear folder
+- run a weekly feedback review
 
-1) Create a tracker.
+> "If you cannot see your pipeline, you cannot fix it."
 
-Why: You need a single source of truth.
-
-Example columns:
-
-```text
-Company | Role | Date | Follow-up | Proof link
-```
-
-2) Pick a small target list.
-
-Why: Broad lists waste time.
+## What this is
 
-3) Attach proof links to each skill.
+**High level:** A job search is a pipeline of applications, follow ups, and improvements.
 
-Why: Claims without proof are weak.
+**Low level:** You track each application, store each resume version, and review results weekly.
 
-4) Do a weekly review.
+**Key terms:**
 
-Why: You need to cut low-quality targets.
+- **Tracker:** A file that records applications and outcomes.
+- **Resume version:** A tailored copy of your resume for one role.
+- **Feedback loop:** A short review to change what is not working.
 
-5) Update the tracker after each action.
+## What you need
 
-Why: Accurate notes prevent repeated work.
+- A folder for job search files
+- A spreadsheet or CSV file
+- A calendar reminder
+
+## Start to Finish
+
+### Step 1: Create a tracker
+Goal:
+Build a single file that tracks every application.
+
+Actions:
+- File path: `job-search/tracker.csv`
+- Add headers:
+  ```text
+  company,role,date_applied,status,link,notes
+  ```
+- Command:
+  ```bash
+  mkdir -p job-search
+  $EDITOR job-search/tracker.csv
+  ```
+
+Why:
+A tracker makes your search visible. It stops you from forgetting who you applied to. It also shows patterns in rejections and responses. Without a tracker, you repeat mistakes.
+
+Verify:
+- Open the file and add one real application row.
+- Expected: one line with real data.
+- This confirms the tracker is live.
+
+If it fails:
+- Symptom: you never update the file.
+- Fix: set a weekly reminder and add one row at a time.
+
+### Step 2: Store resume versions
+Goal:
+Keep each resume version tied to a role.
+
+Actions:
+- File path: `job-search/resumes/`
+- Create one file per role:
+  ```text
+  job-search/resumes/2025-02-20_company_role.pdf
+  ```
+
+Why:
+Tailored resumes are easier to manage when stored with a naming pattern. It also lets you review what you sent later. This prevents conflicting versions. You can compare outcomes against each version.
+
+Verify:
+- Ensure at least one PDF exists in the folder.
+- Expected: file names include date and company.
+- This confirms the structure is consistent.
+
+If it fails:
+- Symptom: files are scattered.
+- Fix: move them into the folder and rename them once.
+
+### Step 3: Add a weekly review note
+Goal:
+Record what is working and what is not.
+
+Actions:
+- File path: `job-search/weekly-notes.md`
+- Add a template:
+  ```text
+  Week of:
+  - Applied:
+  - Interviews:
+  - Rejections:
+  - Change next week:
+  ```
+
+Why:
+A weekly note creates a feedback loop. It forces you to adjust your approach. It also keeps you from repeating the same week. One note per week is enough.
+
+Verify:
+- Fill in the template for the current week.
+- This confirms the loop is active.
+
+If it fails:
+- Symptom: the note stays empty.
+- Fix: add just one line and keep going.
 
 ## Verify it worked
 
-- You can open the tracker and see the next action.
-- Each skill in your resume has a proof link.
-- You removed at least one weak target this week.
-
-If the tracker is messy, simplify it.
+- The tracker has real entries.
+- Resume versions are stored and named.
+- Weekly notes are filled in.
 
 ## Common mistakes
 
-- **Symptom**: Too many applications, no replies.
-  **Cause**: Applying without targeting.
-  **Fix**: Cut the list and improve proof.
+- **Symptom:** You forget where you applied.  
+  **Cause:** No tracker.  
+  **Fix:** add every application on the same day.
 
-- **Symptom**: Resume looks strong but projects are weak.
-  **Cause**: No proof links.
-  **Fix**: Add one working demo per claim.
+- **Symptom:** Resume versions get mixed.  
+  **Cause:** No naming pattern.  
+  **Fix:** add date and company to the file name.
 
-- **Symptom**: No follow-ups.
-  **Cause**: No review cadence.
-  **Fix**: Add a weekly review block.
+- **Symptom:** You keep applying without feedback.  
+  **Cause:** No weekly review.  
+  **Fix:** write one change per week.
 
 ## Cheat sheet
 
-- One tracker.
-- One proof link per skill.
-- Weekly review.
-- Drop weak targets.
+- One tracker file.
+- One resume folder.
+- One weekly note.
 
 ## Next steps
 
-- Tighten your proof links today.
-- Remove one claim you cannot prove.
-- Follow up on two strong roles.
+- Add a short list of target companies.
+- Add a follow up reminder in your calendar.
 
-Related links:
+## Related links
 
-- https://bradleymatera.dev/projects/
+- https://www.indeed.com/career-advice/resumes-cover-letters/how-to-write-a-resume
 
-Final CTA:
+## Final CTA
 
-If you want my tracker template, ask and I will share it.
+Track the work and change one thing each week. That is how this gets better.
diff --git a/content/posts/making-triangle-webgpu-demo-match-reality/index.mdx b/content/posts/making-triangle-webgpu-demo-match-reality/index.mdx
index 1da07ba..47faa13 100644
--- a/content/posts/making-triangle-webgpu-demo-match-reality/index.mdx
+++ b/content/posts/making-triangle-webgpu-demo-match-reality/index.mdx
@@ -1,103 +1,173 @@
 ---
-title: "WebGPU Demo Honesty Audit: Full Step by Step"
+title: "WebGPU Demo Honesty Audit: Make the Write Up Match the Repo"
 date: "2025-07-22"
 slug: "/making-triangle-webgpu-demo-match-reality"
 tags: ["webgpu", "graphics", "debugging"]
-description: "A complete audit process for making a WebGPU write-up match the repo."
+description: "A concrete audit checklist to align a WebGPU write up with actual code."
 theme: "Graphics"
 ---
 
-This post is about honesty, not extra features.
+A demo is only as honest as its write up.
 
-I use this process to make sure a WebGPU write-up matches the repo.
+This post shows how I audit a WebGPU demo post against the repo.
 
-You will get the audit steps, checks, and common mistakes.
+You will compare claims to code, fix mismatches, and verify links.
 
-If you publish demos, this is for you.
+These steps are verifiable in this blog repo.
+
+## The promise
+
+By the end, you will:
+
+- find mismatched claims in a post
+- replace them with verifiable statements
+- confirm every link works
+
+> "If a claim is not visible in the repo, I cut it."
 
 ## What this is
 
-High level, this is a documentation audit.
+**High level:** An honesty audit compares what you wrote to what the code actually does.
 
-Low level, it is a line-by-line check of claims versus code.
+**Low level:** You open the repo, list the real features, and edit the post to match.
 
-Key terms:
+**Key terms:**
 
-- **Claim**: a statement about a feature or result.
-- **Proof link**: a repo file or demo URL.
-- **Audit**: verifying every claim against evidence.
+- **Claim:** A statement about what the project does.
+- **Proof link:** A link to code or a demo that confirms the claim.
+- **Mismatch:** A claim that is not present in the code.
 
 ## What you need
 
-- The demo repo.
-- The demo link.
-- The post draft.
+- The demo repo open in a browser
+- The blog post file open in your editor
+- A list of real features from the repo
 
-## Start to finish
+## Start to Finish
 
-1) List every claim in the post.
+### Step 1: List the real features from the repo
+Goal:
+Write down only what the repo actually contains.
 
-Why: You need a clear inventory.
+Actions:
+- Open the repo in your browser.
+- Read the README and the main entry file.
+- Create a list in a scratch file:
+  ```text
+  Real features:
+  - Renders a triangle
+  - One shader
+  - One render loop
+  ```
 
-2) Match each claim to a file.
+Why:
+You cannot audit without a source of truth. The README and main entry file are the safest sources. A short list keeps you focused. This avoids inflating the write up.
 
-Why: Every claim should have evidence.
+Verify:
+- You can point to the exact file that implements each feature.
+- This confirms your list is grounded in code.
 
-3) Remove anything you cannot prove.
+If it fails:
+- Symptom: You cannot find a feature in the repo.
+- Fix: remove it from the list.
 
-Why: Unverified claims damage trust.
+### Step 2: Open the blog post file
+Goal:
+Edit the post where the claims live.
 
-4) Add a limits section.
+Actions:
+- File path: `content/posts/making-triangle-webgpu-demo-match-reality/index.mdx`
+- Open the file in your editor.
 
-Why: Limits make the scope clear.
+Why:
+This is the source of the claims you need to fix. Editing the post directly keeps the change visible in git. You can review the diff before publishing. This is the only place the public copy lives.
 
-### Example limits block
+Verify:
+- You can see the frontmatter and the body text.
+- This confirms you opened the correct file.
 
-```md
-## Limits
-- No performance benchmarks
-- No advanced shading
-```
+If it fails:
+- Symptom: file not found.
+- Fix: check the `content/posts` folder for the slug name.
 
-## Verify it worked
+### Step 3: Replace mismatched claims
+Goal:
+Remove or rewrite any claim that is not backed by the repo.
+
+Actions:
+- Find each claim in the post body.
+- Replace it with a verifiable line, for example:
+  ```text
+  The demo renders one triangle with a single shader and a single draw call.
+  ```
+- Remove any metric or automation claims you cannot point to in code.
+
+Why:
+Honesty is the product here. If the repo does not show it, the post should not say it. This keeps the blog credible. It also helps you avoid accidental oversell in interviews.
+
+Verify:
+- Every line in the post maps to something you can see in the repo.
+- This confirms the write up matches reality.
 
-- Every claim maps to a file or demo.
-- The limits are clear and visible.
-- The post matches the code.
+If it fails:
+- Symptom: You are unsure about a claim.
+- Fix: delete it or move it to a "Next experiments" section.
+
+### Step 4: Verify links
+Goal:
+Ensure every repo and demo link works.
+
+Actions:
+- Copy each link from the post and open it.
+- If a link 404s, fix it in the post.
+
+Why:
+Broken links make honest claims look fake. Link checks are a fast credibility win. They also prevent wasted time for readers. This step is easy and worth it.
+
+Verify:
+- Each link opens the correct repo or live demo.
+- This confirms your proof links are valid.
+
+If it fails:
+- Symptom: 404 or redirected link.
+- Fix: update the URL or remove the link.
+
+## Verify it worked
 
-If a claim has no proof, remove it.
+- The post contains only claims you can point to in code.
+- All repo and demo links open.
+- No unverified metrics remain.
 
 ## Common mistakes
 
-- **Symptom**: Post feels inflated.
-  **Cause**: Old claims left in.
-  **Fix**: Re-audit after each refactor.
+- **Symptom:** "Planned" features read like they are done.  
+  **Cause:** The post does not label future work.  
+  **Fix:** Add a "Next experiments" section.
 
-- **Symptom**: Demo lacks proof links.
-  **Cause**: Links not added.
-  **Fix**: Add direct repo paths.
+- **Symptom:** Metrics have no proof.  
+  **Cause:** No report or log exists.  
+  **Fix:** remove the metric or add a link to the report.
 
-- **Symptom**: Limits missing.
-  **Cause**: Skipping scope notes.
-  **Fix**: Add a limits section.
+- **Symptom:** Post says WebGPU when repo uses Canvas.  
+  **Cause:** Naming drift over time.  
+  **Fix:** update the title and tags to match the repo.
 
 ## Cheat sheet
 
-- List claims.
-- Match to code.
-- Remove unproven lines.
-- Add limits.
+- List real features first.
+- Replace every claim that lacks proof.
+- Fix every broken link.
+- Use a "Next experiments" section for plans.
 
 ## Next steps
 
-- Audit one demo post today.
-- Add proof links where missing.
+- Run the demo again and capture a short GIF.
+- Add a short "What changed" note in the repo README.
 
-Related links:
+## Related links
 
-- https://github.com/BradleyMatera/TriangleDemo
-- https://bradleymatera.github.io/TriangleDemo/
+- https://developer.chrome.com/docs/web-platform/webgpu/
 
-Final CTA:
+## Final CTA
 
-If you want a second set of eyes on a post, ask me.
+If a claim is not visible in the repo, remove it. That is the whole rule.
diff --git a/content/posts/my-real-ai-development-setup/index.mdx b/content/posts/my-real-ai-development-setup/index.mdx
index 4f0e6fb..ab7059e 100644
--- a/content/posts/my-real-ai-development-setup/index.mdx
+++ b/content/posts/my-real-ai-development-setup/index.mdx
@@ -1,104 +1,158 @@
 ---
-title: "My Real AI Development Setup: Complete Workflow"
-date: "2025-09-05"
+title: "My Real AI Development Setup: Tools and Checks"
+date: "2025-04-20"
 slug: "/my-real-ai-development-setup"
-tags: ["ai", "tools", "productivity"]
-description: "A full workflow for using AI tools with clear verification steps."
+tags: ["ai", "tools", "workflow"]
+description: "A concrete AI dev setup with exact files, commands, and checks."
 theme: "Process"
 ---
 
-This is my actual setup. It is simple and verifiable.
+This is the setup I actually use to work with AI tools.
 
-I use AI for drafts and refactors, then verify everything.
+It is practical, small, and focused on verification.
 
-You will get the tool list, the steps, and the checks.
+You will document your setup and add proof of what you run.
 
-If you use AI in your work, this is for you.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
 
-## What this is
-
-High level, this is a workflow for using AI as a helper.
-
-Low level, it is a set of prompts, checks, and edits.
-
-Key terms:
-
-- **Prompt**: the input you give an AI tool.
-- **Verification**: checking output against docs and code.
-- **Change log**: a short record of what changed.
-
-## What you need
+## The promise
 
-- A text editor.
-- An AI tool like ChatGPT or Copilot.
-- The official docs for your stack.
+By the end, you will:
 
-## Start to finish
+- document your AI tooling in one file
+- add a reproducible install list
+- define a verification step for each tool
 
-1) Define the task in one sentence.
+> "If the tool is not installed and used, it does not belong on the list."
 
-Why: Clear prompts reduce garbage output.
-
-2) Ask for a short plan, not full code.
-
-Why: Plans are easier to verify.
-
-3) Write the first version yourself.
-
-Why: You need to understand the code.
+## What this is
 
-4) Ask AI to refactor or clean up.
+**High level:** A dev setup is a list of tools and the checks you run.
 
-Why: Refactors are safer than full rewrites.
+**Low level:** You write a setup file with commands and verify each tool runs.
 
-5) Verify against docs.
+**Key terms:**
 
-Why: AI can be wrong.
+- **CLI tool:** A command you run from the terminal.
+- **Verification:** A quick check that a tool works.
 
-### Example prompt
+## What you need
 
-```text
-Give me a 5 step plan for this task. Keep it short and direct.
-```
+- A notes folder
+- The tools you actually use
+
+## Start to Finish
+
+### Step 1: Create a setup file
+Goal:
+Document your tools in one place.
+
+Actions:
+- File path: `docs/dev-setup.md`
+- Add:
+  ```text
+  AI dev setup
+  - Node 18
+  - Python 3.11
+  - VS Code
+  ```
+
+Why:
+A single file keeps the setup clear. It avoids scattered notes. This also lets others reproduce your environment. It is the baseline for honest tooling claims.
+
+Verify:
+- Open the file and confirm the list exists.
+- This confirms the setup file is created.
+
+If it fails:
+- Symptom: no file.  
+  **Fix:** create the file and add the list.
+
+### Step 2: Add install commands
+Goal:
+List the exact commands that install the tools.
+
+Actions:
+- File path: `docs/dev-setup.md`
+- Add:
+  ```text
+  Install
+  - Node: https://nodejs.org/
+  - Python: https://www.python.org/downloads/
+  ```
+
+Why:
+A setup is only useful if it is repeatable. Links to official installers are the safest option. This keeps the setup honest. It also avoids vague instructions.
+
+Verify:
+- Click each link.
+- Expected: official download pages open.
+- This confirms the install sources are valid.
+
+If it fails:
+- Symptom: broken link.  
+  **Fix:** update the URL.
+
+### Step 3: Add a verification command
+Goal:
+Prove each tool works on your machine.
+
+Actions:
+- File path: `docs/dev-setup.md`
+- Add:
+  ```text
+  Verify
+  - node -v
+  - python3 --version
+  ```
+
+Why:
+A version command is the simplest proof. It confirms the tool is installed and on PATH. This makes the setup reproducible. It also prevents phantom tools in your list.
+
+Verify:
+- Run the commands and confirm they print a version.
+- This confirms the tools are active.
+
+If it fails:
+- Symptom: command not found.  
+  **Fix:** reinstall the tool or update PATH.
 
 ## Verify it worked
 
-- The code runs locally.
-- The changes match the plan.
-- The docs support the approach.
-
-If you cannot explain a line, remove it.
+- The setup file exists.
+- Install sources are listed.
+- Version commands return output.
 
 ## Common mistakes
 
-- **Symptom**: Code compiles but fails at runtime.
-  **Cause**: Skipped verification.
-  **Fix**: Test each change locally.
+- **Symptom:** Tools listed but not installed.  
+  **Cause:** No verify step.  
+  **Fix:** add version checks.
 
-- **Symptom**: Overly complex output.
-  **Cause**: Prompt too broad.
-  **Fix**: Ask for a smaller plan.
+- **Symptom:** Links point to random guides.  
+  **Cause:** No official source.  
+  **Fix:** link to official downloads.
 
-- **Symptom**: Claims that are not true.
-  **Cause**: Copying AI text into docs.
-  **Fix**: Rewrite in your own words.
+- **Symptom:** The setup grows too large.  
+  **Cause:** Listing tools you do not use.  
+  **Fix:** remove anything you did not use this month.
 
 ## Cheat sheet
 
-- One sentence task.
-- Ask for a plan.
-- Write first version.
-- Verify with docs.
+- One setup file.
+- One install list.
+- One verification per tool.
 
 ## Next steps
 
-- Add a short change log to your README.
-- Keep prompts short.
+- Add your editor extensions list.
+- Add one sample project that proves the setup works.
 
-Related links:
+## Related links
 
-- https://github.com/features/copilot
+- https://nodejs.org/
+- https://www.python.org/downloads/
 
-Final CTA:
+## Final CTA
 
-If you want my prompt list, ask and I will share it.
+Keep the setup short, verified, and honest.
diff --git a/content/posts/native-react/index.mdx b/content/posts/native-react/index.mdx
index 13c4b97..c59fdcf 100644
--- a/content/posts/native-react/index.mdx
+++ b/content/posts/native-react/index.mdx
@@ -1,106 +1,162 @@
 ---
-title: "React Native Prototype: Full Start to Finish Lab"
-date: "2025-05-26"
+title: "React Native Prototype: A Small, Verifiable Start"
+date: "2025-04-28"
 slug: "/native-react"
-tags: ["react", "mobile", "projects"]
-description: "A complete lab guide for a small React Native prototype with clear checks."
-theme: "Front-End & Full-Stack"
+tags: ["react-native", "mobile", "prototype"]
+description: "A minimal React Native prototype with exact steps, checks, and fixes."
+theme: "Mobile"
 ---
 
-This is a prototype lab, not a shipped app.
+Mobile prototypes only matter if they run on a device.
 
-I built it to learn the core flow and document the steps.
+This post shows a small React Native setup you can verify.
 
-You will get setup, build, verify, and cleanup.
+You will create a new app, render one screen, and confirm it runs.
 
-If you are new to React Native, this is for you.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
 
-## What this is
-
-High level, this is a mobile UI prototype using React Native.
-
-Low level, it is a project setup, one screen, and a basic navigation flow.
-
-Key terms:
-
-- **Metro**: the dev server for React Native.
-- **Expo**: a toolchain that simplifies React Native dev.
-- **Prototype**: a small build to test one idea.
-
-## What you need
+## The promise
 
-- Node.js 18 or later.
-- Expo CLI installed.
-- A phone or emulator.
+By the end, you will:
 
-Docs used:
+- create a React Native project
+- render a simple screen
+- verify it runs on a simulator
 
-- https://docs.expo.dev/
-- https://reactnative.dev/docs/getting-started
+> "If it does not run on a device, it is not done."
 
-## Start to finish
-
-1) Create a new Expo app.
-
-Why: Expo handles setup fast.
-
-```bash
-npx create-expo-app my-prototype
-cd my-prototype
-npm start
-```
+## What this is
 
-2) Build one screen.
+**High level:** React Native lets you build mobile apps with React.
 
-Why: You need a working UI to test.
+**Low level:** You use the CLI to scaffold a project and edit `App.tsx`.
 
-3) Add one navigation route.
+**Key terms:**
 
-Why: Navigation is the first real test.
+- **Simulator:** A local device emulator.
+- **Bundle:** The JavaScript code packaged for the app.
 
-4) Run on a device.
+## What you need
 
-Why: Mobile behavior is different from web.
+- Node 18+
+- React Native CLI
+- iOS Simulator or Android Emulator
+
+## Start to Finish
+
+### Step 1: Create the project
+Goal:
+Scaffold a new React Native app.
+
+Actions:
+- Command:
+  ```bash
+  npx react-native@latest init NativeLab
+  cd NativeLab
+  ```
+
+Why:
+The CLI sets up the native project structure. It also creates iOS and Android folders for you. This is the fastest way to get a runnable app. It avoids manual config mistakes.
+
+Verify:
+- Expected: `ios/` and `android/` folders exist.
+- This confirms the scaffold worked.
+
+If it fails:
+- Symptom: CLI error about dependencies.  
+  **Fix:** follow the CLI output and install required tools.
+
+### Step 2: Edit the main screen
+Goal:
+Render one clear view so you can verify changes.
+
+Actions:
+- File path: `App.tsx`
+- Replace the body with:
+  ```tsx
+  import React from "react";
+  import { SafeAreaView, Text } from "react-native";
+
+  export default function App() {
+    return (
+      <SafeAreaView style={{ padding: 24 }}>
+        <Text>NativeLab is running.</Text>
+      </SafeAreaView>
+    );
+  }
+  ```
+
+Why:
+A simple screen is enough to verify the app runs. It avoids extra components. It also makes hot reload obvious. This is your baseline for future changes.
+
+Verify:
+- Run the app and confirm the text appears.
+- This confirms the edit is live.
+
+If it fails:
+- Symptom: red screen error.  
+  **Fix:** check imports and file name.
+
+### Step 3: Run the app
+Goal:
+Launch the app in a simulator.
+
+Actions:
+- Command for iOS:
+  ```bash
+  npx react-native run-ios
+  ```
+- Command for Android:
+  ```bash
+  npx react-native run-android
+  ```
+
+Why:
+Running the app is the proof. It confirms your environment is correct. It also confirms your code compiles. Without this, the project is not real.
+
+Verify:
+- Expected: app opens and shows "NativeLab is running."
+- This confirms the build and run steps work.
+
+If it fails:
+- Symptom: emulator not found.  
+  **Fix:** start the simulator first.
 
 ## Verify it worked
 
-- The app runs on the device.
-- The screen renders without errors.
-- Navigation works in both directions.
-
-If it does not run, restart Metro and clear cache.
+- The app launches.
+- The text appears.
+- No red screen errors show.
 
 ## Common mistakes
 
-- **Symptom**: App fails to load.
-  **Cause**: Metro cache issues.
-  **Fix**: Restart with `expo start -c`.
+- **Symptom:** Metro server not running.  
+  **Cause:** no bundler process.  
+  **Fix:** run `npx react-native start`.
 
-- **Symptom**: Navigation errors.
-  **Cause**: Missing navigation setup.
-  **Fix**: Follow the React Navigation docs.
+- **Symptom:** iOS build fails.  
+  **Cause:** missing Xcode tools.  
+  **Fix:** install Xcode and accept the license.
 
-- **Symptom**: UI looks wrong on device.
-  **Cause**: Using web-only styles.
-  **Fix**: Use React Native style rules.
+- **Symptom:** Android build fails.  
+  **Cause:** emulator not running.  
+  **Fix:** start Android Studio emulator.
 
 ## Cheat sheet
 
-- Use Expo for setup.
-- Build one screen first.
-- Test on a real device.
+- Scaffold with `react-native init`.
+- Edit `App.tsx`.
+- Run on a simulator.
 
 ## Next steps
 
-- Add a second screen.
-- Add a simple form.
-- Keep the prototype small.
+- Add one button.
+- Add a second screen with navigation.
 
-Related links:
+## Related links
 
-- https://docs.expo.dev/
-- https://reactnative.dev/docs/getting-started
+- https://reactnative.dev/docs/environment-setup
 
-Final CTA:
+## Final CTA
 
-If you want a small React Native checklist, ask and I will share it.
+Get one screen running before you build anything bigger.
diff --git a/content/posts/obj-parser/index.mdx b/content/posts/obj-parser/index.mdx
index db06f0f..af7972c 100644
--- a/content/posts/obj-parser/index.mdx
+++ b/content/posts/obj-parser/index.mdx
@@ -1,115 +1,208 @@
 ---
-title: "OBJ Parser Lab: Full Walkthrough"
+title: "OBJ Parser Lab: Count Vertices and Faces in Zig"
 date: "2025-02-22"
 slug: "/obj-parser"
 tags: ["zig", "parsing", "systems"]
-description: "A complete lab guide for parsing OBJ files with clear steps and verification."
+description: "A complete Zig OBJ parsing lab with exact steps, checks, and fixes."
 theme: "Systems"
 ---
 
-This is a small parsing lab. It is not production code.
+This is a small parsing lab, not production code.
 
-I built it to learn parsing, errors, and data validation.
+You will build a minimal OBJ parser that counts vertices and faces.
 
-You will get the full steps and checks.
+It is small enough to reason about and strict enough to verify.
 
-If you want a simple systems exercise, this is for you.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
 
-## What this is
-
-High level, this is a parser that turns text lines into data structures.
-
-Low level, it is a line reader plus token parsing and validation.
+## The promise
 
-Key terms:
+By the end, you will:
 
-- **OBJ**: a text format for 3D geometry.
-- **Token**: a value parsed from a line.
-- **Validation**: checks that the parsed data is correct.
-
-## What you need
-
-- Zig 0.13.x installed.
-- A small OBJ file.
-- A terminal.
-
-Docs used:
-
-- https://ziglang.org/documentation/0.13.0/
-- https://en.wikipedia.org/wiki/Wavefront_.obj_file
+- parse `v` and `f` lines from an OBJ file
+- print counts with a predictable output
+- debug the most common parse errors
 
-## Start to finish
+> "If the counts match the file, the parser is doing its job."
 
-1) Create a Zig project.
-
-Why: You need a clean place to test parsing.
-
-```bash
-mkdir obj-parser && cd obj-parser
-zig init
-```
-
-2) Read the file line by line.
+## What this is
 
-Why: OBJ is line-based.
+**High level:** An OBJ parser reads a text file and extracts geometry data.
 
-```zig
-const line = try reader.readUntilDelimiterOrEofAlloc(allocator, '\n', 2048);
-```
+**Low level:** You read each line, split tokens, and increment counters for `v` and `f` lines.
 
-3) Parse vertex lines.
+**Key terms:**
 
-Why: Vertices are the core data.
+- **OBJ:** Plain text file format for 3D geometry.
+- **Vertex line:** A line starting with `v`.
+- **Face line:** A line starting with `f`.
+- **Token:** A split part of a line.
 
-4) Count faces.
+## What you need
 
-Why: Face counts verify the file.
+- Zig installed
+- A terminal
+- A test OBJ file
+
+## Start to Finish
+
+### Step 1: Create the project
+Goal:
+Scaffold a Zig project and add a test OBJ file.
+
+Actions:
+- Commands:
+  ```bash
+  mkdir obj-parser && cd obj-parser
+  zig init-exe
+  mkdir data
+  ```
+- File path: `data/sample.obj`
+- Add:
+  ```text
+  v 0 0 0
+  v 1 0 0
+  v 1 1 0
+  v 0 1 0
+  f 1 2 3
+  f 1 3 4
+  ```
+
+Why:
+You need a tiny, known file to verify counts. Four vertices and two faces keep the output simple. Using `zig init-exe` ensures the build system is ready. This keeps the lab deterministic.
+
+Verify:
+- Run:
+  ```bash
+  cat data/sample.obj
+  ```
+- Expected: the exact lines above.
+- This confirms the input is correct.
+
+If it fails:
+- Symptom: missing file.
+- Fix: ensure `data/` exists and save the file again.
+
+### Step 2: Implement the parser
+Goal:
+Count vertices and faces by reading the file line by line.
+
+Actions:
+- File path: `src/main.zig`
+- Replace contents with:
+  ```zig
+  const std = @import("std");
+
+  pub fn main() !void {
+      var gpa = std.heap.GeneralPurposeAllocator(.{}){};
+      defer _ = gpa.deinit();
+      const allocator = gpa.allocator();
+
+      var file = try std.fs.cwd().openFile("data/sample.obj", .{});
+      defer file.close();
+
+      var buf_reader = std.io.bufferedReader(file.reader());
+      var in_stream = buf_reader.reader();
+
+      var vertex_count: usize = 0;
+      var face_count: usize = 0;
+
+      var line_buf: [256]u8 = undefined;
+      while (try in_stream.readUntilDelimiterOrEof(&line_buf, '\n')) |line| {
+          if (line.len == 0) continue;
+          if (line[0] == 'v') vertex_count += 1;
+          if (line[0] == 'f') face_count += 1;
+      }
+
+      const out = std.io.getStdOut().writer();
+      try out.print("vertices: {d}\nfaces: {d}\n", .{ vertex_count, face_count });
+  }
+  ```
+
+Why:
+Counting lines is the simplest parser that still proves the flow works. It forces you to handle file IO and line splitting. It also gives a clear verification output. This is a safe first step before parsing full geometry.
+
+Verify:
+- Run:
+  ```bash
+  zig build run
+  ```
+- Expected output:
+  ```text
+  vertices: 4
+  faces: 2
+  ```
+- This confirms the parser counts lines correctly.
+
+If it fails:
+- Symptom: counts are wrong.
+- Fix: ensure your sample file matches the lines above.
+
+### Step 3: Add a basic validation check
+Goal:
+Detect invalid lines early and print a warning.
+
+Actions:
+- File path: `src/main.zig`
+- Add inside the loop:
+  ```zig
+  if (line[0] != 'v' and line[0] != 'f') {
+      const out = std.io.getStdOut().writer();
+      try out.print("skipped line: {s}\n", .{line});
+  }
+  ```
+
+Why:
+OBJ files can include comments and other line types. Printing skipped lines helps you see what you are ignoring. This makes debugging faster. It also keeps the parser honest about what it does and does not handle.
+
+Verify:
+- Add a comment line to the file: `# comment`
+- Run `zig build run`.
+- Expected: a `skipped line` message.
+- This confirms the validation path works.
+
+If it fails:
+- Symptom: no skipped line prints.
+- Fix: ensure the line starts with `#` and is not blank.
 
 ## Verify it worked
 
-- Print vertex and face counts.
-- Compare against the file.
-
-Example output:
-
-```text
-vertices: 8
-faces: 12
-```
-
-If counts are off, log the first failing line.
+- Counts match the file.
+- Skipped lines log correctly.
+- The program exits without errors.
 
 ## Common mistakes
 
-- **Symptom**: Crash on large files.
-  **Cause**: Buffer too small.
-  **Fix**: Increase the read buffer.
+- **Symptom:** Buffer overflow error.  
+  **Cause:** Line buffer too small.  
+  **Fix:** Increase `line_buf` size.
 
-- **Symptom**: Wrong counts.
-  **Cause**: Blank lines not skipped.
-  **Fix**: Skip empty lines.
+- **Symptom:** Counts are off by one.  
+  **Cause:** Blank line at end of file.  
+  **Fix:** Keep the `line.len == 0` guard.
 
-- **Symptom**: Float parse errors.
-  **Cause**: Extra spaces.
-  **Fix**: Trim and split on whitespace.
+- **Symptom:** Parser crashes on large files.  
+  **Cause:** The line buffer is too small.  
+  **Fix:** Increase the buffer or use `readUntilDelimiterOrEofAlloc`.
 
 ## Cheat sheet
 
-- Read line by line.
-- Parse tokens carefully.
-- Log errors early.
-- Verify counts.
+- Use a tiny file with known counts.
+- Count `v` and `f` lines first.
+- Print output as proof.
+- Add skipped line logs for validation.
 
 ## Next steps
 
-- Add unit tests for bad lines.
-- Add benchmarks for big files.
+- Parse vertex floats and face indices.
+- Add unit tests with malformed files.
+- Track line numbers for better errors.
 
-Related links:
+## Related links
 
-- https://ziglang.org/documentation/0.13.0/
+- https://ziglang.org/learn/
 - https://en.wikipedia.org/wiki/Wavefront_.obj_file
 
-Final CTA:
+## Final CTA
 
-If you want my test files, ask and I will share them.
+Make the counts match the file before you add more features.
diff --git a/content/posts/pokedex/index.mdx b/content/posts/pokedex/index.mdx
index a9fa7a6..b4385eb 100644
--- a/content/posts/pokedex/index.mdx
+++ b/content/posts/pokedex/index.mdx
@@ -1,114 +1,162 @@
 ---
-title: "Interactive Pokedex Demo: Full Build Walkthrough"
-date: "2025-06-18"
+title: "Interactive Pokedex: Static Build With Search"
+date: "2025-05-01"
 slug: "/interactive-pokedex"
-tags: ["next-js", "react", "web-development"]
-description: "A complete tutorial for a static Pokedex demo with search and detail pages."
-theme: "Front-End & Full-Stack"
+tags: ["nextjs", "api", "frontend"]
+description: "A minimal static Pokedex build with search and clear verification steps."
+theme: "Front-End"
 ---
 
-This is a static demo built to practice data loading and search.
+This is a static Pokedex demo with client side search.
 
-There is no backend. The data is loaded at build time.
+It uses build time data and a simple search filter.
 
-You will build the list, add search, and verify the result.
+You will fetch data, render cards, and verify search works.
 
-If you want a full static demo guide, this is for you.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
 
-## What this is
-
-High level, this is a static site that lists Pokemon and supports search.
-
-Low level, it uses a build-time fetch and client-side filtering.
-
-Key terms:
-
-- **Static site**: pages built ahead of time.
-- **Build-time fetch**: data loaded when you build, not at runtime.
-- **Client-side filter**: filtering in the browser.
-
-## What you need
-
-- Node.js 18 or later.
-- A Next.js project.
-- Access to the PokeAPI.
-
-Docs used:
+## The promise
 
-- https://nextjs.org/docs
-- https://pokeapi.co/
-
-## Start to finish
-
-1) Create a Next.js app.
-
-Why: You need a clean app skeleton.
+By the end, you will:
 
-```bash
-npx create-next-app@latest pokedex
-cd pokedex
-```
+- build a static list from a public API
+- add a simple search filter
+- verify the output with clear checks
 
-2) Fetch data at build time.
+> "If the list renders and search filters, the demo works."
 
-Why: The list is static.
-
-```js
-const res = await fetch("https://pokeapi.co/api/v2/pokemon?limit=151");
-const data = await res.json();
-```
+## What this is
 
-3) Render the list.
+**High level:** A static Pokedex is a list of Pokemon that is built ahead of time.
 
-Why: You need a visible grid of items.
+**Low level:** You fetch data during build, render a list, and filter it in the browser.
 
-4) Add client-side search.
+**Key terms:**
 
-Why: Search happens in the browser.
+- **Static build:** HTML generated at build time.
+- **Client side search:** Filtering done in the browser after load.
 
-5) Add a detail page.
+## What you need
 
-Why: Users need a single item view.
+- Node 18+
+- Next.js
+- PokéAPI
+
+## Start to Finish
+
+### Step 1: Create the Next.js app
+Goal:
+Scaffold a new Next.js project.
+
+Actions:
+- Command:
+  ```bash
+  npx create-next-app@latest pokedex-demo
+  cd pokedex-demo
+  ```
+
+Why:
+Next.js gives you a static build option. It also includes routing by default. This keeps the demo simple. You can deploy the static output to GitHub Pages later.
+
+Verify:
+- Run:
+  ```bash
+  npm run dev
+  ```
+- Expected: the starter page loads.
+- This confirms the project runs.
+
+If it fails:
+- Symptom: dev server fails to start.  
+  **Fix:** reinstall dependencies and retry.
+
+### Step 2: Fetch Pokemon at build time
+Goal:
+Load a list of Pokemon for static rendering.
+
+Actions:
+- File path: `app/page.tsx` or `pages/index.tsx`
+- Add a fetch call:
+  ```ts
+  const res = await fetch("https://pokeapi.co/api/v2/pokemon?limit=151");
+  const data = await res.json();
+  ```
+- Render the list of names.
+
+Why:
+Build time fetch makes the page static. It also avoids a server at runtime. This keeps the demo fast and simple. You can verify the list output easily.
+
+Verify:
+- Run:
+  ```bash
+  npm run dev
+  ```
+- Expected: a list of names appears.
+- This confirms the fetch and render work.
+
+If it fails:
+- Symptom: fetch error.  
+  **Fix:** check the API URL and network.
+
+### Step 3: Add search filtering
+Goal:
+Filter the list by input.
+
+Actions:
+- Add a state value for the query.
+- Filter the list before rendering:
+  ```ts
+  const filtered = items.filter((p) => p.name.includes(query.toLowerCase()));
+  ```
+
+Why:
+Search is the main interactive feature. Filtering in the browser is fast for small lists. This keeps the demo offline friendly. It also makes the UI more useful.
+
+Verify:
+- Type a name into the search input.
+- Expected: the list filters immediately.
+- This confirms search works.
+
+If it fails:
+- Symptom: no filtering.  
+  **Fix:** ensure the query value updates on input.
 
 ## Verify it worked
 
-- The list shows 151 items.
+- The list renders.
 - Search filters results.
-- A detail page renders.
-
-If the list is empty, check the fetch call and build logs.
+- The page loads without errors.
 
 ## Common mistakes
 
-- **Symptom**: API fetch fails.
-  **Cause**: Network errors or rate limits.
-  **Fix**: Retry or cache data.
+- **Symptom:** API fetch fails at build time.  
+  **Cause:** missing network access.  
+  **Fix:** run the build with internet access or cache results.
 
-- **Symptom**: Search is slow.
-  **Cause**: Filtering on every keystroke without debounce.
-  **Fix**: Add a small debounce.
+- **Symptom:** Search is case sensitive.  
+  **Cause:** no normalization.  
+  **Fix:** use `toLowerCase()` on both sides.
 
-- **Symptom**: Detail page 404.
-  **Cause**: Missing dynamic route.
-  **Fix**: Create the correct route file.
+- **Symptom:** Build fails on fetch.  
+  **Cause:** incorrect fetch usage in the app router.  
+  **Fix:** move fetch to a server component.
 
 ## Cheat sheet
 
-- Build-time fetch for the list.
-- Client-side filter for search.
-- Detail page for one item.
+- Fetch 151 Pokemon.
+- Render the list.
+- Filter by query.
 
 ## Next steps
 
+- Add detail pages.
 - Add type filters.
-- Add loading skeletons.
-- Document the data source in README.
 
-Related links:
+## Related links
 
 - https://pokeapi.co/
 - https://nextjs.org/docs
 
-Final CTA:
+## Final CTA
 
-If you want a quick code review of your Pokedex demo, ask and I will help.
+Keep the demo small and verify the core behavior first.
diff --git a/content/posts/portfolio-case-studies/index.mdx b/content/posts/portfolio-case-studies/index.mdx
index df9efc2..0ef300f 100644
--- a/content/posts/portfolio-case-studies/index.mdx
+++ b/content/posts/portfolio-case-studies/index.mdx
@@ -1,104 +1,155 @@
 ---
-title: "Portfolio Case Studies: Full Template and Process"
-date: "2025-05-30"
+title: "Portfolio Case Studies: A Simple, Verifiable Template"
+date: "2025-03-22"
 slug: "/portfolio-case-studies"
-tags: ["portfolio", "projects", "process"]
-description: "A complete case study template with steps, checks, and proof links."
-theme: "Process"
+tags: ["portfolio", "writing", "process"]
+description: "A short case study template you can verify with proof links."
+theme: "Portfolio"
 ---
 
-Case studies only work when they match the code.
+A case study is a proof document, not a sales page.
 
-This post is a full template and process for writing honest case studies.
+This post gives you a short template that stays honest and scannable.
 
-You will get the exact sections, checks, and mistakes to avoid.
+You will write one case study that links to real proof.
 
-If you write portfolio case studies, this is for you.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
+
+## The promise
+
+By the end, you will:
+
+- write a short case study with proof links
+- avoid inflated claims
+- keep it readable for recruiters
+
+> "If the proof link is missing, the claim goes away."
 
 ## What this is
 
-High level, this is a structure for explaining a project clearly.
+**High level:** A case study explains the problem, solution, and result with proof.
 
-Low level, it is a set of sections and proof links tied to the repo.
+**Low level:** You write a short MDX file with fixed sections and links.
 
-Key terms:
+**Key terms:**
 
-- **Case study**: a short report of a project and its results.
-- **Proof link**: a repo or demo URL that backs a claim.
-- **Limitations**: a list of what the project does not do.
+- **Proof link:** A repo or demo URL.
+- **Outcome:** What the project achieved, stated plainly.
 
 ## What you need
 
-- A repo and demo.
-- A short project summary.
-- A list of limits.
+- A project repo
+- A live demo URL
+- A place to write the case study
+
+## Start to Finish
+
+### Step 1: Create the case study file
+Goal:
+Create a single file with fixed sections.
 
-## Start to finish
+Actions:
+- File path: `content/case-studies/project-name.mdx`
+- Add:
+  ```text
+  # Project name
 
-1) Write the one sentence problem.
+  ## Problem
+  ## Solution
+  ## What I built
+  ## Outcomes and proof
+  ```
 
-Why: It frames the project fast.
+Why:
+Fixed sections keep the structure clear. It makes the writing faster. It also makes scanning easier. This prevents the case study from drifting into vague storytelling.
 
-2) Write the one sentence solution.
+Verify:
+- Open the file and confirm the headings exist.
+- This confirms the structure is set.
 
-Why: It explains what you built.
+If it fails:
+- Symptom: file missing.  
+  **Fix:** create the folder and file.
 
-3) Add proof links.
+### Step 2: Add proof links
+Goal:
+Attach real proof to outcomes.
 
-Why: You need evidence.
+Actions:
+- Under "Outcomes and proof" add:
+  ```text
+  - Repo: https://github.com/yourname/project
+  - Demo: https://yourname.github.io/project
+  ```
 
-4) Add limits.
+Why:
+Proof links are the heart of the case study. They make the claims verifiable. This avoids inflated statements. It also saves reviewers time.
 
-Why: Limits prevent over-claiming.
+Verify:
+- Click each link.
+- Expected: the repo and demo load.
+- This confirms the proof is real.
 
-### Example case study block
+If it fails:
+- Symptom: 404.  
+  **Fix:** update the URL or remove the link.
 
-```md
-Problem: one sentence
-Solution: one sentence
-Proof: repo and demo
-Limits: one sentence
-```
+### Step 3: Add one concrete outcome
+Goal:
+State one result that is visible in the demo or repo.
+
+Actions:
+- Example outcome:
+  ```text
+  - Search filters the list in real time.
+  ```
+
+Why:
+Outcomes should be observable, not vague. One concrete outcome is enough. This keeps the case study honest. It also makes it easier to verify.
+
+Verify:
+- Open the demo and check the outcome.
+- This confirms the statement is true.
+
+If it fails:
+- Symptom: the outcome is not visible.  
+  **Fix:** remove the outcome or update the demo.
 
 ## Verify it worked
 
-- Claims match the repo.
+- The case study has all four sections.
 - Proof links work.
-- Limits are visible.
-
-If a claim is not in the code, remove it.
+- Outcomes are visible in the demo.
 
 ## Common mistakes
 
-- **Symptom**: Long paragraphs with no proof.
-  **Cause**: No structure.
-  **Fix**: Use the template.
+- **Symptom:** Case study reads like a resume bullet.  
+  **Cause:** No problem or solution section.  
+  **Fix:** use the template headings.
 
-- **Symptom**: Tech stack mismatch.
-  **Cause**: Copying old text.
-  **Fix**: Update from the repo.
+- **Symptom:** Outcomes are vague.  
+  **Cause:** No proof links.  
+  **Fix:** add a demo link and describe what you can see.
 
-- **Symptom**: Missing limits.
-  **Cause**: Skipping the hard parts.
-  **Fix**: Add a limits section.
+- **Symptom:** Claims do not match the repo.  
+  **Cause:** outdated writing.  
+  **Fix:** update the case study after each change.
 
 ## Cheat sheet
 
-- One sentence problem.
-- One sentence solution.
-- Proof links.
-- Limits listed.
+- Use fixed sections.
+- Add proof links.
+- Keep outcomes visible.
 
 ## Next steps
 
-- Update one case study today.
-- Remove one unverified claim.
-- Add one proof link.
+- Add one screenshot per case study.
+- Keep a changelog for updates.
 
-Related links:
+## Related links
 
 - https://bradleymatera.dev/projects/
 
-Final CTA:
+## Final CTA
 
-If you want my case study template file, ask and I will share it.
+Write the shortest case study that still has proof. That is the goal.
diff --git a/content/posts/rebuilt-webgpu-triangle-demo/index.mdx b/content/posts/rebuilt-webgpu-triangle-demo/index.mdx
index 7914cbf..69d601d 100644
--- a/content/posts/rebuilt-webgpu-triangle-demo/index.mdx
+++ b/content/posts/rebuilt-webgpu-triangle-demo/index.mdx
@@ -1,104 +1,174 @@
 ---
-title: "Rebuilding a WebGPU Triangle Demo: Full Debug Guide"
+title: "Rebuilding a WebGPU Triangle Demo: A Clean Fix Pass"
 date: "2025-05-02"
 slug: "/rebuilt-webgpu-triangle-demo"
 tags: ["webgpu", "graphics", "learning"]
-description: "A full debug walkthrough for rebuilding a WebGPU triangle demo."
+description: "A focused WebGPU rebuild with exact changes and verification steps."
 theme: "Graphics"
 ---
 
-This post is a full rebuild guide for a small WebGPU demo.
+Small WebGPU demos break for boring reasons.
 
-It focuses on the fixes that made the demo stable.
+This post shows a clean fix pass and how to verify the rebuild.
 
-You will get the steps, verification, and common mistakes.
+You will isolate one issue at a time and prove the fix works.
 
-If you are debugging a WebGPU demo, this is for you.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
 
-## What this is
-
-High level, this is a rebuild process for a small GPU demo.
-
-Low level, it is a set of fixes with verification checks.
-
-Key terms:
-
-- **Render loop**: the function that redraws each frame.
-- **Pipeline**: the GPU setup for a draw call.
-- **Shader**: the GPU program that controls drawing.
-
-## What you need
+## The promise
 
-- A working WebGPU setup.
-- A simple triangle demo.
-- A browser with WebGPU support.
+By the end, you will:
 
-Docs used:
+- fix a broken triangle render
+- clean up the render loop
+- verify the output is stable
 
-- https://developer.chrome.com/docs/web-platform/webgpu/
-
-## Start to finish
+> "Fix one thing, then verify it, then move on."
 
-1) Start from the smallest triangle.
-
-Why: You need a baseline.
+## What this is
 
-2) Replace one part at a time.
+**High level:** This is a minimal rebuild pass for a broken WebGPU demo.
 
-Why: You need to isolate changes.
+**Low level:** You verify the adapter, fix the pipeline, and confirm the draw call runs.
 
-3) Log errors after each change.
+**Key terms:**
 
-Why: WebGPU errors are easy to miss.
+- **Render loop:** A function that draws each frame.
+- **Pipeline:** The config for shaders and output.
 
-### Example render loop
+## What you need
 
-```js
-function frame() {
-  // update
-  // draw
-  requestAnimationFrame(frame);
-}
-```
+- A WebGPU capable browser
+- A minimal WebGPU demo project
+
+## Start to Finish
+
+### Step 1: Confirm the adapter and device
+Goal:
+Ensure WebGPU is available before debugging anything else.
+
+Actions:
+- File path: `main.js`
+- Add:
+  ```js
+  const adapter = await navigator.gpu.requestAdapter();
+  if (!adapter) throw new Error("No GPU adapter");
+  const device = await adapter.requestDevice();
+  ```
+
+Why:
+If the adapter is missing, nothing else matters. This is the first hard check. It reduces false debugging. It also tells you whether the browser supports WebGPU.
+
+Verify:
+- Run the demo and check the console.
+- Expected: no "No GPU adapter" error.
+- This confirms WebGPU is available.
+
+If it fails:
+- Symptom: adapter is null.  
+  **Fix:** enable WebGPU in the browser flags.
+
+### Step 2: Rebuild the pipeline
+Goal:
+Create a minimal pipeline that can draw a triangle.
+
+Actions:
+- File path: `main.js`
+- Add:
+  ```js
+  const pipeline = device.createRenderPipeline({
+    layout: "auto",
+    vertex: { module, entryPoint: "vs_main" },
+    fragment: { module, entryPoint: "fs_main", targets: [{ format }] },
+    primitive: { topology: "triangle-list" }
+  });
+  ```
+
+Why:
+If the pipeline is wrong, nothing draws. A minimal pipeline reduces the number of moving parts. This also makes shader issues easier to spot. It is the fastest route back to a working demo.
+
+Verify:
+- Run the demo and look for a triangle.
+- Expected: a triangle appears.
+- This confirms the pipeline works.
+
+If it fails:
+- Symptom: blank canvas.  
+  **Fix:** verify shader entry point names.
+
+### Step 3: Clean the render loop
+Goal:
+Ensure each frame clears and draws correctly.
+
+Actions:
+- File path: `main.js`
+- Use this loop:
+  ```js
+  function frame() {
+    const encoder = device.createCommandEncoder();
+    const pass = encoder.beginRenderPass({
+      colorAttachments: [{
+        view: context.getCurrentTexture().createView(),
+        clearValue: { r: 0.05, g: 0.05, b: 0.08, a: 1 },
+        loadOp: "clear",
+        storeOp: "store"
+      }]
+    });
+    pass.setPipeline(pipeline);
+    pass.draw(3);
+    pass.end();
+    device.queue.submit([encoder.finish()]);
+    requestAnimationFrame(frame);
+  }
+  frame();
+  ```
+
+Why:
+A clean render loop removes flicker and blank frames. It also makes the draw call obvious. This is the simplest loop that still updates every frame. It is the most reliable baseline.
+
+Verify:
+- Expected: the triangle stays visible without flicker.
+- This confirms the loop works.
+
+If it fails:
+- Symptom: triangle flickers.  
+  **Fix:** ensure you use `loadOp: "clear"` and submit the command buffer.
 
 ## Verify it worked
 
-- The triangle renders.
-- No WebGPU errors in the console.
-- The demo stays stable for 30 seconds.
-
-If it fails, revert one change at a time.
+- A triangle appears.
+- The render loop runs without errors.
+- The output is stable.
 
 ## Common mistakes
 
-- **Symptom**: Nothing renders.
-  **Cause**: Pipeline or shader mismatch.
-  **Fix**: Check shader inputs and pipeline layout.
+- **Symptom:** Nothing renders.  
+  **Cause:** Shader entry points mismatch.  
+  **Fix:** align entry point names in code and shader.
 
-- **Symptom**: Flicker or crash.
-  **Cause**: Render loop errors.
-  **Fix**: Simplify the loop.
+- **Symptom:** Canvas is black.  
+  **Cause:** No clear color or draw call.  
+  **Fix:** add `clearValue` and `draw(3)`.
 
-- **Symptom**: Random errors.
-  **Cause**: Changes in too many files.
-  **Fix**: Change one file at a time.
+- **Symptom:** Device lost errors.  
+  **Cause:** driver issues.  
+  **Fix:** reload and try again.
 
 ## Cheat sheet
 
-- Start with a minimal triangle.
-- Change one piece at a time.
-- Verify after each change.
+- Check adapter and device.
+- Use a minimal pipeline.
+- Use a clean render loop.
 
 ## Next steps
 
-- Add a color toggle.
-- Add a resize handler.
+- Add resize handling.
+- Add a uniform for color.
 
-Related links:
+## Related links
 
-- https://bradleymatera.github.io/TriangleDemo/
-- https://github.com/BradleyMatera/TriangleDemo
+- https://developer.chrome.com/docs/web-platform/webgpu/
 
-Final CTA:
+## Final CTA
 
-If you want my debug notes, ask and I will share them.
+Fix one issue at a time and verify after every change.
diff --git a/content/posts/recent-projects-and-lessons/index.mdx b/content/posts/recent-projects-and-lessons/index.mdx
index 663d00b..e45ca52 100644
--- a/content/posts/recent-projects-and-lessons/index.mdx
+++ b/content/posts/recent-projects-and-lessons/index.mdx
@@ -1,110 +1,150 @@
 ---
-title: "Recent Projects and Lessons: A Full Reality Log"
-date: "2025-08-28"
+title: "Recent Projects and Lessons: A Short, Verifiable Format"
+date: "2025-06-01"
 slug: "/recent-projects-and-lessons"
-tags: ["projects", "learning", "web-development"]
-description: "A complete log format for projects, lessons, and proof links."
+tags: ["projects", "learning", "process"]
+description: "A compact format for sharing recent demos with proof links."
 theme: "Process"
 ---
 
-Shipping without a lesson log makes you repeat mistakes.
+Recent projects only matter if the proof links work.
 
-This post shows the full format I use to capture lessons from demos.
+This post shows a short format for listing demos and lessons.
 
-You will get a template, steps, and verification checks.
+You will add a project list, proof links, and one lesson per demo.
 
-If you ship small projects, this is for you.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
+
+## The promise
+
+By the end, you will:
+
+- list projects with working links
+- write one lesson per project
+- keep the format consistent
+
+> "One proof link beats a page of claims."
 
 ## What this is
 
-High level, this is a project log with lessons and proof.
+**High level:** This is a simple pattern for sharing work without overselling it.
 
-Low level, it is a short template and a weekly routine.
+**Low level:** You write a list of projects and add a one line lesson for each.
 
-Key terms:
+**Key terms:**
 
-- **Lesson log**: a short record of what changed and what you learned.
-- **Proof link**: a repo or demo URL that backs the project.
-- **Reality snapshot**: a short list of limits and gaps.
+- **Proof link:** A repo or demo URL.
+- **Lesson:** One specific takeaway.
 
 ## What you need
 
-- A list of your demos.
-- One proof link per demo.
-- A place to store short notes.
+- A list of projects
+- Working repo and demo links
 
-## Start to finish
+## Start to Finish
 
-1) Pick the last demo you shipped.
+### Step 1: Create the list file
+Goal:
+Write a consistent list of recent projects.
 
-Why: Start with the freshest memory.
+Actions:
+- File path: `docs/recent-projects.md`
+- Add:
+  ```text
+  Recent projects
+  - AnimalSounds: https://bradleymatera.github.io/AnimalSounds/
+  - WebGPU Triangle Demo: https://bradleymatera.github.io/TriangleDemo/
+  ```
 
-2) Write a three line lesson.
+Why:
+A short list makes the projects easy to scan. It keeps the post honest. It also makes updates fast. Proof links are required for trust.
 
-Why: Short notes are easier to maintain.
+Verify:
+- Click each link.
+- Expected: each demo loads.
+- This confirms the list is valid.
 
-3) Add a reality snapshot.
+If it fails:
+- Symptom: 404.  
+  **Fix:** update the link or remove the project.
 
-Why: Limits must be visible.
+### Step 2: Add one lesson per project
+Goal:
+Write one clear lesson for each demo.
 
-4) Add proof links.
+Actions:
+- Add under each project:
+  ```text
+  Lesson: Base path matters for GitHub Pages.
+  ```
 
-Why: A lesson log without links is weak.
+Why:
+A lesson turns the list into useful content. It also keeps the post from being just a link dump. One sentence is enough. It is easier to keep honest and updated.
 
-### Example lesson format
+Verify:
+- Each project has exactly one lesson.
+- This confirms consistency.
 
-```text
-Project:
-What shipped:
-What I learned:
-```
+If it fails:
+- Symptom: lessons are vague.  
+  **Fix:** tie them to a specific fix or check.
 
-### Example reality snapshot
+### Step 3: Add an update date
+Goal:
+Show when the list was last refreshed.
 
-```text
-What works:
-What is missing:
-What is planned:
-```
+Actions:
+- Add to the top of the file:
+  ```text
+  Updated: 2025-06-01
+  ```
 
-## Verify it worked
+Why:
+Dates show recency. It prevents stale lists from looking current. This keeps the page honest. It also helps you remember to update it.
+
+Verify:
+- The date appears at the top.
+- This confirms the update time is visible.
 
-- The lesson log is one screen long.
-- Every project has a working link.
-- Limits are listed.
+If it fails:
+- Symptom: no date.  
+  **Fix:** add one line and move on.
+
+## Verify it worked
 
-If a link is broken, fix it before sharing.
+- The list has proof links.
+- Each project has one lesson.
+- The update date is present.
 
 ## Common mistakes
 
-- **Symptom**: Notes get long.
-  **Cause**: Writing essays.
-  **Fix**: Keep to three lines.
+- **Symptom:** Projects listed without proof.  
+  **Cause:** missing links.  
+  **Fix:** add links or remove the item.
 
-- **Symptom**: No proof links.
-  **Cause**: Skipping link checks.
-  **Fix**: Add the link as part of the log.
+- **Symptom:** Lessons are too long.  
+  **Cause:** trying to explain the whole project.  
+  **Fix:** keep it to one sentence.
 
-- **Symptom**: Limits missing.
-  **Cause**: Avoiding the hard parts.
-  **Fix**: Write the limits first.
+- **Symptom:** List goes stale.  
+  **Cause:** no update date.  
+  **Fix:** add one and refresh monthly.
 
 ## Cheat sheet
 
-- Three line lesson.
-- Reality snapshot.
-- Proof links.
-- Weekly review.
+- List the project.
+- Link the proof.
+- Add one lesson.
 
 ## Next steps
 
-- Add a lesson log to one README.
-- Update it after your next demo.
+- Add screenshots for the top two projects.
+- Move older projects to an archive section.
 
-Related links:
+## Related links
 
 - https://bradleymatera.dev/projects/
 
-Final CTA:
+## Final CTA
 
-If you want my lesson template, ask and I will share it.
+Keep the list short, current, and verifiable.
diff --git a/content/posts/secure-authentication-cognito-react/index.mdx b/content/posts/secure-authentication-cognito-react/index.mdx
index 49bc134..db29b15 100644
--- a/content/posts/secure-authentication-cognito-react/index.mdx
+++ b/content/posts/secure-authentication-cognito-react/index.mdx
@@ -1,112 +1,273 @@
 ---
-title: "Cognito + React Auth Lab: Full Setup and Verification"
-date: "2025-08-16"
+title: "Cognito Authentication With React: A Small, Verifiable Setup"
+date: "2025-05-14"
 slug: "/secure-authentication-cognito-react"
-tags: ["aws", "authentication", "react", "security"]
-description: "A complete lab guide for wiring Cognito into a React demo with clear checks."
-theme: "Security"
+tags: ["aws", "authentication", "react"]
+description: "A step by step Cognito setup with exact files, commands, and verification checks."
+theme: "Cloud"
 ---
 
-This is a lab guide for a small React auth demo.
+Authentication is not magic. It is config, tokens, and clear checks.
 
-It does not claim production ownership or enterprise setup.
+This post shows a minimal Cognito plus React setup you can verify.
 
-You will set up a user pool, wire it to React, and verify the flow.
+You will create a user pool, wire a frontend, and confirm login works.
 
-If you are new to Cognito, this is for you.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
 
-## What this is
-
-High level, Cognito manages user sign up and sign in.
-
-Low level, you create a user pool and connect it in your app config.
+## The promise
 
-Key terms:
+By the end, you will:
 
-- **User pool**: Cognito’s user directory.
-- **App client**: a client id that your app uses to connect.
-- **Hosted UI**: Cognito’s optional login UI.
-
-## What you need
-
-- An AWS account.
-- A React app.
-- Basic AWS console access.
-
-Docs used:
-
-- https://docs.aws.amazon.com/cognito/
+- create a Cognito user pool with email sign in
+- wire a React app to Cognito with Amplify
+- verify login and token storage in the browser
 
-## Start to finish
+> "If you can log in and read the ID token, your setup is real."
 
-1) Create a user pool.
-
-Why: The user pool is the core auth store.
-
-2) Create an app client.
-
-Why: Your app needs a client id to connect.
-
-3) Copy the pool and client IDs into your app config.
-
-Why: Your app needs to know where to authenticate.
+## What this is
 
-Example config:
+**High level:** Cognito is AWS’s hosted identity service. It stores users and issues tokens when they sign in.
 
-```js
-export const authConfig = {
-  region: "us-east-1",
-  userPoolId: "us-east-1_XXXX",
-  clientId: "abcd1234",
-};
-```
+**Low level:** You create a user pool, configure an app client, then point your React app at the pool using config values.
 
-4) Add a basic sign in screen.
+**Key terms:**
 
-Why: You need a UI to test the flow.
+- **User pool:** The Cognito database of users.
+- **App client:** The OAuth client for your frontend.
+- **ID token:** A JWT that represents the signed in user.
+- **Amplify Auth:** A library that handles the hosted auth flow.
 
-5) Test sign up and sign in.
+## What you need
 
-Why: This verifies the pool works.
+- AWS account
+- Node 18 or later
+- A React app
+
+Tools:
+
+- AWS Console
+- npm
+
+## Start to Finish
+
+### Step 1: Create the user pool
+Goal:
+Create a user pool with email as the sign in method.
+
+Actions:
+- AWS Console path: Cognito → User pools → Create user pool
+- Set:
+  - Sign in options: Email
+  - MFA: Optional or Off
+  - Email verification: On
+- Save the pool.
+
+Why:
+The user pool is the identity store. Email sign in keeps the flow simple. MFA can be added later after the basics work. Verification ensures the email is real. This is the minimum to support a demo login.
+
+Verify:
+- In the pool, open Users.
+- Expected: empty list with an Add user button.
+- This confirms the pool exists.
+
+If it fails:
+- Symptom: You cannot create the pool.
+- Fix: Check AWS region permissions and try again.
+
+### Step 2: Create an app client
+Goal:
+Create an app client so the frontend can authenticate.
+
+Actions:
+- AWS Console path: User pools → App integration → App clients → Create app client
+- Set:
+  - Public client
+  - No client secret
+- Save and copy:
+  - User Pool ID
+  - App Client ID
+
+Why:
+The app client is the OAuth entry point. A public client matches a browser app. No secret prevents exposing sensitive credentials. The IDs are required for your frontend config. These values are the connection between AWS and your app.
+
+Verify:
+- You can see the App Client ID in the list.
+- Expected: a string like `2a1b3c4d5e6f7g8h9i0j`.
+- This confirms the client exists.
+
+If it fails:
+- Symptom: You cannot save the client.
+- Fix: Ensure you did not enable a client secret.
+
+### Step 3: Create the React app
+Goal:
+Create a minimal React app with Amplify installed.
+
+Actions:
+- Commands:
+  ```bash
+  npm create vite@latest cognito-demo -- --template react
+  cd cognito-demo
+  npm install aws-amplify
+  ```
+- File path: `src/main.jsx`
+- Keep the default render call.
+
+Why:
+Vite gives a clean React starter without extra config. Amplify provides the Auth helpers you need. This keeps the tutorial small and focused. You can swap tooling later once the flow works.
+
+Verify:
+- Run:
+  ```bash
+  npm run dev
+  ```
+- Expected: the Vite dev server starts and the default page loads.
+- This confirms the app runs locally.
+
+If it fails:
+- Symptom: `npm run dev` fails.
+- Fix: Ensure Node 18 is installed and dependencies are installed.
+
+### Step 4: Configure Amplify
+Goal:
+Point the app at your Cognito pool.
+
+Actions:
+- File path: `src/aws-exports.js`
+- Add:
+  ```js
+  export const awsConfig = {
+    Auth: {
+      Cognito: {
+        userPoolId: "YOUR_USER_POOL_ID",
+        userPoolClientId: "YOUR_APP_CLIENT_ID"
+      }
+    }
+  };
+  ```
+- File path: `src/main.jsx`
+- Import and configure:
+  ```js
+  import { Amplify } from "aws-amplify";
+  import { awsConfig } from "./aws-exports";
+
+  Amplify.configure(awsConfig);
+  ```
+
+Why:
+Amplify needs explicit IDs to contact Cognito. Putting config in one file keeps it simple and reusable. The config must be loaded before any auth calls. This step is the actual connection between AWS and your app.
+
+Verify:
+- Run:
+  ```bash
+  npm run dev
+  ```
+- Expected: app loads with no console errors about Auth or Amplify.
+- This confirms config is parsed correctly.
+
+If it fails:
+- Symptom: `Auth` or `Amplify` errors in console.
+- Fix: Check the import paths and the config keys.
+
+### Step 5: Add a basic sign in form
+Goal:
+Create a minimal sign in form that calls Cognito.
+
+Actions:
+- File path: `src/App.jsx`
+- Replace with:
+  ```jsx
+  import { useState } from "react";
+  import { signIn, signOut, getCurrentUser } from "aws-amplify/auth";
+
+  export default function App() {
+    const [email, setEmail] = useState("");
+    const [password, setPassword] = useState("");
+    const [status, setStatus] = useState("signed out");
+
+    async function handleSignIn(e) {
+      e.preventDefault();
+      await signIn({ username: email, password });
+      const user = await getCurrentUser();
+      setStatus(`signed in as ${user.username}`);
+    }
+
+    async function handleSignOut() {
+      await signOut();
+      setStatus("signed out");
+    }
+
+    return (
+      <div style={{ padding: 24 }}>
+        <h1>Cognito login</h1>
+        <p>{status}</p>
+        <form onSubmit={handleSignIn}>
+          <input value={email} onChange={(e) => setEmail(e.target.value)} placeholder="email" />
+          <input value={password} onChange={(e) => setPassword(e.target.value)} placeholder="password" type="password" />
+          <button type="submit">Sign in</button>
+        </form>
+        <button onClick={handleSignOut}>Sign out</button>
+      </div>
+    );
+  }
+  ```
+
+Why:
+You need a real auth call to confirm the pool works. A minimal form keeps the surface area small. Using `getCurrentUser` confirms the session actually exists. This is enough for a demo without adding hosted UI.
+
+Verify:
+- Create a test user in Cognito.
+- Run:
+  ```bash
+  npm run dev
+  ```
+- Sign in and expect: `signed in as <email>`.
+- This confirms token issuance and user session.
+
+If it fails:
+- Symptom: `UserNotFoundException`.
+- Fix: Create the user in Cognito or enable self sign up.
 
 ## Verify it worked
 
-- You can create a user.
-- You can sign in.
-- Tokens appear in your app logs.
-
-If sign in fails, check the app client and region.
+- You can sign in and see the status line update.
+- Refreshing the page still shows signed in after a new login.
+- Signing out resets the status.
 
 ## Common mistakes
 
-- **Symptom**: Invalid client error.
-  **Cause**: Wrong client id.
-  **Fix**: Copy the correct id from AWS.
+- **Symptom:** `UserPoolId` errors.  
+  **Cause:** Wrong pool ID in config.  
+  **Fix:** Copy the ID directly from Cognito.
 
-- **Symptom**: Region mismatch.
-  **Cause**: Using the wrong region.
-  **Fix**: Set the correct region in config.
+- **Symptom:** Sign in works once, then fails.  
+  **Cause:** User is not confirmed.  
+  **Fix:** Confirm the user in Cognito or enable auto confirmation.
 
-- **Symptom**: Callback errors.
-  **Cause**: Missing callback URL.
-  **Fix**: Add the correct URL in Cognito.
+- **Symptom:** CORS error.  
+  **Cause:** Wrong region or endpoint.  
+  **Fix:** Ensure you created the pool in the same region you are using.
 
 ## Cheat sheet
 
-- Create user pool.
-- Create app client.
-- Configure app.
-- Test sign up and sign in.
+- Create a user pool with email sign in.
+- Create a public app client with no secret.
+- Install Amplify and configure it early.
+- Build a minimal sign in form.
+- Verify with a real Cognito user.
 
 ## Next steps
 
-- Add MFA in a lab.
-- Add error states in UI.
+- Add sign up and password reset flows.
+- Add route protection for authenticated pages.
+- Store tokens only in memory for demos.
 
-Related links:
+## Related links
 
 - https://docs.aws.amazon.com/cognito/
+- https://docs.amplify.aws/javascript/build-a-backend/auth/
 
-Final CTA:
+## Final CTA
 
-If you want my config checklist, ask and I will share it.
+Build the smallest auth flow possible, then expand only after login works.
diff --git a/content/posts/sell-your-skills-small-project/index.mdx b/content/posts/sell-your-skills-small-project/index.mdx
index 855372c..f88c3f4 100644
--- a/content/posts/sell-your-skills-small-project/index.mdx
+++ b/content/posts/sell-your-skills-small-project/index.mdx
@@ -1,102 +1,150 @@
 ---
-title: "Sell Your Skills With a Small Project: Full Guide"
-date: "2025-08-03"
-slug: "/sell-your-skills-small-project"
+title: "Sell Your Skills With One Small Project"
+date: "2025-05-25"
+slug: "/sell-your-skills-with-a-small-project"
 tags: ["portfolio", "career", "projects"]
-description: "A complete guide for building a small demo that proves one skill."
-theme: "Career"
+description: "A concrete way to prove a skill with one small demo and proof links."
+theme: "Portfolio"
 ---
 
-Small projects are easier to finish and easier to verify.
+Small projects are the fastest way to show a skill.
 
-This post shows a full process for shipping a tiny demo that proves one skill.
+This post shows how to build one demo and document it clearly.
 
-You will get steps, checks, and common mistakes.
+You will ship one feature, add proof links, and write a short case note.
 
-If you need portfolio proof fast, this is for you.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
+
+## The promise
+
+By the end, you will:
+
+- pick one skill to prove
+- build one small demo
+- add proof links and a short explanation
+
+> "One clear demo beats a vague resume line."
 
 ## What this is
 
-High level, this is a method to prove a single skill with a small demo.
+**High level:** A skill proof is a small project that shows one capability.
 
-Low level, it is a scope checklist, a build plan, and proof links.
+**Low level:** You build one feature, deploy it, and link the repo.
 
-Key terms:
+**Key terms:**
 
-- **Proof demo**: a small project that shows one skill.
-- **Scope**: the smallest version of the idea.
-- **CTA**: a clear action such as “View demo.”
+- **Skill proof:** A demo that shows one skill.
+- **Proof link:** A repo or live URL.
 
 ## What you need
 
-- One skill to prove.
-- A small project idea.
-- A repo and demo host.
+- A repo
+- A deploy target
+- A short README
 
-## Start to finish
+## Start to Finish
 
-1) Write the skill at the top of the README.
+### Step 1: Choose the skill
+Goal:
+Pick a single skill you can demonstrate quickly.
 
-Why: It keeps the demo focused.
+Actions:
+- Write the skill in `README.md`:
+  ```text
+  Skill to prove: client side search
+  ```
 
-2) Define the smallest demo.
+Why:
+One skill keeps the project focused. It also makes the output easy to judge. This prevents scope creep. You can do another skill later.
 
-Why: Small scope ships faster.
+Verify:
+- The skill is one sentence.
+- This confirms the scope is small.
 
-3) Build one working flow.
+If it fails:
+- Symptom: you list five skills.  
+  **Fix:** keep only one.
 
-Why: A flow is stronger than a list of features.
+### Step 2: Build the smallest demo
+Goal:
+Implement the feature in the simplest way.
 
-4) Add proof links.
+Actions:
+- Build a list and filter it by input.
+- Example code:
+  ```js
+  const filtered = items.filter((i) => i.name.includes(query));
+  ```
 
-Why: You need evidence for claims.
+Why:
+The demo should be easy to review. A small example makes the skill obvious. It also reduces bugs. This keeps the project finishable.
 
-### Example summary block
+Verify:
+- Search input filters the list.
+- This confirms the feature works.
 
-```md
-Skill: API fetch
-Demo: one page list
-Proof: repo + live link
-```
+If it fails:
+- Symptom: list does not update.  
+  **Fix:** ensure the query state updates on input.
 
-## Verify it worked
+### Step 3: Add proof links
+Goal:
+Make the demo verifiable.
 
-- The demo runs locally.
-- The live link loads.
-- The README matches the code.
+Actions:
+- Add to README:
+  ```text
+  Repo: https://github.com/yourname/project
+  Demo: https://yourname.github.io/project
+  ```
+
+Why:
+Proof links make the skill real. They also save time for reviewers. This is the most important part of the project. Without proof links, the skill is just a claim.
+
+Verify:
+- Click each link.
+- Expected: repo and demo load.
+- This confirms the proof works.
+
+If it fails:
+- Symptom: 404.  
+  **Fix:** update the URLs.
+
+## Verify it worked
 
-If you cannot verify it, cut the claim.
+- The demo shows the skill.
+- The proof links open.
+- The README states the goal clearly.
 
 ## Common mistakes
 
-- **Symptom**: Demo never ships.
-  **Cause**: Scope too large.
-  **Fix**: Cut the feature list in half.
+- **Symptom:** The project proves too many things.  
+  **Cause:** scope creep.  
+  **Fix:** cut it down to one feature.
 
-- **Symptom**: Proof link missing.
-  **Cause**: Skipping the publish step.
-  **Fix**: Add the live link before sharing.
+- **Symptom:** Proof links are missing.  
+  **Cause:** no deploy step.  
+  **Fix:** deploy and add links.
 
-- **Symptom**: Claims feel vague.
-  **Cause**: No concrete output.
-  **Fix**: Describe the exact flow.
+- **Symptom:** The skill is unclear.  
+  **Cause:** no goal statement.  
+  **Fix:** add one sentence to README.
 
 ## Cheat sheet
 
 - One skill.
-- One flow.
-- One proof link.
-- One short README.
+- One demo.
+- Two proof links.
 
 ## Next steps
 
-- Pick one skill and build this week.
-- Add proof links after publishing.
+- Add a short case study paragraph.
+- Add one screenshot of the demo.
 
-Related links:
+## Related links
 
 - https://bradleymatera.dev/projects/
 
-Final CTA:
+## Final CTA
 
-If you want feedback on a small demo, ask and I will help.
+Prove one skill clearly, then move to the next.
diff --git a/content/posts/service-worker-held-site-hostage/index.mdx b/content/posts/service-worker-held-site-hostage/index.mdx
index 8b358c7..dcf2e5b 100644
--- a/content/posts/service-worker-held-site-hostage/index.mdx
+++ b/content/posts/service-worker-held-site-hostage/index.mdx
@@ -1,110 +1,188 @@
 ---
-title: "Service Worker Bug: Full Fix From Cache to Clean"
-date: "2025-04-12"
+title: "Service Worker Held My Site Hostage: How I Removed It"
+date: "2025-03-11"
 slug: "/service-worker-held-site-hostage"
-tags: ["debugging", "web-development", "caching"]
-description: "A full guide to finding and clearing a stale service worker."
-theme: "Front-End & Full-Stack"
+tags: ["debugging", "gatsby", "web"]
+description: "Exact steps to find a stuck service worker, remove it, and verify the site updates."
+theme: "Debugging"
 ---
 
-A stale service worker can make a correct deploy look broken.
+A stale service worker can make your site lie to you.
 
-This post shows the full fix from detection to cleanup.
+This post shows the exact steps I used to remove a stuck service worker and confirm updates shipped.
 
-You will get steps, verification, and common mistakes.
+Everything here is verifiable in a browser and in this repo.
 
-If your site shows old assets, this is for you.
+## The promise
+
+By the end, you will:
+
+- find a service worker that is caching old content
+- remove it cleanly
+- verify the site updates without stale files
+
+> "If your browser still serves yesterday’s build, the fix is in the Application tab."
 
 ## What this is
 
-High level, a service worker can cache files to speed up repeat visits.
+**High level:** A service worker is a background script that can cache files and serve them even after deploys.
 
-Low level, it is a script that runs in the background and can override new assets.
+**Low level:** The browser registers a worker, stores cached assets, and serves those cached assets until you unregister it.
 
-Key terms:
+**Key terms:**
 
-- **Service worker**: a background script that can cache responses.
-- **Cache**: stored files the browser reuses.
-- **Unregister**: removing the service worker from the browser.
+- **Service worker:** A browser script that can intercept network requests.
+- **Cache storage:** The browser’s saved asset store used by the worker.
+- **Unregister:** The action that removes the worker.
 
 ## What you need
 
-- The deployed site.
-- Browser DevTools.
-- Access to the repo or build.
+- Chrome or Edge
+- Access to DevTools
+- This repo checked out
 
-Docs used:
+## Start to Finish
 
-- https://developer.mozilla.org/docs/Web/API/Service_Worker_API
+### Step 1: Confirm the service worker is active
+Goal:
+Verify that a service worker is controlling the page.
 
-## Start to finish
+Actions:
+- Open the site in Chrome.
+- Open DevTools → Application → Service Workers.
+- Look for an active registration.
 
-1) Open DevTools and go to Application.
+Why:
+You need to prove a worker exists before changing anything. The Application tab shows exactly which worker is controlling the page. This avoids guessing. If no worker is active, your issue is not a worker cache.
 
-Why: Service worker controls are there.
+Verify:
+- Expected: a service worker entry with "activated" or "running".
+- This confirms the worker is active.
 
-2) Check for an active service worker.
+If it fails:
+- Symptom: no worker listed.
+- Fix: skip to other cache issues like hard reload or CDN cache.
 
-Why: You need to confirm it exists.
+### Step 2: Unregister the worker
+Goal:
+Remove the worker so it can no longer intercept requests.
 
-3) Unregister the worker.
+Actions:
+- In DevTools → Application → Service Workers, click "Unregister".
+- Check "Update on reload".
+- Hard reload the page.
 
-Why: It stops the stale cache.
+Why:
+Unregistering removes the worker script. The hard reload forces the browser to fetch live assets. "Update on reload" ensures you are not seeing cached files. This isolates the problem to the worker.
 
-```js
-if ("serviceWorker" in navigator) {
-  navigator.serviceWorker.getRegistrations().then((regs) => {
-    regs.forEach((reg) => reg.unregister());
-  });
-}
-```
+Verify:
+- Expected: no active worker listed after reload.
+- This confirms the worker is gone.
 
-4) Clear storage.
+If it fails:
+- Symptom: worker reappears after reload.
+- Fix: clear site data in Application → Storage and reload again.
 
-Why: Old caches can still linger.
+### Step 3: Clear cached data
+Goal:
+Remove any cached files the worker left behind.
 
-5) Reload the site in a private window.
+Actions:
+- DevTools → Application → Storage
+- Click "Clear site data"
+- Reload the page
 
-Why: It avoids cached state.
+Why:
+Even after unregistering, caches can stick around. Clearing site data removes cached files and storage keys. This guarantees the next load is from the network. It also removes old app state that could be misleading.
 
-## Verify it worked
+Verify:
+- Expected: the cache storage list is empty.
+- This confirms no stale assets remain.
+
+If it fails:
+- Symptom: cache entries remain.
+- Fix: close the tab, reopen, and clear again.
+
+### Step 4: Ensure the repo does not re-register a worker
+Goal:
+Confirm the project does not re-add a service worker on build.
+
+Actions:
+- File path: `gatsby-config.ts`
+- Check the plugins list.
+- Ensure `gatsby-plugin-offline` is not present.
+
+Why:
+`gatsby-plugin-offline` installs a service worker. If it is in the plugin list, the worker will come back. Removing the plugin prevents future re-registration. This is the root fix in a Gatsby site.
 
-- The new assets load.
-- The site reflects the latest deploy.
-- The service worker list is empty.
+Verify:
+- Expected: no `gatsby-plugin-offline` entry in `gatsby-config.ts`.
+- This confirms the worker will not be re-added.
+
+If it fails:
+- Symptom: plugin exists.
+- Fix: remove it and restart the dev server.
+
+### Step 5: Verify the live content updates
+Goal:
+Confirm that updates appear without stale content.
+
+Actions:
+- Edit a visible line in the page content.
+- Run:
+  ```bash
+  bun run dev
+  ```
+- Reload the page in the browser.
+
+Why:
+A visible change is the easiest check. The dev server rebuilds on save. If the new text appears, the cache is no longer hiding changes. This confirms the issue is resolved.
+
+Verify:
+- Expected: the edited text appears immediately after reload.
+- This confirms the worker is not blocking updates.
+
+If it fails:
+- Symptom: old text still shows.
+- Fix: hard reload and clear site data again.
+
+## Verify it worked
 
-If it still shows old assets, you have another cache layer.
+- No active service worker in Application tab.
+- Cache storage is empty.
+- Content updates show immediately.
 
 ## Common mistakes
 
-- **Symptom**: Assets still old.
-  **Cause**: Cache not cleared.
-  **Fix**: Clear storage and reload.
+- **Symptom:** Changes show in dev server but not in browser.  
+  **Cause:** Worker cache still active.  
+  **Fix:** Unregister and clear site data.
 
-- **Symptom**: Worker comes back.
-  **Cause**: Offline plugin still enabled.
-  **Fix**: Remove the plugin and rebuild.
+- **Symptom:** Worker keeps returning.  
+  **Cause:** Offline plugin still enabled.  
+  **Fix:** Remove the plugin and rebuild.
 
-- **Symptom**: Site breaks after unregister.
-  **Cause**: App depends on cached data.
-  **Fix**: Remove that dependency.
+- **Symptom:** Cached assets appear after deploy.  
+  **Cause:** The browser did not update.  
+  **Fix:** Hard reload with cache disabled in DevTools.
 
 ## Cheat sheet
 
-- Check Application tab.
-- Unregister worker.
-- Clear storage.
-- Reload in private window.
+- Application tab tells the truth.
+- Unregister, then clear site data.
+- Remove offline plugin if present.
+- Verify with a visible content change.
 
 ## Next steps
 
-- Remove unused offline plugins.
-- Add a deploy note about cache.
+- Add a short "cache" note to your README.
+- Avoid service workers unless you need offline support.
 
-Related links:
+## Related links
 
-- https://developer.mozilla.org/docs/Web/API/Service_Worker_API
+- https://developer.chrome.com/docs/devtools/application/service-workers/
+- https://www.gatsbyjs.com/docs/how-to/progressive-web-apps/offline/
 
-Final CTA:
+## Final CTA
 
-If you hit this bug, send me the steps you used and I will compare notes.
+When the page lies to you, check the worker first.
diff --git a/content/posts/technical-stack/index.mdx b/content/posts/technical-stack/index.mdx
index d01bf0a..90d7404 100644
--- a/content/posts/technical-stack/index.mdx
+++ b/content/posts/technical-stack/index.mdx
@@ -1,97 +1,159 @@
 ---
-title: "My Technical Stack: Complete Inventory With Proof"
-date: "2025-04-28"
+title: "Technical Stack 2025: A Verifiable Inventory"
+date: "2025-10-12"
 slug: "/technical-stack-2025"
-tags: ["tools", "process", "web-development"]
-description: "A full inventory of tools, comfort levels, and proof links."
-theme: "Front-End & Full-Stack"
+tags: ["stack", "process", "documentation"]
+description: "A simple way to document what you actually use and what you are still learning."
+theme: "Process"
 ---
 
-This is a strict inventory, not a wishlist.
+Your stack should be a list you can prove, not a wish list.
 
-I only list tools I have used in a demo or lab.
+This post shows how I document what I use, what I am learning, and what is next.
 
-You will get the structure, the checks, and a template to keep it honest.
+You will create a stack file and verify each item with a repo or demo.
 
-If you want to publish your stack, this is for you.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
 
-## What this is
-
-High level, this is a list of tools tied to evidence.
-
-Low level, it is a format that links each tool to a repo or demo.
-
-Key terms:
-
-- **Comfort level**: how confident you are using a tool.
-- **Proof link**: a repo or demo URL that backs the tool.
-- **Planned**: a tool you plan to try, not one you already used.
+## The promise
 
-## What you need
-
-- A list of tools you actually used.
-- One proof link per tool.
-- A short note for planned tools.
+By the end, you will:
 
-## Start to finish
+- write a stack inventory you can defend
+- link each tool to proof
+- separate comfort from learning
 
-1) List each tool you used.
+> "If there is no proof link, it is not on the stack list."
 
-Why: This is the base inventory.
+## What this is
 
-2) Add a comfort level.
+**High level:** A stack inventory is a short list of tools you actually use.
 
-Why: It shows honesty and gaps.
+**Low level:** You list tools and link to a repo or demo that proves usage.
 
-3) Attach a proof link.
+**Key terms:**
 
-Why: The proof link prevents over-claiming.
+- **Comfort:** Tools you can ship with today.
+- **Learning:** Tools you are still practicing.
+- **Proof link:** A repo or demo that shows you used the tool.
 
-### Example format
+## What you need
 
-```text
-Tool: React
-Status: comfortable
-Proof: https://bradleymatera.dev
-```
+- A notes file
+- Links to your repos and demos
+
+## Start to Finish
+
+### Step 1: Create a stack file
+Goal:
+Write down your current stack categories.
+
+Actions:
+- File path: `docs/stack.md`
+- Add:
+  ```text
+  Stack
+  Comfort:
+  - React
+  - TypeScript
+  Learning:
+  - WebGPU
+  - AWS
+  ```
+
+Why:
+A clear list prevents overclaiming. It also makes updates easy. Splitting comfort and learning keeps your story honest. This format scales as your skills change.
+
+Verify:
+- Open the file and confirm the categories exist.
+- This confirms the structure is ready.
+
+If it fails:
+- Symptom: list is vague.  
+  **Fix:** use specific tool names.
+
+### Step 2: Add proof links
+Goal:
+Link each tool to a real example.
+
+Actions:
+- File path: `docs/stack.md`
+- Add:
+  ```text
+  Proof:
+  - React: https://github.com/BradleyMatera/AnimalSounds
+  - WebGPU: https://github.com/BradleyMatera/TriangleDemo
+  ```
+
+Why:
+Proof links turn claims into facts. They also help readers verify quickly. This keeps the stack list grounded in work you can show. It is the simplest honesty check.
+
+Verify:
+- Click each link.
+- Expected: repo or demo opens.
+- This confirms the proof is real.
+
+If it fails:
+- Symptom: link 404.  
+  **Fix:** update the URL or remove the tool.
+
+### Step 3: Add a short update note
+Goal:
+Record one change to the stack.
+
+Actions:
+- File path: `docs/stack.md`
+- Add:
+  ```text
+  Update: Added WebGPU after shipping TriangleDemo.
+  ```
+
+Why:
+A short update note shows progression. It also prevents the list from becoming stale. This makes your stack more believable. One line is enough.
+
+Verify:
+- Confirm the update is a single sentence.
+- This confirms the note is concise.
+
+If it fails:
+- Symptom: the note reads like a blog post.  
+  **Fix:** cut it to one sentence.
 
 ## Verify it worked
 
-- Each tool has one proof link.
-- Planned tools are labeled clearly.
-- There are no vague claims.
-
-If you cannot prove a tool, remove it.
+- Stack list is split into comfort and learning.
+- Every tool has a proof link.
+- Update note exists.
 
 ## Common mistakes
 
-- **Symptom**: Stack looks impressive but vague.
-  **Cause**: No proof links.
-  **Fix**: Add the link or remove the tool.
+- **Symptom:** Stack reads like a buzzword list.  
+  **Cause:** No proof.  
+  **Fix:** add links or remove items.
 
-- **Symptom**: Claims feel inflated.
-  **Cause**: Mixing planned and real tools.
-  **Fix**: Separate planned items.
+- **Symptom:** No difference between comfort and learning.  
+  **Cause:** Everything is listed as equal.  
+  **Fix:** split into two lists.
 
-- **Symptom**: List grows too large.
-  **Cause**: No pruning.
-  **Fix**: Remove unused tools monthly.
+- **Symptom:** Stack never updates.  
+  **Cause:** No update note.  
+  **Fix:** add one update line per quarter.
 
 ## Cheat sheet
 
-- One tool, one proof link.
-- Comfort level for each tool.
-- Planned tools labeled.
+- Split comfort and learning.
+- Add proof links.
+- Add one update note.
 
 ## Next steps
 
-- Add proof links to your stack today.
-- Remove one tool you cannot prove.
+- Replace one learning item with comfort once you ship a demo.
+- Add a link to your stack file from your README.
 
-Related links:
+## Related links
 
 - https://bradleymatera.dev/projects/
 
-Final CTA:
+## Final CTA
 
-If you want my stack template, ask and I will share it.
+If you cannot prove a tool, leave it off the list.
diff --git a/content/posts/testing-matters/index.mdx b/content/posts/testing-matters/index.mdx
index 7dbfe89..51da5f6 100644
--- a/content/posts/testing-matters/index.mdx
+++ b/content/posts/testing-matters/index.mdx
@@ -3,128 +3,186 @@ title: "Testing That I Actually Run: A Small Pyramid"
 date: "2025-09-29"
 slug: "/why-testing-matters"
 tags: ["testing", "quality", "devops"]
-description: "A small, honest testing stack for demos with clear checks and proof links."
+description: "A small, honest testing stack with exact commands, checks, and gaps."
 theme: "Front-End & Full-Stack"
 ---
 
 I test to catch real bugs, not to impress anyone.
 
-This post shows a small testing pyramid I actually run on my demos.
+This post shows a tiny testing pyramid that I actually run on demos.
 
-You will get a clean definition of each layer, the exact commands, and how I verify results.
+You will set up a single test command, run it, and document the gaps.
 
-If you only want one takeaway, it is this: a tiny suite that runs is better than a big suite that does not.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
 
 ## The promise
 
-By the end, you will be able to:
+By the end, you will:
 
-- choose a test scope that matches a small project
-- run unit and component tests without extra setup
-- document what is not tested yet
+- run a single test command
+- add one focused unit test
+- document one testing gap you will fix later
 
 > "A small test suite that runs beats a big suite that does not."
 
 ## What this is
 
-**High level:** A testing pyramid is a simple way to balance test types. You run lots of fast unit tests, a few component tests, and a handful of manual checks.
+**High level:** A testing pyramid keeps most tests small and fast, then adds fewer higher-level checks.
 
-**Low level:** You wire a single test command, run it locally, and record gaps in a short checklist.
+**Low level:** You wire one test runner, add one unit test, then perform one manual smoke check.
 
 **Key terms:**
 
-- **Unit test:** A test that checks one function in isolation.
-- **Component test:** A test that renders a UI component and checks behavior.
-- **Smoke check:** A short manual pass that verifies key flows still work.
+- **Unit test:** A test for a single function.
+- **Component test:** A test for a UI component.
+- **Smoke check:** A quick manual run of core flows.
 
 ## What you need
 
-- A repo with a `test` script in `package.json`
-- A local Node runtime that matches the repo
-- One test runner already in the project (Vitest or Jest)
-
-If your repo has no tests at all, start with a single unit test and a single smoke check.
-
-## Start to finish
-
-1. **Find the test command.**
-   Open `package.json` and look for the `test` script. This is the only command you need.
-
-   ```json
-   {
-     "scripts": {
-       "test": "vitest run"
-     }
-   }
-   ```
-
-2. **Run tests once.**
-   This confirms the test runner works and gives you a baseline.
-
-   ```bash
-   npm test
-   ```
-
-3. **Add one unit test if none exist.**
-   Pick a small pure function and assert one input and one output.
-
-4. **Add one component test if the repo includes React.**
-   Render a component, then assert a visible label or button.
-
-5. **Do one smoke check.**
-   Load the demo, click through one core flow, and write down what you touched.
-
-6. **Write a gap note.**
-   Keep this honest. One or two bullets is enough.
-
-   ```text
-   Gaps: No integration tests. No API error state checks.
-   ```
+- Node 18+
+- A repo with a `package.json`
+- A test runner like Vitest or Jest
+
+## Start to Finish
+
+### Step 1: Add a test script
+Goal:
+Ensure the repo has a single command for tests.
+
+Actions:
+- File path: `package.json`
+- Add or confirm:
+  ```json
+  {
+    "scripts": {
+      "test": "vitest run"
+    }
+  }
+  ```
+- Command:
+  ```bash
+  npm install -D vitest
+  ```
+
+Why:
+A single command is how you avoid broken test instructions. It also makes CI possible later. Without a consistent script, tests become tribal knowledge. This step makes your test entry point explicit.
+
+Verify:
+- Run:
+  ```bash
+  npm test
+  ```
+- Expected: Vitest runs and exits without crashing.
+- This confirms the test runner works.
+
+If it fails:
+- Symptom: `vitest: command not found`.
+- Fix: install Vitest and re-run `npm test`.
+
+### Step 2: Add one unit test
+Goal:
+Add a small unit test that proves the runner works.
+
+Actions:
+- File path: `src/sum.ts`
+- Add:
+  ```ts
+  export function sum(a: number, b: number) {
+    return a + b;
+  }
+  ```
+- File path: `src/sum.test.ts`
+- Add:
+  ```ts
+  import { sum } from "./sum";
+  import { describe, it, expect } from "vitest";
+
+  describe("sum", () => {
+    it("adds two numbers", () => {
+      expect(sum(2, 3)).toBe(5);
+    });
+  });
+  ```
+
+Why:
+A tiny test proves the entire testing loop. It also shows the exact pattern you can repeat later. This avoids complex setup and keeps the first pass fast. One passing test is more valuable than ten unrun tests.
+
+Verify:
+- Run:
+  ```bash
+  npm test
+  ```
+- Expected: one passing test called "adds two numbers".
+- This confirms the test file is detected and executed.
+
+If it fails:
+- Symptom: test file not found.
+- Fix: ensure the file name ends with `.test.ts`.
+
+### Step 3: Record one smoke check
+Goal:
+Document the one manual flow you check every time.
+
+Actions:
+- File path: `docs/testing-notes.md`
+- Add:
+  ```text
+  Smoke check:
+  - Load the home page
+  - Click the primary CTA
+  - Confirm the page navigates
+  ```
+
+Why:
+Manual checks are still valid, but they need to be written down. This keeps the check repeatable. It also tells future you what was actually verified. A simple file is enough.
+
+Verify:
+- Open the app locally and follow the bullets.
+- Expected: each step works without errors.
+- This confirms your smoke check is real.
+
+If it fails:
+- Symptom: you forgot the steps.
+- Fix: update the list to reflect what you actually tested.
 
 ## Verify it worked
 
-Use these checks:
-
-- The test command exits with code 0.
-- The output lists at least one passing test.
-- The smoke check matches what you claimed.
-
-If you see a failing test, fix it or delete it. Do not leave broken tests around and call it done.
+- `npm test` runs without crashing.
+- At least one test passes.
+- The smoke check steps are documented.
 
 ## Common mistakes
 
-- **Symptom:** Tests never run on your machine.  
-  **Cause:** The `test` script is missing or wrong.  
-  **Fix:** Add a test script and rerun locally.
+- **Symptom:** Tests never run.  
+  **Cause:** No test script defined.  
+  **Fix:** Add the `test` script and run it once.
 
-- **Symptom:** You claim CI gates but there is no workflow.  
-  **Cause:** The workflow file does not exist.  
-  **Fix:** Remove the claim or add the workflow later as a planned step.
+- **Symptom:** Tests pass locally but fail for others.  
+  **Cause:** Missing dev dependency.  
+  **Fix:** Add the test runner to `devDependencies`.
 
 - **Symptom:** Smoke checks are vague.  
-  **Cause:** You did not record what you clicked.  
-  **Fix:** Write a two line note so you can repeat it next time.
+  **Cause:** No written steps.  
+  **Fix:** Write down exact clicks and expected outcomes.
 
 ## Cheat sheet
 
-- Keep unit tests small and fast.
-- Keep component tests to a few critical UI bits.
-- Run one smoke check per release.
+- One `npm test` command.
+- One unit test.
+- One smoke check note.
 - Document gaps honestly.
-- Do not claim automation you cannot point to.
 
 ## Next steps
 
-- Add one more test for the most fragile function.
-- Add a simple README section called "Testing" with the command and gaps.
+- Add a second unit test for the most fragile function.
+- Add a basic component test if you use React.
 - If you want CI, add it later and link the workflow file.
 
 ## Related links
 
 - https://vitest.dev/
 - https://testing-library.com/docs/
-- https://github.com/BradleyMatera/CheeseMath-Jest-Tests
 
 ## Final CTA
 
-Run your test command today and write one honest gap note. That is enough to level up.
+Run your tests once today and document one gap. That is enough to move forward.
diff --git a/content/posts/webgpu-getting-started/index.mdx b/content/posts/webgpu-getting-started/index.mdx
index 6b90018..16069a9 100644
--- a/content/posts/webgpu-getting-started/index.mdx
+++ b/content/posts/webgpu-getting-started/index.mdx
@@ -3,184 +3,242 @@ title: "WebGPU Triangle Demo: Start, Fix, and Keep It Honest"
 date: "2025-07-22"
 slug: "/webgpu-getting-started"
 tags: ["webgpu", "graphics", "learning"]
-description: "A complete, minimal WebGPU triangle guide with setup, fixes, and verification."
+description: "A complete WebGPU triangle guide with exact steps and verification."
 theme: "Graphics"
 ---
 
-WebGPU looks scary until you ship one triangle.
+WebGPU gets easier after the first triangle.
 
-This is a combined guide that covers the minimal start, the rebuild fixes, and the honesty audit.
+This is the combined guide for the minimal setup, rebuild fixes, and honesty checks.
 
-You will get a working setup, the exact steps, and a clear checklist to keep the write up honest.
+You will render a triangle, verify the output, and keep the write up strict.
 
-If you only build one WebGPU thing, build this.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
 
 ## The promise
 
 By the end, you will:
 
-- understand what WebGPU actually is
-- render a triangle with a minimal pipeline
-- debug common setup failures without guessing
+- render a WebGPU triangle
+- verify the render loop
+- keep the write up aligned with the code
 
 > "One triangle is enough to learn the pipeline."
 
 ## What this is
 
-**High level:** WebGPU is the browser API for modern GPU access. It lets you submit commands to the GPU for fast graphics work.
+**High level:** WebGPU is a browser API for GPU rendering.
 
-**Low level:** You request a GPU adapter, create a device, define a render pipeline, and issue a draw call into a canvas context.
+**Low level:** You request a device, configure a canvas, build a pipeline, and draw.
 
 **Key terms:**
 
-- **Adapter:** The GPU driver entry point that tells you what the system supports.
-- **Device:** The object you use to create buffers, shaders, and pipelines.
-- **Pipeline:** The configuration that connects your shaders to the GPU render stages.
+- **Adapter:** GPU capability entry point.
+- **Device:** The object that creates GPU resources.
+- **Pipeline:** The configuration that ties shaders to output.
 
 ## What you need
 
-- A browser that supports WebGPU (Chrome or Edge stable)
-- A simple static site or local dev server
-- A single HTML file and one JS file
-
-If WebGPU is disabled, you will need to turn it on in `chrome://flags`.
-
-## Start to finish
-
-1. **Create the HTML shell.**
-   The canvas is the only required element.
-
-   ```html
-   <canvas id="gfx" width="640" height="360"></canvas>
-   <script type="module" src="main.js"></script>
-   ```
-
-2. **Request the adapter and device.**
-   This is the minimum handshake.
-
-   ```js
-   const adapter = await navigator.gpu.requestAdapter();
-   if (!adapter) throw new Error("No GPU adapter");
-   const device = await adapter.requestDevice();
-   ```
-
-3. **Create the canvas context.**
-   You must configure it with the preferred format.
-
-   ```js
-   const canvas = document.getElementById("gfx");
-   const context = canvas.getContext("webgpu");
-   const format = navigator.gpu.getPreferredCanvasFormat();
-   context.configure({ device, format, alphaMode: "opaque" });
-   ```
-
-4. **Define a minimal shader.**
-   Use WGSL with a hard coded triangle.
-
-   ```wgsl
-   @vertex
-   fn vs_main(@builtin(vertex_index) vi: u32) -> @builtin(position) vec4f {
-     var positions = array<vec2f, 3>(
-       vec2f(0.0, 0.6),
-       vec2f(-0.6, -0.6),
-       vec2f(0.6, -0.6)
-     );
-     return vec4f(positions[vi], 0.0, 1.0);
-   }
-
-   @fragment
-   fn fs_main() -> @location(0) vec4f {
-     return vec4f(0.2, 0.6, 1.0, 1.0);
-   }
-   ```
-
-5. **Create the pipeline.**
-   This binds the shader to the render pass.
-
-   ```js
-   const module = device.createShaderModule({ code: shaderCode });
-   const pipeline = device.createRenderPipeline({
-     layout: "auto",
-     vertex: { module, entryPoint: "vs_main" },
-     fragment: { module, entryPoint: "fs_main", targets: [{ format }] },
-     primitive: { topology: "triangle-list" }
-   });
-   ```
-
-6. **Draw the frame.**
-   This is the minimal render loop.
-
-   ```js
-   function frame() {
-     const encoder = device.createCommandEncoder();
-     const pass = encoder.beginRenderPass({
-       colorAttachments: [{
-         view: context.getCurrentTexture().createView(),
-         clearValue: { r: 0.05, g: 0.05, b: 0.08, a: 1 },
-         loadOp: "clear",
-         storeOp: "store"
-       }]
-     });
-     pass.setPipeline(pipeline);
-     pass.draw(3);
-     pass.end();
-     device.queue.submit([encoder.finish()]);
-     requestAnimationFrame(frame);
-   }
-   frame();
-   ```
-
-7. **Write the honesty note.**
-   If you share this, say exactly what it does and what it does not.
-
-   ```text
-   What works: One triangle, one pipeline, one draw call.
-   What is next: Resize handling and input controls.
-   ```
+- Chrome or Edge with WebGPU enabled
+- A local static server
+
+## Start to Finish
+
+### Step 1: Create the HTML shell
+Goal:
+Set up a canvas and script entry point.
+
+Actions:
+- File path: `index.html`
+- Add:
+  ```html
+  <canvas id="gfx" width="640" height="360"></canvas>
+  <script type="module" src="main.js"></script>
+  ```
+
+Why:
+The canvas is the render target. The module script lets you use top level await. This keeps the demo minimal and modern. Without the canvas, nothing will render.
+
+Verify:
+- Open the HTML file in a browser.
+- Expected: a blank page with no errors.
+- This confirms the shell loads.
+
+If it fails:
+- Symptom: script not found.  
+  **Fix:** ensure `main.js` exists in the same folder.
+
+### Step 2: Request the adapter and device
+Goal:
+Confirm WebGPU is available and create a device.
+
+Actions:
+- File path: `main.js`
+- Add:
+  ```js
+  const adapter = await navigator.gpu.requestAdapter();
+  if (!adapter) throw new Error("No GPU adapter");
+  const device = await adapter.requestDevice();
+  ```
+
+Why:
+The adapter check is the first gate. The device is required to create pipelines and buffers. This step prevents silent failures. It also tells you whether the browser supports WebGPU.
+
+Verify:
+- Open DevTools Console.
+- Expected: no error thrown.
+- This confirms WebGPU is available.
+
+If it fails:
+- Symptom: `No GPU adapter`.  
+  **Fix:** enable WebGPU in browser flags.
+
+### Step 3: Configure the canvas context
+Goal:
+Bind the canvas to WebGPU and set the format.
+
+Actions:
+- File path: `main.js`
+- Add:
+  ```js
+  const canvas = document.getElementById("gfx");
+  const context = canvas.getContext("webgpu");
+  const format = navigator.gpu.getPreferredCanvasFormat();
+  context.configure({ device, format, alphaMode: "opaque" });
+  ```
+
+Why:
+The context is how WebGPU draws to the canvas. The format must match the pipeline output. This is the most common source of blank screens. Setting `opaque` avoids transparency bugs.
+
+Verify:
+- Expected: no errors in console.
+- This confirms the context is configured.
+
+If it fails:
+- Symptom: `context` is null.  
+  **Fix:** confirm the canvas ID and WebGPU support.
+
+### Step 4: Build a minimal pipeline
+Goal:
+Create a pipeline that draws one triangle.
+
+Actions:
+- File path: `main.js`
+- Add a shader string:
+  ```js
+  const shaderCode = `
+  @vertex
+  fn vs_main(@builtin(vertex_index) vi: u32) -> @builtin(position) vec4f {
+    var positions = array<vec2f, 3>(
+      vec2f(0.0, 0.6),
+      vec2f(-0.6, -0.6),
+      vec2f(0.6, -0.6)
+    );
+    return vec4f(positions[vi], 0.0, 1.0);
+  }
+  @fragment
+  fn fs_main() -> @location(0) vec4f {
+    return vec4f(0.2, 0.6, 1.0, 1.0);
+  }`;
+  ```
+- Create the pipeline:
+  ```js
+  const module = device.createShaderModule({ code: shaderCode });
+  const pipeline = device.createRenderPipeline({
+    layout: "auto",
+    vertex: { module, entryPoint: "vs_main" },
+    fragment: { module, entryPoint: "fs_main", targets: [{ format }] },
+    primitive: { topology: "triangle-list" }
+  });
+  ```
+
+Why:
+A minimal shader removes variables. The pipeline ties the shader to the render pass. This is the core of the demo. Without a valid pipeline, nothing will render.
+
+Verify:
+- Expected: no shader compile errors in console.
+- This confirms the pipeline is valid.
+
+If it fails:
+- Symptom: shader compile error.  
+  **Fix:** check entry point names and WGSL syntax.
+
+### Step 5: Draw the frame
+Goal:
+Issue one draw call and render the triangle.
+
+Actions:
+- File path: `main.js`
+- Add:
+  ```js
+  function frame() {
+    const encoder = device.createCommandEncoder();
+    const pass = encoder.beginRenderPass({
+      colorAttachments: [{
+        view: context.getCurrentTexture().createView(),
+        clearValue: { r: 0.05, g: 0.05, b: 0.08, a: 1 },
+        loadOp: "clear",
+        storeOp: "store"
+      }]
+    });
+    pass.setPipeline(pipeline);
+    pass.draw(3);
+    pass.end();
+    device.queue.submit([encoder.finish()]);
+    requestAnimationFrame(frame);
+  }
+  frame();
+  ```
+
+Why:
+This is the full draw loop. It clears the screen and draws three vertices. `requestAnimationFrame` keeps it running. This is the most reliable baseline for WebGPU demos.
+
+Verify:
+- Expected: a blue triangle appears on a dark background.
+- This confirms the render loop works.
+
+If it fails:
+- Symptom: blank canvas.  
+  **Fix:** ensure the pipeline and context use the same format.
 
 ## Verify it worked
 
-- The canvas shows a triangle on a dark background.
-- The browser console has no WebGPU errors.
-- The frame loop runs without throwing.
-
-If you do not see a triangle, check the console first. Most failures are misconfigured context or shader errors.
+- The triangle is visible.
+- No console errors appear.
+- The output is stable.
 
 ## Common mistakes
 
 - **Symptom:** `navigator.gpu` is undefined.  
-  **Cause:** WebGPU is not enabled or the browser is too old.  
-  **Fix:** Update the browser and enable WebGPU in flags.
+  **Cause:** WebGPU disabled.  
+  **Fix:** enable WebGPU in browser flags.
 
-- **Symptom:** Clear color shows but no triangle.  
-  **Cause:** Shader compile error or wrong entry point names.  
-  **Fix:** Ensure `vs_main` and `fs_main` match your pipeline config.
+- **Symptom:** Triangle flickers.  
+  **Cause:** missing `loadOp: "clear"`.  
+  **Fix:** add the clear operation.
 
-- **Symptom:** Canvas is blank and console shows device lost.  
-  **Cause:** GPU reset or driver issue.  
-  **Fix:** Reload the page or try a different GPU.
+- **Symptom:** Shader compile errors.  
+  **Cause:** WGSL syntax issues.  
+  **Fix:** check the shader code carefully.
 
 ## Cheat sheet
 
 - Request adapter and device.
-- Configure the canvas with the preferred format.
-- Build a pipeline with WGSL.
-- Draw 3 vertices.
-- Keep your write up strict and honest.
+- Configure the canvas context.
+- Build a minimal pipeline.
+- Draw three vertices.
 
 ## Next steps
 
-- Add a resize handler.
-- Add a uniform buffer for color.
-- Export this demo as a static page.
+- Add resize handling.
+- Add a uniform for color.
+- Keep the write up aligned with the code.
 
 ## Related links
 
 - https://developer.chrome.com/docs/web-platform/webgpu/
 - https://www.w3.org/TR/webgpu/
-- https://github.com/BradleyMatera/TriangleDemo
-- https://bradleymatera.github.io/TriangleDemo/
 
 ## Final CTA
 
-Ship one triangle, then build one small improvement. Do not skip the verify step.
+Get one triangle working, then build one small improvement at a time.
diff --git a/content/posts/what-i-learned-three-simple-projects/index.mdx b/content/posts/what-i-learned-three-simple-projects/index.mdx
index d982525..8d56a99 100644
--- a/content/posts/what-i-learned-three-simple-projects/index.mdx
+++ b/content/posts/what-i-learned-three-simple-projects/index.mdx
@@ -3,121 +3,150 @@ title: "What I Learned From Three Small Projects"
 date: "2025-10-04"
 slug: "/what-i-learned-three-simple-projects"
 tags: ["projects", "process", "web-development"]
-description: "Three small demos, the real lessons, and a repeatable way to document them."
+description: "Three small demos, the real lessons, and a repeatable format with proof."
 theme: "Process"
 ---
 
-Small projects teach faster than big plans.
+Small projects teach faster than long plans.
 
-This post shows three demos and the exact lessons they gave me.
+This post shows three demos and the exact lesson format I use.
 
-You will get the format I use, the steps I repeat, and links to each demo.
+You will build a short project list, add proof links, and write one lesson each.
 
-If you want a clean way to learn without fluff, this is it.
+I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.
 
 ## The promise
 
 By the end, you will:
 
-- pick three small project ideas that finish fast
-- build and ship each demo
-- write short lessons that are easy to verify
+- list three small demos with proof links
+- write one lesson per demo
+- keep the list current with an update date
 
-> "A small demo with proof beats a big demo with vague claims."
+> "One lesson with proof beats a page of claims."
 
 ## What this is
 
-**High level:** A small project is a focused demo with a single goal, a short build time, and a public link.
+**High level:** This is a small project review format that keeps learning visible.
 
-**Low level:** You ship a feature, verify it in the browser, and write a short lesson with proof links.
+**Low level:** You write a list of demos and add a one line lesson under each.
 
 **Key terms:**
 
-- **Demo:** A small public project with one clear capability.
-- **Proof link:** A link to a repo or live page that shows the claim is real.
-- **Lesson:** A short statement that says what worked and what to fix next.
+- **Proof link:** A URL that shows the demo works.
+- **Lesson:** A short statement of what you learned.
 
 ## What you need
 
-- A public GitHub repo for each demo
-- A live URL for each demo
-- A note template with three lines
+- Three demos
+- A notes file
+- Working URLs
 
-Here are the three demos I use as examples:
+## Start to Finish
 
-- AnimalSounds: https://bradleymatera.github.io/AnimalSounds/
-- CheeseMath: https://bradleymatera.github.io/CheeseMath-Jest-Tests/
-- EthicsFrontEndDemo: https://bradleymatera.github.io/EthicsFrontEndDemo/
+### Step 1: Create the list file
+Goal:
+Create a single file that lists the three projects.
 
-## Start to finish
+Actions:
+- File path: `docs/three-projects.md`
+- Add:
+  ```text
+  Three projects
+  - AnimalSounds: https://bradleymatera.github.io/AnimalSounds/
+  - CheeseMath: https://bradleymatera.github.io/CheeseMath-Jest-Tests/
+  - EthicsFrontEndDemo: https://bradleymatera.github.io/EthicsFrontEndDemo/
+  ```
 
-1. **Pick the smallest possible goal.**
-   Examples: play a sound, render a triangle, show a secure pattern.
+Why:
+A list keeps the content scannable. It also ensures the proof links are visible. This prevents vague summaries. It makes updates quick.
 
-2. **Define the one success check.**
-   If it does not pass that check, it is not done.
+Verify:
+- Click each link.
+- Expected: each demo loads.
+- This confirms the proof works.
 
-3. **Build the demo.**
-   Keep the scope tight. No extra features.
+If it fails:
+- Symptom: 404.  
+  **Fix:** update the link or remove the project.
 
-4. **Publish a live link.**
-   The demo is not real until it is accessible.
+### Step 2: Add one lesson per project
+Goal:
+Write one short lesson tied to each demo.
 
-5. **Write the lesson.**
-   Use the three line template below.
+Actions:
+- Under each project line, add:
+  ```text
+  Lesson: Base paths matter for GitHub Pages.
+  ```
 
-   ```text
-   Project:
-   One thing I learned:
-   One thing I would change next:
-   ```
+Why:
+A single lesson keeps the post short and honest. It also makes the demo more useful. One line is easier to verify than a paragraph. It keeps the format consistent.
 
-6. **Add proof links.**
-   Put the repo and live URL in the post.
+Verify:
+- Each project has exactly one lesson line.
+- This confirms consistency.
 
-## Verify it worked
+If it fails:
+- Symptom: lessons are long and vague.  
+  **Fix:** cut to one sentence with a concrete fix.
+
+### Step 3: Add an update date
+Goal:
+Show when the list was last refreshed.
+
+Actions:
+- Add to the top of the file:
+  ```text
+  Updated: 2025-10-04
+  ```
+
+Why:
+An update date signals recency. It keeps the list honest. It also makes it easy to see when the list is stale. One line is enough.
 
-- The live demo loads without errors.
-- The repo README explains how to run it locally.
-- The lesson matches what the demo actually does.
+Verify:
+- The date appears at the top.
+- This confirms the update is visible.
+
+If it fails:
+- Symptom: date missing.  
+  **Fix:** add it and move on.
+
+## Verify it worked
 
-If any of those fail, fix the demo or delete the claim.
+- Proof links load.
+- Each project has one lesson.
+- Update date is present.
 
 ## Common mistakes
 
-- **Symptom:** Projects never finish.  
-  **Cause:** The goal was too big.  
-  **Fix:** Cut the goal until it can ship in one weekend.
+- **Symptom:** Project list gets bloated.  
+  **Cause:** no limit.  
+  **Fix:** keep it to three projects.
 
-- **Symptom:** The demo is live but the README is empty.  
-  **Cause:** You skipped documentation.  
-  **Fix:** Add a short setup section and a proof link.
+- **Symptom:** Lessons are generic.  
+  **Cause:** no specific outcome.  
+  **Fix:** tie the lesson to a visible behavior.
 
-- **Symptom:** The lesson sounds like marketing.  
-  **Cause:** The lesson is not tied to a real check.  
-  **Fix:** Use the template and keep it factual.
+- **Symptom:** Links break.  
+  **Cause:** demos removed.  
+  **Fix:** update or remove the link.
 
 ## Cheat sheet
 
-- One goal per demo.
-- One success check.
-- One live link.
-- One lesson template.
-- Proof links everywhere.
+- Three projects.
+- Three proof links.
+- Three lessons.
 
 ## Next steps
 
-- Add a small "Lessons" section to each repo README.
-- Share one lesson with a peer and ask for feedback.
-- Build one more demo with a different tool.
+- Rotate older projects out every quarter.
+- Add a screenshot per demo.
 
 ## Related links
 
-- https://github.com/BradleyMatera/AnimalSounds
-- https://github.com/BradleyMatera/CheeseMath-Jest-Tests
-- https://github.com/BradleyMatera/EthicsFrontEndDemo
 - https://bradleymatera.dev/projects/
 
 ## Final CTA
 
-Pick one small idea and ship it in two days. Then write the lesson while it is fresh.
+Keep the list small, current, and verifiable.

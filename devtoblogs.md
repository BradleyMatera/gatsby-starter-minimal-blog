Job hunting is poopie
## Not a Success Story. Just the Truth About Trying.  
![overwhelmed](https://media.giphy.com/media/3og0IPxMM0erATueVW/giphy.gif)

I did grow up around computers but i never took any AP computer science classes nor did i attend any robotics club. I spent most of my adult life doing work that had nothing to do with terminals, VS Code, or GitHub. I was a medic in the 82nd Airborne. I walked roofs with nail guns. I worked security at night. I dealt with overdoses and addiction when I worked case management. I cleaned cat cages at a rescue shelter for years. None of that pointed toward writing software for a living. And for most of my life, I didn’t think tech was even an option for someone like me.

When I finally opened a terminal for the first time, it didn’t feel intimidating — but it also didn’t feel normal. Basic commands felt like learning a new language. Learning what a for loop was felt like being introduced to a logic system I had never used before. And learning React felt like someone pulled the rug halfway under me — just enough for me to stand but not enough to feel fully steady.

My degree at Full Sail moved so fast that memory didn’t really stick. Each class lasted one month — then it was gone, and the next topic took its place. HTML, CSS, JavaScript, SQL, UX, Python, AWS basics, testing, deployment — all in separate months. I didn’t have time to master any one subject, but weirdly, momentum started forming. I didn’t remember everything, but repetition slowly built foundation. Not perfect, not deep — but enough.

And something clicked anyway. Even when I didn’t remember the syntax — I still liked this work. I liked that you could build something real and click on it. I liked reading error logs and figuring out why something broke. I liked seeing things appear in the browser and knowing I made that happen. It felt like work where effort actually turned into skill. I never had that feeling before. Not in construction. Not in security. Not even in the army. This field felt different. It felt like I finally found work that didn’t just drain me — it built me.

---

## Understanding Where My Knowledge Really Sits  
![thinking](https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExZzRmdDJvZHEzbmFva2JpM2o1ODg1M2E1ZHJ0MXV5cnE3NHZ2eWNiMCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/c2xuRmH2e2Xbq/giphy.gif)

I graduated one month ago. I completed dozens of projects across school and landed a 12-week AWS internship at Amazon. Inside that short window, I passed two AWS certifications — Solutions Architect Associate and AI Practitioner — and operated inside real cloud environments without shortcuts. I had to study, read documentation, and follow real runbooks like everyone else. No one spoon-fed me anything. That experience helped me more than any class.

But graduating didn’t mean everything suddenly fell into place. What I really have right now is *practical skill, but not perfect foundations*. I can read JavaScript, React, HTML, CSS, and Python code and understand what it’s doing. I can follow data through a full stack app once it exists. I can deploy things to AWS or static hosting. I can debug real errors using logs, terminal output, CloudWatch dashboards, DevTools, and trial and error. I can get things working *eventually* — not because I know everything, but because I don’t stop until I find the right pattern.

What I can’t do yet is whiteboard complex algorithms from memory. I don’t claim to be able to solve medium LeetCode under pressure. But I can read code, understand it, adjust it, and ship working systems. I feel like I am exactly in the middle — past beginner but not “senior” anything. And honestly, that middle space is probably where most people actually live — even if social media makes it seem like everyone can build full systems straight from their head.

There is a huge difference between writing code from nothing and steering a working system forward. Right now, I am better at the second one. And at least that gives me direction.

---

## What I’ve Learned About the Tech Industry So Far  
![industry-challenge](https://media.giphy.com/media/3o6Zt8MgUuvSbkZYWc/giphy.gif)

### **1. Hard Work Does Not Guarantee Anything**  
![trying](https://media.giphy.com/media/26ufnwz3wDUli7GU0/giphy.gif)

In the past year, I have applied to more companies than I can count. Startups. Big tech. Small dev shops. Contract roles. “Entry-level” positions requiring five years of experience. Recruiter submissions. LinkedIn postings. Indeed. Company portals. Referrals. Networking messages. All of it. Most applications go silent. Some bounce back instantly with automated rejections. A few move forward, then disappear entirely. You keep trying. You keep submitting. But eventually you start to realize — **there is no guaranteed point where effort turns into opportunity.**

Some roles were clearly fake — posted but already filled so the company could meet some hiring policy. Some roles were meant for internal candidates but listed publicly for legal reasons. Some wanted someone’s nephew. Some wanted data structure mastery as a barrier even when the role didn’t require it. Some just wanted content writers, not engineers. You start realizing that “job hunting” in tech is really just trying to figure out which pathways are actually real and which are dead ends before you waste months chasing them.

That doesn’t mean giving up. It means being honest about what you’re actually facing. This field rewards persistence — but not always immediately *and not always the way you’d expect*. Sometimes skill gets ignored. Sometimes impressive projects don’t mean anything. Sometimes it has nothing to do with you at all.

It took me time to accept that working hard does not guarantee anything — but *not working hard guarantees nothing*.

---

### **2. Tech Is Merit-Based — and Also Not Merit-Based at All**  
![duality](https://media.giphy.com/media/3oEjI6SIIHBdRxXI40/giphy.gif)

There are **two systems** running at the same time, and both are real:

| System | What It Rewards |
|-------|------------------|
| Classic Merit System | People with very strong foundations — DSA, algorithms, system design, math — especially those who can explain logic cleanly without references. These are the top ~1–5% who can operate with abstract thinking. That group will *always* find work. |
| Real-World System | People with connections. People who know someone inside. People who get referred. People who are already trusted. Sometimes, people who just happen to be in the right place at the right time. |

Most new grads probably do **not** have the first category mastered — especially if their education moved fast like mine. You come out able to build things and deploy things — but maybe not solve everything on a whiteboard from scratch. That doesn’t make you bad — it just puts you in category two, where getting seen usually requires personal connections, projects people can click on, or someone vouching for your ability.

People online often pretend it’s all merit-based or all nepotism — but that’s just online posturing. The truth is quieter:  
**it’s both. And it depends which door you’re trying to walk through.**

---

### **3. Connections Change Everything — And They Don’t Come Easy**  
![connections](https://media.giphy.com/media/3oEduQAsYcJKQH2XsI/giphy.gif)

When I was younger, I thought “knowing people” meant cheating. Now I understand that in industries flooded with applicants, referrals are the only way a lot of hiring even functions. It isn’t always malicious — sometimes companies literally cannot review thousands of applications fairly. That means conversations become more valuable than resumes. When engineers vouch for you, momentum starts. When no one knows you, you often stay invisible, even if your work is good.

That doesn’t mean people get hired for doing nothing. It means most people need someone to open the door first so their work can actually be seen. And that is difficult when you are coming from outside the field. It takes longer. It takes more follow-ups. More messages. More rejection. More willingness to reach out when every instinct tells you not to bother.

Good work matters — but trust moves faster than GitHub links.

---

## Closing Notes  
![hopeful-again](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExZXNkeG5zeG05OW9yMmJlcnIzZGQyeDkxNzYzdjNxZW1hczA4aDZzaiZlcD12MV9naWZzX3NlYXJjaCZjdD1n/1QhmDy91F9veMRLpvK/giphy.gif)

I am not writing this as a success story or a lesson in perseverance. Right now, it just feels like I have come too far to walk away without at least finishing what I started. I spent years learning how to build things that actually work — not perfect, but real. I proved I can build and deploy working systems. That should matter somewhere, but so far it hasn’t moved anything.

I know the system is uneven. I know luck and connections move people ahead faster than effort. I know some roles are already filled before the posting even goes live. I know real skill often never gets seen. And I know none of that changes just because I work hard.

So I am not saying “I’ll make it.” I do not know that. I’m saying I am still here because right now I don’t see a better option. I keep building because stopping would make everything I already did feel pointless. I keep learning because forgetting what I’ve learned would make the past few years feel wasted. I keep trying because I don’t know what else someone in my position is supposed to do.

I am not looking for motivation. I am looking for an opening.

Until one shows up, I’ll keep moving — not because I believe in the system, but because turning everything off would feel worse than trying again tomorrow.

-Thank you for reading my vent

PS. i know this sounds like an edgy emo kid rant, thats cause im an edgy emo kid at heart.


How to Launch AWS Stacks for Free — And What Happens When the Free Tier Ends

## An honest guide to running cloud infrastructure without surprise bills

When I started building projects that hit the cloud, one of the biggest doubts was: “Can I really launch this stuff for free?” The short answer: yes, you can launch on AWS for free — or at least extremely low cost — but the longer and more honest answer is that you need to understand exactly how the free tier works, what the limits are, and when you’ll cross into paying territory. This post walks through how I spin up full stacks on AWS, what free means in practice, where the hidden traps are, and how much things cost when the free buffer disappears.

---

## Understanding the AWS Free Tier

When you sign up for AWS as a new account, you get access to a Free Tier. According to AWS:  
“Gain free, hands-on experience … with select services” as part of the Free Tier.

The Free Tier breaks down into three categories:

### **1. 12-Month Free Tier**  
Many core services (like EC2 micro instances, RDS micro DBs, S3 storage) are free for up to **12 months** from account creation, within certain usage limits.

### **2. Always Free**  
Some services or usage levels remain free even after the 12 months, but with limited quotas (Lambda requests, DynamoDB storage, etc).

### **3. Short-Term Trials**  
Occasionally specific services get temporary trials.

One of the biggest parts of the Free Tier: 750 hours/month of a **t2.micro or t3.micro EC2 instance** (enough to run 1 instance 24/7) during the first 12 months.

---

## How I Use It To Launch a Stack

Here’s how I actually spin up a full AWS stack for free or very close to free.

I start by defining what I actually need:

- a micro EC2 instance (or Lambda + API Gateway)
- an S3 bucket for static assets
- maybe a small RDS micro DB
- IAM roles
- CloudWatch logs
- security groups and basic networking

Then I check whether each part is Free Tier eligible, and whether I’m still within the 12-month window.

I deploy the smallest possible instance sizes, choose the cheapest region, and I aggressively clean up anything I’m not using:

- terminate EC2 instances  
- delete EBS volumes  
- empty and delete buckets  
- remove unused IPs  
- remove snapshots  

Free is not automatic. If you forget resources, they run up costs.

---

## Hidden Traps: How “Free” Becomes “I Just Got Billed”

I’ve been hit by random charges for dumb reasons. These are the traps most people fall into:

### **Idle EC2 instances / leftover volumes**  
Free Tier covers *compute hours*, but **EBS storage still costs money** if you leave volumes behind.

### **Data transfer, snapshots, cross-region mistakes**  
Free Tier doesn’t usually cover:

- inter-region traffic  
- snapshot storage  
- elastic IPs not attached to running instances  

### **Multi-region deployments**  
Free Tier quotas apply per account, *not* per region.  
If you deploy in two regions, you burn double the hours.

### **CloudWatch logs piling up**  
Logs accumulate endlessly unless you set retention rules.

### **Free doesn’t mean forever**  
Once your 12 months run out, you pay normal pricing across the board.

If you don’t monitor, “free” can turn into a surprise $50–200 bill very quickly.

---

## Actual Cost Breakdown (Hourly, Daily, Weekly, Monthly, Yearly)

Below are realistic numbers based on common AWS services.

### **EC2 t2.micro or t3.micro (after free tier ends)**  
On-demand cost in us-east-1 is around **$0.0116/hr**.

- Hourly: ~$0.0116  
- Daily: ~$0.28  
- Weekly: ~$1.96  
- Monthly: ~$8.40  
- Year: ~$102  

Two forgotten micro instances?  
Double it: ~$200/year.

---

### **S3 Standard Storage (after free tier 5GB)**  
S3 Standard costs about **$0.023/GB/mo**.

Example: 50GB stored:

- 50 × $0.023 = **$1.15/month**  
- Yearly: $13.80  

But PUT/GET requests and transfer can add to that.

---

### **CloudWatch Logs**  
CloudWatch has free ingestion + free Live Tail minutes, but if you exceed:

Example: 18,200 billable minutes  
→ 18,200 × $0.01 = **$182/month**  
just for log viewing.

This happens fast if you don’t set retention.

---

## Putting It All Together

If you’re disciplined:

- **$0/month** is totally realistic.

If you exceed limits a little:

- **$10–$20/month**.

If you forget resources:

- **$50–$200/month**.

If you forget *everything* for a year:

- **$600–$2,400** depending on what’s running.

---

## My Workflow for Keeping AWS Actually Free

This is how I avoid bills:

- Launch the minimum necessary resources  
- Use micro instances only  
- Prefer S3 + CloudFront static hosting  
- Turn off or delete unused resources immediately  
- Add deletion-lifecycle policies for logs  
- Add AWS Budgets alerts (I set $5/month alarms)  
- Use tools like Lambda and DynamoDB which have Always Free quotas  
- Tag resources so I know what everything is for  

The best rule I use is simple:  
**If I’m not actively using it, I delete it.**

---

## What Changes After the Free Tier Ends

After the 12-month period:

- EC2 charges fully  
- RDS charges fully  
- S3 beyond Always Free charges fully  
- CloudWatch logs charge based on storage and ingestion  
- AWS will not shut down anything automatically  

You graduate from the sandbox into pay-as-you-go.

It’s still powerful.  
It’s just not free anymore.

---

## Final Thoughts

Launching on AWS for free is absolutely possible — I’ve done it countless times. But you need to treat “free” as a strategic window, not a permanent state. The moment you stop watching usage, you can rack up very real bills.

For prototypes, portfolios, student work, and small tools: AWS Free Tier is perfect.

For production without cost controls: it becomes expensive fast.

Stay curious. Watch your resources. And always check your bill before AWS checks it for you.

— **Bradley Matera**  
(bradleysgatsbyblog.netlify.app)


How I Actually Build Full End To End Projects Using AI
#
career
#
javascript
#
webdev
#
productivity

I build a lot of projects. Full stacks, dashboards, demos, WebGPU stuff, AWS deployments, Docker setups, GitHub Actions pipelines, authentication flows, routing, UI components, all of it. On paper it looks like I am some senior engineer cranking out production apps. I am not. What I am is someone who learned how to use AI extremely well, who understands enough about web development to steer a project, and who has built so many things that the repetition turned into a weird practical skillset. This is how I actually build things end to end with my current knowledge. No hype. No fake humility. Just what really happens.

---

## Where My Knowledge Actually Sits

I have a B.S. in Web Development from Full Sail University. That program moves fast, one class a month and then it is gone. We switched topics constantly and you do not really get the luxury of living inside one subject long enough for everything to sink in.

What I got from school was basic HTML, CSS, and JavaScript, some React, some Python and SQL, some UX and layout fundamentals, some cloud concepts, some testing ideas. Enough to not be lost, but not enough to feel like a classic engineer.

On the flip side, things I do not have include any real data structures background, no algorithms training, no serious math foundation, no whiteboard experience, and I absolutely cannot sit down and write a full solution from a blank file with no references.

What I can do is read JavaScript, React, HTML, CSS, and Python code and understand what it is doing, follow how a full stack app behaves once it exists, describe exactly what I want in plain language, steer AI toward what I am actually trying to build, debug broken systems until they behave, and deploy almost anything if you give me time, logs, and a terminal.

My AWS internship helped a lot because it made me comfortable with AWS consoles, the CLI, EC2, S3, RDS, IAM basics, CloudWatch dashboards and alarms, and runbooks and troubleshooting workflows. It did not turn me into a principal cloud engineer, but AWS stopped feeling like some sacred scary thing and turned into a normal toolbox. That is my actual starting point. Everything else comes from building and breaking things in public.

---

## How I Start Projects (Even When I Don’t Know Enough)

Most real engineers can open an empty folder, run npm init, and start coding from their head. I cannot. So I start in a different way.

I describe the project like I am talking to a friend. I do not open with architecture or design patterns. I open with vibes and behavior.

My first messages to AI look like:

“I want a full stack app where users can sign up, log in, and post stuff to a feed,”  
“I want a clean WebGPU page that renders a triangle and maybe a cube I can flip between,”  
“I want a simple Node backend that handles authentication and a React front end that consumes it,”  
“I want a static portfolio with a blog and project cards that link to my real work.”

I never start with words like microservices or hexagonal architecture. I talk like a normal person and AI fills in the technical details. I correct it if it goes too wild. The point is that I anchor the idea in behavior, not implementation.

After the idea is clear, I let AI propose the first structure. I ask things like “show me the folder structure you would use for this,” or “where would you put pages, components, API routes, and config,” or “which files should I create first.” AI gives me a tree. Sometimes it's bloated and sometimes it’s too minimal. I tweak it until it feels like something I can actually navigate. I am not optimizing for textbook best practices. I am optimizing for “will my brain understand this when I come back in three days.”

Then I ask for the first file instead of the whole app. I never say “write the full app.” I say things like “give me the entry file and a single route that returns hello world,” or “give me a React page that renders a placeholder layout,” or “give me the WebGPU init function that just clears the screen to a color.”

Then I paste it in, install dependencies, and try to run it. It almost never works the first time. That is fine. I send AI the error and we start the real work.

From there I let the project teach me what it needs next. If I need authentication, I ask for that. If I need a Dockerfile, I ask for that. If I need a GitHub Actions workflow, I ask for that. If the layout is ugly, I fix it. I do not sit down and architect everything up front. I build until there is friction and then I deal with the friction. The system slowly reveals its shape over time. I do not architect from theory. I architect from pain and repetition.

---

## Where AI Helps and Where It Breaks Everything

AI is basically my pair programmer, except it has no idea where any of the files really live and it lies sometimes.

It is good at generating boilerplate quickly, filling in syntax I cannot remember, wiring up simple React components, writing repetitive CRUD routes, creating example tests once I describe what needs to be tested, translating my plain language idea into a first-pass implementation, and even giving me rough AWS diagrams or CloudFormation ideas based on a description.

But it is terrible at remembering my exact file paths, sticking to one pattern instead of switching styles mid project, generating security-safe code unless I constantly force it to, not hallucinating older versions of APIs or tools, randomly inventing config options that do not exist, messing with service workers and caches without warning, and it absolutely cannot handle WebGPU unless I spoon-feed it context from my own files.

My workflow always ends up looking like this:

I ask for a piece of code, AI gives me version one, I run it, something breaks, I paste the error back, AI adjusts, I run it again, something else breaks or behaves weird, and this repeats until it finally works.

People on social media love to claim “AI is doing everything,” but that completely ignores the part where I am the one deciding when the answer is nonsense, gluing all the pieces together, tracking the overall shape, dealing with the real world behavior, and connecting project to deploy instead of just code to code. AI is the engine. I am the driver and the mechanic.

---

## What I Bring That AI Cannot

There is a lot AI cannot see and cannot touch, and that is where I come in.

My biggest strength is that I understand the system once it exists. I can read through a codebase, trace a request from entry to response, follow state through the frontend, understand what the API is doing, understand where the data goes, and make sense of how the folders and files map to the mental model of the app.

I debug in the real environment, which AI cannot do. I open DevTools, inspect the network tab, check request and response bodies, read console logs, tail logs in Docker, look at GitHub Actions logs and artifacts, curl endpoints and diff them against local, inspect service workers, clear caches, and poke at CDN behavior. AI only sees what I paste in. I see absolutely everything.

I also make the real decisions. AI does not know that I am deploying to GitHub Pages or that I want a static export instead of server rendering or that I want to keep the stack simple instead of over engineered. I know what I am aiming for. That means I choose when to flatten the file structure, when to split a file, when something needs Docker, when to keep state management simple, when to use Next.js versus plain React, and what makes the project maintainable for me. AI outputs code, but I define the boundaries and the shape.

And when it comes time to actually ship a project, AI is nowhere near helpful. AI cannot log into AWS. AI cannot configure IAM. AI cannot fix DNS. AI cannot deal with real SSL errors. I am the one setting environment variables, mapping domains, fixing path issues in static exports, clearing stale caches and stuck service workers, and rerunning builds. Deployment is not “hit the AI button.” Deployment is me in a terminal, a dashboard, and a browser making everything behave.

---

## What the AWS Internship Changed For Me

The AWS internship changed a lot for me because it showed me that I can survive in real technical environments.

I got two certifications, AWS Solutions Architect Associate and AWS AI Practitioner, during a 12-week internship window, both passed by week 7 without AI. It was real studying and real labs.

That gave me comfort navigating AWS consoles and docs, understanding alarms, runbooks, and escalation paths, and familiarity with EC2, S3, RDS, IAM, and CloudWatch in actual workflows. I got a sense of what “normal” looks like when systems are healthy and how big companies design reference architectures.

When you mix that with my AI-heavy building style, you get a weird combination where AI writes a lot of code, my AWS background keeps me from being scared of the infrastructure, and my debugging brain glues it all together. I am not walking around acting like a principal architect, but I am also not lost when someone says “we need to deploy this to AWS.”

---

## The Real Cycle Behind Every Project I Build

If you zoom out and look at every project I do, the pattern is simple even if it is messy.

I describe what I want in plain language, AI gives me a structure and some starter files, I wire it up and try to run it, it fails, I debug with AI and my own eyes, it eventually works locally, I set up a build and deploy target, the deploy fails in some new way, I debug that using logs and DevTools and trial and error, it finally works in production, I polish the UI and text so it looks intentional, I document it enough that people can understand what it does, and then I move on to the next idea.

There is no magic. Just a lot of cycles.

---

## I Don’t Start With Structure — I Grow Into It

I do not start with structure the way traditional engineers do. I grow into it. I start with how the product should feel and a couple behaviors I care about, and then I let the system evolve until the structure becomes obvious.

The project keeps telling me when a page is doing too much, when logic is duplicated, when a folder is chaotic, or when a route belongs somewhere else.

Structure is not planned up front. It emerges from friction. Pain is the signal.

If something is hard to find, I move it.  
If something breaks too often, I isolate it.  
If something is confusing to name, I rethink what it actually is.  

It is slower and messier than perfect architecture, but it fits how my brain works.

---

## Debugging Is the Education

Debugging is not a side effect for me. It is the education.

I do not learn by memorizing theory. I learn because something breaks. A runtime error, a layout issue, a failed deploy, a permissions problem, a cache bug, undefined data, missing files, broken WebGPU behavior — these are what force me to form real understanding.

When something is broken, suddenly I have an actual question to chase.

Why is this undefined here but not there?  
Why is this page loading old assets?  
Why does this route work locally but 404 in production?  
Why does this WebGPU demo run on Chrome but not Safari?

Chasing these questions is what makes knowledge stick.

---

## How My Background Shapes This Whole Approach

My background as a medic in the 82nd Airborne plays into this too.

In that world you rarely get perfect information. You get symptoms, pressure, and a small window to act. You stabilize, triage, move, reassess, and repeat.

Web projects obviously are not life or death, but the mindset carries over. I do not freeze because I don’t know everything. I move based on what I can see: logs, browser errors, CLI output, network behavior, AWS console states, and feedback from the environment.

AI works extremely well in this loop because it gives me something to react to fast. I take that and push the system forward.

---

## What This All Adds Up To

All of this, when you strip away the noise, adds up to real skills.

I have practical literacy in modern web stacks.  
I can read and understand code I did not write.  
I know how to steer AI toward working systems instead of random snippets.  
I have real debugging experience from real deployments.  
I have enough AWS experience to not drown in cloud workflows.  
I am comfortable with containers, pipelines, and static hosting.  
And I have shipped a lot of projects that prove I am not just talking.

Things I do not have include low-level systems programming knowledge, advanced algorithms, leetcode skills, or the ability to write everything from scratch without leaning on tools.

What I do have is the ability to build real working systems — systems people can click on, systems that deploy cleanly, systems that are understandable, systems that
 behave.

If a company wants deep algorithmic knowledge, hardcore math, systems programming, or whiteboard-from-scratch engineering, that is not me.

But if a company wants someone who can work inside an existing stack, understand the codebase, use AI as a multiplier, ship features end to end with support, debug weird real world behavior, and actually deploy things users can touch, that is where I fit.

---

## Final Thoughts

AI is how I write a lot of my code. Debugging is how I learn. My AWS internship and my degree gave me enough of a foundation to not drown. My projects are where everything actually came together.

I am not a textbook software engineer. I am a practical builder who uses AI heavily, learns through friction, and ships things anyway.

You do not need perfect background or deep theory to build real systems. You need a problem, a tool, a feedback loop, and enough stubbornness to keep going until the thing in your head exists in the browser.

That is how I work right now. Honestly.


The Day My Gatsby Nav Bar Made Me Question Reality
When I rebuilt my portfolio with Gatsby, I expected normal front end issues like spacing or a missing import. What I did not expect was the nav bar fighting me like it had a mind of its own. On desktop it looked fine at first glance. On mobile and certain screen sizes it turned into something between a glitch and a crime scene. The spacing was wrong, the padding drifted depending on the browser, and random items would shift like they were trying to escape the layout.

At first I thought it was just sloppy CSS on my part, but the deeper I went the stranger it got. I fixed a margin on one breakpoint and it broke the layout on another. I aligned the logo and it pushed the entire menu off screen. It felt like fighting a ghost that would fix itself when I looked away and then break again when I touched literally anything.

## The first sign something was off

I started noticing weird inconsistencies:

- on desktop the nav looked clean  
- on iPhone it stacked wrong  
- on an Android tablet one of the links slipped down a few pixels  
- in Chrome responsive mode the entire thing bounced when resizing  

It made zero sense. The code was simple. Normal React components. Normal Gatsby layout. Normal styled components. But the nav acted like it was built out of jello.

The worst part was that every person who looked at it gave different feedback. One guy said the spacing looked fine. Another sent me screenshots where the text looked like it was orbiting the logo. I had no idea which version was real.

## The first round of fixes that went nowhere

I tried everything that made sense:

- tightened padding  
- deleted margin rules  
- replaced flexbox with grid  
- replaced grid with flexbox again  
- normalized line height  
- used gap utilities  
- tried a CSS reset  
- rebuilt the component from scratch  

Nothing solved it across every viewport. Every fix created a new problem. At one point the mobile menu icon drifted so far off the screen it looked like it was trying to run away from my codebase. I knew I was missing something.

## Finding the root cause

When I finally slowed down and treated the nav like an actual debugging problem instead of a quick CSS annoyance, I noticed the real issue: the structure of the component itself was messy. Not broken, just built in a way that made every style rule interfere with something else.

Here were the real issues hiding underneath:

- nested containers fighting for space  
- padding declared on both parent and child elements  
- flexbox alignment rules that canceled each other out  
- inconsistent font sizes causing invisible height changes  
- Gatsby theme defaults adding margins I never set  
- no global spacing system  

Individually these things seemed small but together they created a death spiral. There was no single source of truth. The nav bar was balancing itself on invisible calculations.

## The move that finally fixed everything

Instead of duct taping the layout, I tore the whole thing down and rebuilt it with actual intent. Clean, simple, and predictable. I created a real spacing system. I gave the nav its own dedicated wrapper. I made sure no child element used padding unless it had a reason. And I kept the font sizes consistent so they did not secretly affect vertical alignment.

The final structure looked like this:

```html
<header class="site-header">
  <nav class="nav-container">
    <div class="nav-left">
      <a class="logo" href="/">Bradley Matera</a>
    </div>
    <div class="nav-right">
      <ul class="nav-links">
        <li><a href="/projects">Projects</a></li>
        <li><a href="/blogs">Blog</a></li>
        <li><a href="/about">About</a></li>
      </ul>
    </div>
  </nav>
</header>
```
Clean. Predictable. No unnecessary wrappers. No random spacing rules.

## Why it worked

Once the structure was clean, fixing the padding and alignment became normal work again. Responsive breakpoints were actually responsive. The mobile menu icon stayed where it was supposed to. The logo stopped drifting like it was floating in space.

The layout finally behaved because nothing was fighting behind the scenes. No inherited margins. No double padding. No default Gatsby theme styles sneaking in. Everything had one clear job.

## What I learned

This entire mess taught me something simple:

If the layout keeps breaking in random ways, the structure is probably wrong.

Bad structure means every style rule becomes a gamble. You can fix a small thing and accidentally break something two components down because everything is chained together in ways you cannot see.

Rebuilding the nav from scratch took less time than all the hours I spent firefighting random alignment bugs.

## Final thoughts

Front end bugs are loud when the problem is small and quiet when the problem is structural. The loud stuff distracts you, but the quiet stuff hurts you.

Once I cleaned up the skeleton of the component, the nav bar stopped acting haunted and started acting like a normal layout again.

Sometimes the fix is not more CSS. Sometimes the fix is admitting the component needs to be rebuilt instead of patched.

The new nav bar works everywhere and finally matches what I had in my head from the start.

The Day a Service Worker Held My Entire Site Hostage
#
webdev
#
devops
#
devbugsmash
#
How a single cached file made my entire GitHub Pages site refuse to update, why it happened, and exactly how to fix it if it ever happens to you.

## What actually happened

My GitHub Pages frontend absolutely refused to update after a full rebuild. I deleted the old code, pushed a new Next.js build, cleaned every folder, checked the deploy logs, and refreshed the live site a hundred times. The same old UI kept loading. Same filters button. Same old DOM. It looked like my deploy did not even run.

GitHub Actions passed every time. The artifact had the correct static export. My repo was clean. My build folder was clean. I even ran `grep` across the entire project looking for any leftover UI code and found nothing.

Then I hit the live URL with `curl` and it returned the correct new markup. But the browser still showed the old one. So at that point the server and the browser disagreed. When that happens, something in the middle is lying to you.

And that something was a service worker I did not even know existed.

## What the actual root problem was

A stale service worker was still registered in the browser from an earlier version of the site. I did not write the service worker myself. AI generated it once, and I never questioned it. I did not know what “sw.js” even meant at the time.

A service worker sits between the browser and the network. It can intercept requests and decide what to return. My service worker had cached the old version of the site and never let go of it. So even though the server was sending the new version, the browser ignored it.

The browser trusted the service worker more than the server.

This is the exact kind of bug that does not feel logical until you understand how aggressive service worker caching is.

## The fix that finally worked

Here is the fix that instantly solved it:

### Step 1  
Open your site in Chrome.

### Step 2  
Open DevTools with:  
`Ctrl + Shift + I` or `Cmd + Option + I`

### Step 3  
Go to the **Application** tab.

### Step 4  
Click **Service Workers** on the left.

### Step 5  
You will see one or more registered service workers.  
Look for anything like:  
`sw.js`  
`service-worker.js`  
or anything showing “activated”.

### Step 6  
Click **Unregister**.

### Step 7  
Refresh the page.

If the service worker was the problem, the new version of the site will load instantly. No delay. No waiting. It is one of the most dramatic "instant fix" moments you can have in web development.

This is the exact moment it hit me that the entire issue was not my code or GitHub Pages. It was a cached worker from a build that did not exist anymore.

## How to confirm this is your issue too

Here are the fastest ways to check if you have the same problem.

### 1. Hit your site with curl  
Run this in your terminal:

curl -sL https://your-site.github.io | head

If the HTML here is newer than what your browser is showing, your browser is being lied to.

### 2. Open your site in Incognito  
If Incognito shows the new version but your normal browser does not, that is almost always a service worker or cache problem.

### 3. Check DevTools  
Look for an active service worker. If you see one and you did not explicitly write it, that is probably the issue.

## Why this problem happens to many developers

Service workers are extremely powerful. They allow offline support, caching, background syncing, and more. But the downside is that they aggressively hold onto cached files. This is on purpose. They are designed to keep apps working even if the network dies.

The problem is that GitHub Pages does not know anything about your service worker. So even after a fresh deploy, the browser can still keep serving old files because the worker is literally intercepting every request.

This is why so many people complain that their GitHub Pages site does not update after a deploy. The deploy is fine. The browser is not.

## Why this deserves a blog post

Because it is one of the most confusing and frustrating bugs you can hit as a newer developer. You can spend hours thinking your deploy is wrong, your build is wrong, your repo is wrong, or GitHub Pages is broken. But the truth is that your browser is simply loading an old cached version from a script you forgot about, or did not know existed.

Service workers can hold a website hostage if you are not careful.

## What I would do differently now

Since hitting this issue, here is what I changed in my workflow:

- I never generate a service worker unless I know why I need it  
- I always check DevTools when a deploy looks stuck  
- I test live sites in Incognito to avoid cache issues  
- I keep build processes simple until I fully understand what is happening  
- I clean out all AI generated files before deploying anything  

Caching bugs like this are sneaky and they waste time, but once you see how they work, they are easy to avoid.

## Final takeaway

If your website refuses to update even though your deploy is clean, it might be a stale service worker. You can confirm this by comparing the server output with curl and checking the Application tab in DevTools. Unregistering the worker fixes the issue instantly.

This bug taught me more about how the browser actually works than almost anything else I have built so far.

How to Sell Your Skills with a Small Project
#
portfolio
#
tutorial
#
beginners
#

### A Guide for Developers Who Feel Like They Don’t Have Enough Experience  
![overwhelmed](https://media.giphy.com/media/3og0IPxMM0erATueVW/giphy.gif)

A lot of people hold themselves back because they think they need a massive full-stack application to prove they can code. The truth is — a basic web demo can show more than enough skill **if you build it intentionally** and **explain it the right way**.

This guide is about taking something small — literally **three files** — and turning it into something you can confidently put on a résumé, in a portfolio, or discuss in interviews without exaggerating your abilities.


---

## Step 1: Your Project Can Be Small — Just Make It Real  
![determined](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExc2ZqMmdvaXI5ZjE5ZHJxcWFxZTRjOTk1eHU3bG11a21jc3E5cTZoaiZlcD12MV9naWZzX3NlYXJjaCZjdT1n/5xaOcLGvzHxDKjufnLW/giphy.gif)

You don’t need a full SaaS app. You don’t need authentication systems and complex APIs.  
For most junior roles, a realistic demo looks like this:

```txt
index.html
style.css
script.js
```
That can be your entire codebase, hosted on GitHub Pages, and it can still prove you know what you’re doing — if you treat it like a real product.

The key isn’t size — it’s intention. Every file should serve a purpose, and every purpose can be explained as real engineering work.

⸻

Step 2: What Skills Can You Show?
![problem-solving](https://media.giphy.com/media/l0MYt5jPR6QX5pnqM/giphy.gif)

Here’s what a recruiter or engineer sees when they look at even a simple demo site — IF you structure it well and document it clearly.

Frontend Thinking

If your layout makes sense and responds well to different screens, that’s frontend engineering.
If you explain why something goes where it goes — that’s UX/UI understanding.
If you use CSS tokens or variables — that shows planning and scalability.

State & Data Handling

If your site uses localStorage, sessionStorage, or even a simple JSON file — you’re showing backend logic and data flow understanding.
If you handle user input with validation — that’s real logic, not just visuals.

Deploying It Live

If it’s on GitHub Pages, Netlify, Vercel, or any real host — then you’ve already touched DevOps:
	•	version control
	•	build settings
	•	deploy logs
	•	environment setup
	•	troubleshooting live errors

That alone puts you ahead of people who only code locally.

⸻

Step 3: Make the README Do Half the Work for You  
![writing](https://media.giphy.com/media/3ohzdIuqJoo8QdKlnW/giphy.gif)

Your README is where you sell yourself without lying.
You explain what the project is, why you built it, what problem it solves, what decisions you made, and what you’d improve later.

That shows growth, and growth is what recruiters actually look for in junior devs.


⸻

Step 4: Add One Detail That Makes It Stand Out  
![lightbulb](https://media.giphy.com/media/bcKmIWkUMCjVm/giphy.gif)

To turn a basic demo into real proof of skill:
Pick one feature and build it well.

Examples:
	•	A theme toggle with JavaScript and localStorage
	•	Live form validation
	•	Simple data filtering and sorting
	•	A dynamic card system (like a product grid)
	•	A user settings panel that remembers choices

Something small, but intentional.


⸻

Step 5: Talk About the Project Like an Engineer  
![confident-presentation](https://media.giphy.com/media/3oriO7A7bt1wsEP4cw/giphy.gif)

In interviews or your portfolio, never say:

“It’s just a small demo.”

Say this instead:

“I built a small frontend project to show responsive design, basic data handling, and deployment. I focused on user flow, accessibility tags, and real interaction. If I had more time, I’d expand it with a backend and user system.”

You didn’t lie.
You didn’t exaggerate.
But you framed it like someone who thinks in systems — which is what engineers do.


⸻

Final Message to Anyone Who Feels Behind  
![hopeful](https://media.giphy.com/media/l0HlOvJ7yaacpuSas/giphy.gif)

Your project doesn’t have to be huge. It just has to have a reason.
If it runs, if you can explain it, and if you can improve it — that counts.

Three files can become a job interview.
A GitHub Page can become your first portfolio piece.
One small idea can start everything.

Build it small.
Learn from it honestly.
Deploy it anyway.
Let it represent you — until you build the next one.

That’s how you sell your skills without lying — and without feeling like a fraud.

---

My Real AI Development Setup
#
webdev
#
javascript
#
ai
#

# My Actual AI Development Setup

Using AI tools has become part of my normal workflow because they help me stay productive when I get stuck. None of the tools I use solve everything on their own, and each one fills a different gap. VS Code is the main place where most hands-on editing happens, but only one of the agent tools actually runs inside it. Everything else is opened separately depending on what the project needs. This setup works because the tools support the work rather than replace it, and it allows progress even when the project moves into areas that are unfamiliar.

---

## How VS Code Fits Into the Setup

VS Code is the editor where almost all file editing takes place. Cline is the extension that actually interacts with the environment. It can read the project tree, open files, modify them, run commands, react to errors, and propose patches that fit the structure of the repo. Having an agent inside the editor turns small tasks into manageable steps instead of long guesswork loops. It does not handle everything, but it reduces friction when a project involves a lot of moving parts.

---

## How Ollama and Qwen Help With Daily Coding

Ollama runs locally and hosts the models that handle most of the routine reasoning. Qwen models run quickly, do not require tokens, and work without an internet connection, which makes them useful for day-to-day coding. Local models like Qwen help clean up components, fix TypeScript or JavaScript issues, reorganize files, adjust imports, and write small helpers. Since they run inside Cline, they can read the actual project structure and respond based on the files instead of guessing. This makes local models useful for steady incremental progress.

---

## When the Workflow Switches to Claude Models

Sometimes local models run into problems that require better reasoning or a clearer explanation. In those cases, the model used by Cline gets switched to a Claude model. Claude is useful for situations when logic spans multiple files, when a messy section needs to be rewritten, when build problems become confusing, or when WebGPU code needs a deeper breakdown. Claude handles larger context better, and Cline carries out the edits. This combination works well when the project moves beyond small isolated fixes.

---

## Where Cursor Fits In

Cursor is a separate AI IDE and it is not part of the VS Code setup. It gets opened when the project needs larger structural adjustments. Cursor helps when a React codebase becomes disorganized, when a feature touches too many files to fix in small patches, or when a Webflow-related feature needs cleaner output. It is not used constantly. It is used when the shape of the project needs to be fixed rather than the individual lines of code.

---

## How Kiro Fits Into AWS Work

Kiro is another standalone AI IDE and only becomes relevant when a project involves AWS. It handles tasks like diagnosing IAM issues, adjusting S3 bucket policies, checking CloudWatch logs, fixing AWS CLI errors, or reviewing configuration problems. Kiro reacts to real AWS output instead of generating architecture from scratch, which makes it helpful when deployments hit AWS-specific problems. It stays closed unless the work involves infrastructure tasks.

---

## Copilot’s Small Role in the Workflow

GitHub Copilot runs quietly in the background. It handles small things like JSX completion, basic patterns, tiny helper functions, or quick loops. It does not make architectural decisions and it does not interact with the larger structure of the project. It operates like autocomplete. It saves time on typing but does not influence bigger parts of the workflow.

---

## Why Markdown Files Matter

Markdown files act as the stable memory that all tools rely on. They hold the architecture notes, naming conventions, TODO lists, route summaries, design notes, and deployment instructions. These files get opened before making changes so agents understand the direction of the project, and they get updated when large edits are made. This helps avoid drift, keeps everything aligned, and makes sure the project maintains a clear structure even when multiple tools are involved.

---

## What a Real Session Feels Like

A normal session begins by opening VS Code with Cline running and a local model active through Ollama. A small task gets described, Cline inspects the relevant files, and it proposes changes. Those changes get reviewed and either accepted or rejected. The app is run to see what happens. Errors show up, the error output gets pasted into the tool, and Cline attempts to fix the issue. If the reasoning needs to be stronger, Claude is switched in. Once everything works locally, the work is committed and pushed. When deployment breaks, debugging happens with logs, browser tools, or AWS dashboards. If AWS becomes the blocker, Kiro is opened to diagnose the issue.

This loop repeats until the feature is complete. The setup does not automate the process, but it reduces the amount of time spent hunting for solutions in the dark. It keeps the project moving forward even when knowledge gaps show up.

---

## Closing Notes

The mix of tools in this setup is not about trying to appear advanced or creating an automated pipeline. It is simply a practical way to keep working through projects, especially during parts that would normally slow everything down. Each tool handles a specific type of problem, and switching between them depending on the situation makes it easier to finish features and learn at the same time. The tools do not replace development. They support it so progress continues even when the work gets complicated.


The Day I Learned How Confusing GitHub Pages Deployments Can Get
#
webdev
#
devops
#
github
#
githubactions
I always thought GitHub Pages was simple. You build your project, push the static export, and the site updates. That is what the docs say and that is how everyone on Reddit and StackOverflow talks about it. So when my site refused to update and kept serving old HTML from some random folder I forgot existed, I figured I just messed up a command.

Turns out this was one of those problems that teaches you way more about the platform than you ever wanted to know.

This whole thing started because my repo had multiple deployments in it over time. I had old folders, old builds, old static exports, and GitHub Pages settings that I never checked after the very first time I set the repo up. It was the perfect storm of confusion.

## Where things started breaking

My project had gone through a few iterations. I had:

- an older site that used a `/docs` folder  
- a later version that output to `/dist`  
- some leftover static files inside the root  
- GitHub Actions workflows I copied from older repos  
- Pages pointed at a folder I forgot even existed  

At first none of this mattered because I was not rebuilding the repo often. But when I pushed a brand new version of the site, the Pages server kept pulling from that ancient `/docs` folder instead of the new static export.

I did not even remember creating `/docs`. It was from a build system I used months before.

## The symptoms

I would deploy and the site would show:

- old CSS  
- old JavaScript  
- old HTML  
- broken layout  
- missing assets  

I checked the code and everything in my repo was up to date. The build artifact looked correct. The static export was correct. The workflow logs were clean.

Meanwhile the live site looked like it was living three timelines behind my repo.

## The moment I realized the issue

At one point I opened GitHub Pages settings and it hit me immediately.

The Pages source was set to:

/docs

Not:

/dist

Not:

/build

Not anything modern.

Just the old folder I did not delete because I assumed it did not matter.

That small detail caused the entire deployment to serve stale content. Pages was not reading my new build at all. It was faithfully serving whatever was in `/docs` because I had told it to do that a long time ago and then forgot.

## Cleaning it up

Once I figured that out, the fix was straightforward:

1. Delete the old `/docs` folder completely.  
2. Update the Pages deployment settings to point to the correct build output.  
3. Clean the repo of leftover static files.  
4. Commit a fresh static export from Bun or Next.js.  
5. Trigger a clean GitHub Pages build.  

The second I deleted `/docs` and pointed GitHub Pages to the actual build directory, everything snapped back to normal. The new layout loaded. The CSS updated. The site finally reflected the code I was actually writing.

## What the problem really taught me

This was not a bug. It was user error plus how GitHub Pages is designed.

Pages keeps serving whatever folder you told it to serve until you explicitly change it. It does not auto detect changes. It does not switch to a new build folder because it looks “more correct.” It sticks to the literal path you set.

If you point Pages at `/docs`, it will die on that hill until you fix it.

That is both a feature and a trap.

## How to avoid this mess

If your GitHub Pages site serves old content even though your build is correct, check this list:

- Look at the Pages source folder.  
- Make sure your static export path matches that folder exactly.  
- Delete any old build folders you are not using.  
- Clean out orphaned HTML files in the root.  
- Make sure your workflow artifacts match your Pages directory.  

It takes five minutes to check, but if you forget how the repo was originally configured, you can lose hours trying to debug something that is not actually broken.

## Final thoughts

This taught me to treat deployment folders like infrastructure. They matter. They carry history. And if you do not clean them up, they can bite you months later when you least expect it.

Now I check my Pages settings before every new repo upgrade. Not because I am paranoid, but because I now understand exactly how easy it is to break a Pages deployment by accident.

And how annoying it is when GitHub is faithfully serving the wrong folder while you blame everything else.

Why I Rebuilt My WebGPU Triangle Demo From Scratch
#
webgpu
#
webdev
#
learning
#
webgl
I built my Triangle Demo because I wanted a clean WebGPU study lab that made sense for how I learn. I originally forked a public WebGPU project that showed what was possible, and it gave me the push to start exploring how triangle rendering works. The original project was great, but it had a completely different purpose and long-term direction. What I wanted was something smaller, simpler, and focused only on the “hello triangle” and “textured cube” fundamentals.

So instead of trying to bend something bigger into a study tool, I decided to rebuild my own version from the ground up.

## Why I needed my own structure

Once I started exploring WebGPU, I realized I learn best when everything in a project is built by me, for me, and matches what I’m trying to study. I needed:

- a minimal layout  
- no extra abstraction layers  
- short TypeScript files I could read in one sitting  
- a UI shell I understood  
- a direct mapping from code to canvas  

Not because anything else was wrong, but because I wanted a space where every file existed for one reason — to help me understand the pipeline.

## Building it clean

I rebuilt the entire demo using tools I already use daily:

- **Next.js 16** for the UI  
- **NextUI** for the layout  
- **Bun** for scripts and static exports  
- a single `/lib/webgpu` folder that holds the render loop logic  
- compact TypeScript files for the triangle and cube demos  
- simple WGSL shaders with no extra complexity  

I wrote my own initialization steps:

1. request the adapter  
2. request the device  
3. configure the canvas  
4. create buffers  
5. load WGSL  
6. submit draws  

With everything written by me from scratch, I could finally see exactly how each piece of the pipeline fits together.

## What the rebuild gave me

This rebuild wasn’t about fixing anything. It was about **clarity**.

The new setup gives me:

- a WebGPU project I fully understand  
- a clean space to tweak shaders  
- a simple path to experiment with buffers, attributes, and color outputs  
- direct connection between the code and what I see on the canvas  
- a predictable structure I can keep expanding as I learn  

Instead of navigating around a larger ecosystem with its own goals, I now have a focused lab designed specifically for learning WebGPU fundamentals. That alone made the rebuild worth it.

## What I learned

Rebuilding this demo taught me more than I expected:

- how the adapter/device negotiation really works  
- how pipelines, buffers, and shaders interact  
- how the render loop lives and breathes  
- how small changes in WGSL show up instantly on the canvas  
- how much easier learning is when the project matches your learning style  

This project is not an engine. Not a framework. Not a competitor to anything.  
It’s just a compact, personal study lab that helps me understand the basics without distractions.

## Final thoughts

The new Triangle Demo is clean, simple, and built entirely around learning. It keeps the triangle and the textured cube close at hand so I can focus on the actual WebGPU pipeline instead of navigating around unrelated features.

It’s the version that finally made WebGPU click for me, and it gives me a foundation to keep experimenting as the API evolves.


Making My Triangle WebGPU Demo Match What It Actually Is
#
learning
#
webdev
#
nextjs
#
webgpu

# Making My Triangle WebGPU Demo Match What It Actually Is

When I built my Triangle WebGPU Demo, I originally focused on getting the code working: the adapter, the device, the buffers, the shaders, the render loop. The technical side came together fast because I kept the code small and readable. But the site text did not really match the project. The UI looked polished enough that it made the whole thing seem like it was some kind of real graphics tool or early engine, and that was never the intention.

This blog is just me correcting that. I updated the wording on the site so it reflects the actual purpose of the project and does not give off the wrong idea.

## Why the site needed clearer wording

The code itself is extremely simple. It is a small setup built around two WebGPU samples that people use to learn the basics. But the layout, the clean UI components, and the animated demo switching made it look more complex than it really was.

Someone landing on the site with no context might assume:

- it is a lightweight rendering library  
- it is an early GPU tool  
- it is part of a larger graphics project  

None of that is true. It is just a study lab I built for myself so I could learn the fundamentals of WebGPU in a clean environment. The problem was not the code. It was the **presentation**.

## What the project really is

Triangle Shader Lab is just a place where I can see the WebGPU pipeline without noise. It focuses on two demos:

1. a triangle  
2. a textured cube  

That is the entire scope.

The value of the project is not in features. It is in clarity. I kept the main logic in one file, kept the WGSL small, and let the UI show the output right next to the code it comes from. I wanted something where I could tweak a buffer or a shader and instantly see what changed.

## What I changed

I rewrote the site’s text so it explains what the project is and nothing more. The new wording stays factual and avoids implying anything bigger. It focuses on:

- how the demo works  
- how the render loop is structured  
- what the code is for  
- what the project helps you practice  
- where the actual WebGPU learning happens  

The UI still looks nice, but now the description matches the reality. If someone opens the site, they will know exactly what they are looking at.

## Why clarity matters

I am still early in learning WebGPU, and having a simple, honest description of the project actually helps me learn better. When the wording is clean, it forces you to think about what you are actually building instead of what the UI accidentally makes it look like. It keeps you grounded and focused on the fundamentals.

This edit also made me appreciate how much wording affects perception. Even when the code is tiny, a polished layout can make people expect a whole framework behind it. I want the site to be approachable, not misleading.

## Final thoughts

This project is not a graphics engine. It is not a beginner-friendly version of something bigger. It is a small WebGPU learning space that keeps the triangle and the cube right in front of you while you study the pipeline.

Now the site’s text finally says that. Nothing inflated, nothing implied, and nothing outside the actual scope. Just a clean place to learn WebGPU the way I need to learn it.


What I Learned Building Three Simple Projects: AnimalSounds, CheeseMath, and a Node.js Secrets Tutorial
#
javascript
#
testing
#
security
#
webdev
I have been trying to understand how the web actually works by building things instead of watching tutorials. None of these projects are big, but each one taught me something I did not fully understand before. These three projects, AnimalSounds, CheeseMath, and the EthicsFrontEndDemo, helped me get a clearer feel for JavaScript, testing, and basic security.

## 🐾 AnimalSounds, learning how the browser behaves

**Live:** https://bradleymatera.github.io/AnimalSounds/  
**Repo:** https://github.com/BradleyMatera/AnimalSounds  

AnimalSounds is a simple soundboard. You press a button and it plays the sound. Working on it helped me understand DOM events, how the browser handles audio, and how to add basic keyboard accessibility. Building something small like this helped me see how front end code behaves in a real browser.

## 🧀 CheeseMath and Jest, my first real experience with testing

**Live:** https://bradleymatera.github.io/CheeseMath-Jest-Tests/  
**Repo:** https://github.com/BradleyMatera/CheeseMath-Jest-Tests  

CheeseMath looks goofy, but the whole point was learning Jest. This was the first time I wrote unit tests for my own code. I learned why separating logic from the UI matters, how simple tests work, and how tests reveal mistakes or edge cases I did not think about. It changed how I approach small functions and debugging.

## 🔐 EthicsFrontEndDemo, understanding the basics of handling secrets

**Live:** https://bradleymatera.github.io/EthicsFrontEndDemo/  
**Repo:** https://github.com/BradleyMatera/EthicsFrontEndDemo  

This project compares bad and better ways of handling secrets in Node.js and on the front end. Working on it helped me understand why client side secrets do not mean anything, how environment variables actually work, and how to explain the differences clearly. Some of the early text came from AI drafts, but building the project myself gave me a much better understanding of security basics.

## 🧩 What all three taught me

Building small projects like these helped me understand the fundamentals much better, including:

- how the browser handles events and rendering  
- how separating UI from logic makes testing easier  
- why accessibility should be part of the build  
- why tests matter for catching mistakes early  
- why basic security habits matter, even in small demos  

Each project gave me a clearer understanding of how web development works in practice.
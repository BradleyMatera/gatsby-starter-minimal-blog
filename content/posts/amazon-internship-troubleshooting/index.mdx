---
title: "AWS Cloud Support Internship: What I Actually Practiced"
date: "2025-08-18"
slug: "/aws-cloud-support-internship-mastering-troubleshooting-and-architecture"
tags: ["aws", "troubleshooting", "cloud"]
description: "A clear, bounded summary of training labs and a capstone build."
theme: "Cloud"
---

This is an internship post about guided training and a capstone.

It does not describe production ownership or customer tickets.

You will see the exact lab style steps I practiced and how I verified them.

I cannot verify these steps from this blog repo. Treat this as a lab guide you can run locally.

## The promise

By the end, you will:

- understand the scope of the training work
- see a concrete capstone outline
- know what I did and did not do

> "Guided labs build skill, but they are not production ownership."

## What this is

**High level:** This is a summary of guided lab rotations and a capstone build.

**Low level:** The capstone used S3, Lambda, DynamoDB, and an Amplify frontend plus a cost model.

**Key terms:**

- **Guided lab:** A training environment with sample data and instructions.
- **Capstone:** A final project that demonstrates skills in a controlled setting.

## What you need

- AWS account
- Access to S3, Lambda, DynamoDB, and Amplify
- A small test dataset

## Start to Finish

### Step 1: Create the S3 input bucket
Goal:
Create a bucket for uploads that Lambda can read.

Actions:
- AWS Console → S3 → Create bucket
- Name: `metadata-input-demo`
- Region: your default region

Why:
The bucket is the entry point for uploads. It provides a stable source for the workflow. This keeps the capstone simple. You can restrict permissions later if needed.

Verify:
- Expected: bucket appears in the S3 list.
- This confirms the input bucket exists.

If it fails:
- Symptom: bucket name already taken.  
  **Fix:** choose a unique name.

### Step 2: Add a Lambda function
Goal:
Parse object metadata when a file is uploaded.

Actions:
- AWS Console → Lambda → Create function
- Runtime: Node.js 18
- Trigger: S3 upload from `metadata-input-demo`
- Code snippet:
  ```js
  exports.handler = async (event) => {
    const record = event.Records[0];
    const key = record.s3.object.key;
    console.log("uploaded:", key);
    return { ok: true, key };
  };
  ```

Why:
A simple Lambda function proves the event flow. You can extend it later to extract metadata. This keeps the capstone easy to verify. The console log is the first proof step.

Verify:
- Upload a file to the bucket.
- Check CloudWatch logs for the key.
- This confirms the trigger works.

If it fails:
- Symptom: no logs.  
  **Fix:** check the S3 trigger permissions.

### Step 3: Store records in DynamoDB
Goal:
Persist extracted metadata.

Actions:
- AWS Console → DynamoDB → Create table
- Table name: `metadata-items`
- Partition key: `id` (string)
- Update Lambda code to write:
  ```js
  // pseudocode
  // put item with id and metadata
  ```

Why:
A database makes the result queryable. DynamoDB keeps it simple with no servers. This step turns logs into data. It also lets the UI fetch real records.

Verify:
- After uploading, check the DynamoDB table.
- Expected: a new item appears.
- This confirms persistence works.

If it fails:
- Symptom: no new items.  
  **Fix:** confirm the Lambda IAM role allows DynamoDB writes.

### Step 4: Add a simple frontend in Amplify
Goal:
Display the stored metadata in a web UI.

Actions:
- Create a small frontend that calls an API.
- Deploy via AWS Amplify.
- Set the API URL in environment variables.

Why:
A UI makes the capstone visible. Amplify provides a fast deploy path. Environment variables keep the API configurable. This is enough for a demo view.

Verify:
- Open the Amplify URL.
- Expected: a page loads and shows data.
- This confirms the end to end flow works.

If it fails:
- Symptom: API errors.  
  **Fix:** check the API URL and CORS settings.

### Step 5: Add a cost model note
Goal:
Document a basic cost estimate.

Actions:
- File path: `docs/cost-model.md`
- Add inputs:
  ```text
  Inputs:
  - Requests per month
  - GB stored
  - Lambda GB seconds
  - DynamoDB read and write units
  ```

Why:
A cost model shows you thought about pricing. It keeps assumptions visible. It also makes the capstone reproducible. A small note is enough.

Verify:
- Open the file and confirm all inputs are listed.
- This confirms the model is documented.

If it fails:
- Symptom: assumptions missing.  
  **Fix:** add the missing inputs.

## Verify it worked

- Upload triggers Lambda logs.
- DynamoDB contains metadata records.
- UI loads and shows data.
- Cost model inputs are documented.

## Common mistakes

- **Symptom:** Lambda never triggers.  
  **Cause:** Missing S3 trigger.  
  **Fix:** add the trigger and permissions.

- **Symptom:** DynamoDB writes fail.  
  **Cause:** IAM role missing permission.  
  **Fix:** attach the DynamoDB write policy.

- **Symptom:** UI shows no data.  
  **Cause:** API URL or CORS mismatch.  
  **Fix:** update the URL and CORS config.

## Cheat sheet

- S3 upload triggers Lambda.
- Lambda writes to DynamoDB.
- UI reads from API.
- Cost model is a simple note.

## Next steps

- Add retries and error logs.
- Add a small test script for the API.

## Related links

- https://docs.aws.amazon.com/lambda/
- https://docs.aws.amazon.com/s3/
- https://docs.aws.amazon.com/amazondynamodb/

## Final CTA

Keep the capstone small and prove each step. That is the honest path.

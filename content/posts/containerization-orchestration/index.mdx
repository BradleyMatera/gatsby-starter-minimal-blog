---
title: "Venturing Beyond Hello World: Mastering Containerization and Orchestration"
date: "2025-08-04"
slug: "/containerization-and-orchestration"
tags: ["Docker", "Kubernetes", "DevOps", "Infrastructure"]
description: "Embark on my journey from simple container experiments to orchestrating complex, production-ready deployments with Docker Compose and Amazon EKS."
theme: "Cloud & DevOps"
---

> **Context:** Personal journal of how I moved from “docker run hello-world” to multi-service Compose stacks and beginner-friendly Amazon EKS labs. I haven’t owned production containers yet.  
> **AI assist:** ChatGPT helped organise my notes; I edited on 2025-10-15 to match my actual repos (Docker Multilang, ProjectHub proxy).  
> **Status:** Student-level experience—treat any “production-ready” phrasing as goals, not resume claims.

## TL;DR

- **Compose first, Kubernetes later (in labs).** I keep daily work in Docker Compose and spin up EKS only when I want to learn how autoscaling works [^compose][^kubernetes].  
- **Observability = logs + basic probes.** Structured logging, `/healthz` endpoints, and deliberate “kill the container” drills catch most mistakes before anyone else sees them.  
- **Infrastructure as code (for me) = Compose files + Terraform playgrounds.** Everything is versioned so I can reproduce the environment quickly.  
- **Lean images matter.** Multi-stage builds, security scans, and automation keep local dev snappy and hosting costs low when I deploy to Render/Netlify.

## From Hello World to Fleet Operations

- Began with single-container Python and Node demos, then expanded into a three-service stack (`frontend`, `api`, `db`).  
- Optimised each image with multi-stage builds, health checks, and persistent volumes to keep local resilience high.  
- Sharing the environment became trivial—classmates (or future-me) can run `docker compose up` to match the exact same setup [^compose].

## Compose Foundations

**docker-compose.yml highlights:**

```yaml
version: "3.9"
services:
  api:
    build: ./api
    env_file: .env.api
    depends_on: [db]
    ports: ["4000:4000"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4000/healthz"]
      interval: 30s
      retries: 3

  frontend:
    build:
      context: ./frontend
      target: production
    ports: ["8080:80"]
    depends_on: [api]

  db:
    image: postgres:15
    volumes:
      - pgdata:/var/lib/postgresql/data
    environment:
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
volumes:
  pgdata:
```

- Health checks gate traffic, Postgres volumes persist data, and env files keep secrets out of the compose file.

## Observability & Resilience

- **Structured logging:** JSON logs with trace IDs stream to stdout so `docker compose logs` (or CloudWatch in labs) can stitch requests together.  
- **Health probes:** Separate readiness/liveness endpoints tell Compose, k6 scripts, or EKS when to route traffic.  
- **Chaos drills:** Kill containers with `docker compose kill api` or `kubectl delete pod` to validate graceful degradation paths.

## Kubernetes on Amazon EKS (Lab work)

- **Provision with Terraform:** AWS Workshops + Skill Builder labs walk through defining clusters, node groups, VPCs, and IAM roles as code so I can rehearse the motions.  
- **Deploy with manifests:** I templated Deployments/Services/ConfigMaps for my three-service demo, but these only saw synthetic traffic.  
- **Ingress & security:** Used AWS Load Balancer Controller in the lab to set up TLS; secrets still lived in AWS Secrets Manager with sample data.  
- **Observability stack:** Followed workshop steps to install Prometheus + Grafana; no real customers, just metrics from test pods.

**Deployment snippet:**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: api
  template:
    metadata:
      labels:
        app: api
    spec:
      containers:
        - name: api
          image: $ECR_URI/api:${GITHUB_SHA}
          ports:
            - containerPort: 4000
          envFrom:
            - secretRef:
                name: api-secrets
          readinessProbe:
            httpGet:
              path: /ready
              port: 4000
            initialDelaySeconds: 5
            periodSeconds: 10
```

- Autoscaling and rolling upgrades behaved well in the sandbox; I still need mentorship before promising the same in production.

## Lessons from the Voyage

1. **Compose is the sandbox; Kubernetes is the stadium.** Start small, then upgrade once you need automation and autoscaling [^kubernetes].  
2. **Slim images matter.** Multi-stage builds and distroless bases shrink attack surfaces and boost cold-start speed.  
3. **Automate deployments.** CI pipelines build, scan (Trivy), push to ECR, and deploy via `kubectl` or Argo CD.  
4. **Monitor costs.** Track node-hours, enable autoscaling, and mix in spot capacity to avoid surprise invoices.

## Key Takeaways

- **Documentation is your map.** Write runbooks, diagrams, and README updates before the next storm hits.  
- **Health signals prevent firefights.** Treat logs, metrics, and probes as critical application features.  
- **Future focus:** Explore App Mesh and policy-as-code to standardise reliability and security across the fleet.

[^compose]: Docker Documentation, “Overview of Docker Compose,” accessed May 2025, https://docs.docker.com/compose/.
[^kubernetes]: Kubernetes Documentation, “Production Considerations,” accessed May 2025, https://kubernetes.io/docs/setup/production-environment/.
